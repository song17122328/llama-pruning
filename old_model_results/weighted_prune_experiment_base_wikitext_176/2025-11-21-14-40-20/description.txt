- Training Parameters: 
  - base_model: /newdata/LLMs/Llama-3-8B
  - save_ckpt_log_name: weighted_prune_experiment_wikitext_176
  - pruning_ratio: 0.1755
  - importance_method: taylor
  - dataset: wikitext2
  - num_samples: 128
  - gradient_batch_size: 4
  - seq_len: 128
  - use_gradient_checkpointing: False
  - remove_empty_layers: False
  - use_layer_weighting: True
  - layer_weighting_samples: 8
  - head_dim: 128
  - gqa_ratio: 4
  - test_before_prune: False
  - test_after_prune: True
  - finetune: False
  - finetune_method: lora
  - finetune_samples: 500
  - finetune_lr: 0.0001
  - finetune_epochs: 1
  - lora_r: 8
  - lora_alpha: 16
  - save_model: True
  - device: cuda:6
  - layer_start: 0
  - layer_end: None
