{
  "model_path": "weighted_prune_experiment_all_wikitext_500/pytorch_model.bin",
  "timestamp": "2025-11-21T17:44:05.737150",
  "metrics": {
    "model_info": {
      "total_params": 3962257408,
      "trainable_params": 3962257408,
      "total_params_M": 3962.257408,
      "total_params_B": 3.962257408,
      "attention_params": 739246080,
      "mlp_params": 2172137472,
      "attention_params_M": 739.24608,
      "mlp_params_M": 2172.137472,
      "attention_ratio": 0.18657194722064863,
      "mlp_ratio": 0.5482070568192625,
      "num_layers": 32,
      "model_size_mb": 7557.406494140625,
      "model_size_gb": 7.380279779434204
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 369.16015625,
      "ptb": 503.99969482421875
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.3785932721712538,
        "full_results": {
          "alias": "boolq",
          "acc,none": 0.3785932721712538,
          "acc_stderr,none": 0.00848334171802464
        }
      },
      "piqa": {
        "accuracy": 0.559847660500544,
        "full_results": {
          "alias": "piqa",
          "acc,none": 0.5674646354733406,
          "acc_stderr,none": 0.011559142916063173,
          "acc_norm,none": 0.559847660500544,
          "acc_norm_stderr,none": 0.011581954727227383
        }
      },
      "hellaswag": {
        "accuracy": 0.3302131049591715,
        "full_results": {
          "alias": "hellaswag",
          "acc,none": 0.286795459071898,
          "acc_stderr,none": 0.004513409114984252,
          "acc_norm,none": 0.3302131049591715,
          "acc_norm_stderr,none": 0.004693285694663322
        }
      },
      "winogrande": {
        "accuracy": 0.5335438042620363,
        "full_results": {
          "alias": "winogrande",
          "acc,none": 0.5335438042620363,
          "acc_stderr,none": 0.014020826677598101
        }
      },
      "arc_easy": {
        "accuracy": 0.3114478114478115,
        "full_results": {
          "alias": "arc_easy",
          "acc,none": 0.31691919191919193,
          "acc_stderr,none": 0.009547254611446178,
          "acc_norm,none": 0.3114478114478115,
          "acc_norm_stderr,none": 0.009502311567905436
        }
      },
      "arc_challenge": {
        "accuracy": 0.2636518771331058,
        "full_results": {
          "alias": "arc_challenge",
          "acc,none": 0.2235494880546075,
          "acc_stderr,none": 0.012174896631202666,
          "acc_norm,none": 0.2636518771331058,
          "acc_norm_stderr,none": 0.012875929151296968
        }
      },
      "openbookqa": {
        "accuracy": 0.286,
        "full_results": {
          "alias": "openbookqa",
          "acc,none": 0.124,
          "acc_stderr,none": 0.014754096608517476,
          "acc_norm,none": 0.286,
          "acc_norm_stderr,none": 0.020229346329177604
        }
      }
    },
    "avg_zeroshot_acc": 0.380471075781989
  }
}