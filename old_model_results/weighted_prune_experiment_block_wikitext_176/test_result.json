{
  "model_path": "weighted_prune_experiment_block_wikitext_176/pytorch_model.bin",
  "timestamp": "2025-11-21T16:41:58.008998",
  "metrics": {
    "model_info": {
      "total_params": 6620954624,
      "trainable_params": 6620954624,
      "total_params_M": 6620.954624,
      "total_params_B": 6.620954624,
      "attention_params": 1163919360,
      "mlp_params": 4406095872,
      "attention_params_M": 1163.91936,
      "mlp_params_M": 4406.095872,
      "attention_ratio": 0.17579328451836254,
      "mlp_ratio": 0.6654774307059199,
      "num_layers": 32,
      "model_size_mb": 12628.468994140625,
      "model_size_gb": 12.332489252090454
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 34.37156295776367,
      "ptb": 53.877357482910156
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.8403669724770643,
        "full_results": {
          "alias": "boolq",
          "acc,none": 0.8403669724770643,
          "acc_stderr,none": 0.006406021659710458
        }
      },
      "piqa": {
        "accuracy": 0.7464635473340587,
        "full_results": {
          "alias": "piqa",
          "acc,none": 0.7328618063112078,
          "acc_stderr,none": 0.010323440492612343,
          "acc_norm,none": 0.7464635473340587,
          "acc_norm_stderr,none": 0.010150090834551817
        }
      },
      "hellaswag": {
        "accuracy": 0.6550487950607449,
        "full_results": {
          "alias": "hellaswag",
          "acc,none": 0.4872535351523601,
          "acc_stderr,none": 0.004988159744742404,
          "acc_norm,none": 0.6550487950607449,
          "acc_norm_stderr,none": 0.004743808792037672
        }
      },
      "winogrande": {
        "accuracy": 0.6866614048934491,
        "full_results": {
          "alias": "winogrande",
          "acc,none": 0.6866614048934491,
          "acc_stderr,none": 0.013036512096748127
        }
      },
      "arc_easy": {
        "accuracy": 0.6847643097643098,
        "full_results": {
          "alias": "arc_easy",
          "acc,none": 0.7264309764309764,
          "acc_stderr,none": 0.009147424438490918,
          "acc_norm,none": 0.6847643097643098,
          "acc_norm_stderr,none": 0.009533589368506065
        }
      },
      "arc_challenge": {
        "accuracy": 0.45819112627986347,
        "full_results": {
          "alias": "arc_challenge",
          "acc,none": 0.4351535836177474,
          "acc_stderr,none": 0.014487986197186081,
          "acc_norm,none": 0.45819112627986347,
          "acc_norm_stderr,none": 0.014560220308714553
        }
      },
      "openbookqa": {
        "accuracy": 0.382,
        "full_results": {
          "alias": "openbookqa",
          "acc,none": 0.282,
          "acc_stderr,none": 0.020143572847290726,
          "acc_norm,none": 0.382,
          "acc_norm_stderr,none": 0.02175082059125093
        }
      }
    },
    "avg_zeroshot_acc": 0.6362137365442129
  }
}