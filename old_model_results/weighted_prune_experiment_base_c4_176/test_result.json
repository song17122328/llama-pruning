{
  "model_path": "weighted_prune_experiment/pytorch_model.bin",
  "timestamp": "2025-11-21T14:25:33.012829",
  "metrics": {
    "model_info": {
      "total_params": 6623100928,
      "trainable_params": 6623100928,
      "total_params_M": 6623.100928,
      "total_params_B": 6.623100928,
      "attention_params": 964689920,
      "mlp_params": 4607471616,
      "attention_params_M": 964.68992,
      "mlp_params_M": 4607.471616,
      "attention_ratio": 0.1456553252754538,
      "mlp_ratio": 0.6956668282860267,
      "num_layers": 32,
      "model_size_mb": 12632.562744140625,
      "model_size_gb": 12.336487054824829
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 26.595985412597656,
      "ptb": 58.35992431640625
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.537920489296636,
        "full_results": {
          "alias": "boolq",
          "acc,none": 0.537920489296636,
          "acc_stderr,none": 0.008719868567159826
        }
      },
      "piqa": {
        "accuracy": 0.7230685527747551,
        "full_results": {
          "alias": "piqa",
          "acc,none": 0.7241566920565833,
          "acc_stderr,none": 0.010427805502729186,
          "acc_norm,none": 0.7230685527747551,
          "acc_norm_stderr,none": 0.010440499969334648
        }
      },
      "hellaswag": {
        "accuracy": 0.5493925512846046,
        "full_results": {
          "alias": "hellaswag",
          "acc,none": 0.41724756024696275,
          "acc_stderr,none": 0.004920967192255224,
          "acc_norm,none": 0.5493925512846046,
          "acc_norm_stderr,none": 0.0049653753416431514
        }
      },
      "winogrande": {
        "accuracy": 0.5832675611681136,
        "full_results": {
          "alias": "winogrande",
          "acc,none": 0.5832675611681136,
          "acc_stderr,none": 0.013856250072796365
        }
      },
      "arc_easy": {
        "accuracy": 0.5631313131313131,
        "full_results": {
          "alias": "arc_easy",
          "acc,none": 0.6304713804713805,
          "acc_stderr,none": 0.009904325878447303,
          "acc_norm,none": 0.5631313131313131,
          "acc_norm_stderr,none": 0.0101776729281577
        }
      },
      "arc_challenge": {
        "accuracy": 0.33276450511945393,
        "full_results": {
          "alias": "arc_challenge",
          "acc,none": 0.29692832764505117,
          "acc_stderr,none": 0.013352025976725267,
          "acc_norm,none": 0.33276450511945393,
          "acc_norm_stderr,none": 0.013769863046192425
        }
      },
      "openbookqa": {
        "accuracy": 0.36,
        "full_results": {
          "alias": "openbookqa",
          "acc,none": 0.254,
          "acc_stderr,none": 0.01948659680164342,
          "acc_norm,none": 0.36,
          "acc_norm_stderr,none": 0.02148775108972057
        }
      }
    },
    "avg_zeroshot_acc": 0.521363567539268
  }
}