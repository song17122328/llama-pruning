#!/usr/bin/env python3
"""
诊断工具：对比不同序列长度下的梯度分布
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

print("="*80)
print("梯度分布诊断工具")
print("="*80)
print()
print("目的：检查为什么 512 序列长度导致前几层梯度过小")
print()

# 关键假设
print("【假设检查】")
print()
print("1. 不同序列长度的 loss scale")
print("   - HuggingFace transformers: outputs.loss 使用 reduction='mean'")
print("   - loss = sum(token_losses) / num_tokens")
print("   - 理论上：128 tokens 和 512 tokens 的 loss 数值应该接近")
print("   - 梯度：∇L 应该也接近（因为是对平均loss求导）")
print()

print("2. 为什么会出现梯度分布差异？")
print()
print("   可能原因 A：数据分布不同")
print("   - 512 tokens 采样的文本更长，内容可能不同")
print("   - 某些层可能在长文本上不活跃")
print()

print("   可能原因 B：梯度传播路径")
print("   - 更长的序列 → 更多的 token")
print("   - 虽然 loss 是平均的，但梯度的累积方式可能不同")
print()

print("   可能原因 C：注意力机制的影响")
print("   - 512 tokens 的注意力矩阵：512×512")
print("   - 128 tokens 的注意力矩阵：128×128")
print("   - 不同的注意力模式 → 不同的梯度流")
print()

print("3. 为什么前几层（Layer 2-14）受影响最大？")
print()
print("   观察：")
print("   - Layer 0-1: 几乎不剪（梯度正常）")
print("   - Layer 2-14: 剪枝 90-99%（梯度异常小）")
print("   - Layer 15-31: 几乎不剪（梯度正常）")
print()
print("   可能解释：")
print("   - 前几层（0-1）：处理局部特征，序列长度影响小")
print("   - 中间层（2-14）：依赖中等范围上下文，512 tokens 下梯度流不同")
print("   - 后面层（15-31）：处理高层语义，对序列长度不敏感")
print()

print("="*80)
print("诊断步骤")
print("="*80)
print()

print("【步骤 1】确认实验配置一致")
print()
print("运行以下两个实验，确保只有序列长度不同：")
print()
print("实验 A（128序列）：")
print("  python run_global_pruning.py \\")
print("    --base_model /path/to/model \\")
print("    --importance_method taylor_2nd \\  # 使用二阶")
print("    --pruning_ratio 0.2 \\")
print("    --output_name test_seq128")
print()
print("实验 B（512序列）：")
print("  python run_global_pruning.py \\")
print("    --base_model /path/to/model \\")
print("    --importance_method taylor_2nd \\  # 同样使用二阶")
print("    --pruning_ratio 0.2 \\")
print("    --output_name test_seq512")
print()
print("对比两次剪枝分布，看是否依然有差异。")
print()

print("【步骤 2】检查梯度分布")
print()
print("在 run_global_pruning.py 的梯度计算后添加：")
print()
print("```python")
print("# 在 loss.backward() 之后")
print("if batch_idx == 0:  # 只在第一个batch打印")
print("    for layer_idx in [0, 2, 10, 20, 31]:")
print("        layer_name = f'model.layers.{layer_idx}.mlp.gate_proj.weight'")
print("        for name, param in model.named_parameters():")
print("            if name == layer_name:")
print("                grad_mean = param.grad.abs().mean().item()")
print("                print(f'Layer {layer_idx} grad mean: {grad_mean:.6f}')")
print("```")
print()
print("预期：如果 Layer 2-14 的梯度确实很小，说明是梯度传播的问题。")
print()

print("【步骤 3】检查loss值")
print()
print("在 run_global_pruning.py 中打印：")
print()
print("```python")
print("print(f'Batch {batch_idx}: loss = {loss.item():.4f}')")
print("```")
print()
print("预期：")
print("- 如果 512 的 loss 比 128 大很多（如 4 倍），说明 loss scale 有问题")
print("- 如果 loss 接近，说明 loss 计算是正确的")
print()

print("="*80)
print("可能的解决方案")
print("="*80)
print()

print("【方案 1】使用相同的重要性方法")
print()
print("确保两次实验都使用 taylor_2nd（二阶泰勒）")
print()

print("【方案 2】调整序列长度折中")
print()
print("不要直接跳到 512，尝试：")
print("  - 256 tokens：折中选择")
print("  - 384 tokens：更接近 512 但可能更稳定")
print()

print("【方案 3】梯度归一化")
print()
print("在计算重要性之前，对每层的梯度进行归一化：")
print()
print("```python")
print("# 在累加 Hessian 之前")
print("for name, param in model.named_parameters():")
print("    if param.grad is not None:")
print("        # 按层归一化梯度")
print("        grad_norm = param.grad.norm()")
print("        if grad_norm > 0:")
print("            param.grad = param.grad / grad_norm")
print("```")
print()
print("注意：这会改变重要性的相对scale，需要谨慎使用。")
print()

print("【方案 4】使用相对重要性排序")
print()
print("问题：绝对重要性值可能因序列长度而变化")
print("解决：在每层内部使用相对排序，而不是全局排序")
print()
print("（这需要修改剪枝算法，比较复杂）")
print()

print("="*80)
print("总结")
print("="*80)
print()
print("最可能的原因：")
print("  1. 你用了不同的重要性方法（一阶 vs 二阶）")
print("  2. 512 序列长度下，Layer 2-14 的梯度异常小")
print("  3. 缺少某个关键参数（blockwise修正）")
print()
print("建议：")
print("  1. 先确认两次实验用的是相同的 importance_method")
print("  2. 告诉我具体的运行命令和参数")
print("  3. 尝试 256 或 384 作为序列长度折中")
print("  4. 如果一定要用 512，可能需要添加梯度归一化")
print()
print("="*80)
