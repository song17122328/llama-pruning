# å¾®è°ƒè„šæœ¬å¯¹æ¯”è¯´æ˜

## ğŸ“ ä¸¤ä¸ªå¾®è°ƒç›¸å…³çš„æ–‡ä»¶

### 1. `finetune_lora.py`ï¼ˆä¸»ç›®å½•ï¼‰

**ç±»å‹**: ç‹¬ç«‹çš„å‘½ä»¤è¡Œå·¥å…·è„šæœ¬

**ç”¨é€”**:
- å¯¹å·²ç»å‰ªæå®Œæˆçš„æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒ
- ç”¨äºæå‡å‰ªæåæ¨¡å‹çš„æ€§èƒ½

**ç‰¹ç‚¹**:
- âœ… **ç‹¬ç«‹è¿è¡Œ**: å®Œæ•´çš„å‘½ä»¤è¡Œå·¥å…·
- âœ… **ä¸“æ³¨ LoRA**: ä¸“é—¨ä½¿ç”¨ LoRA å¾®è°ƒæ–¹æ³•
- âœ… **ä½¿ç”¨ HuggingFace Trainer**: åŸºäº `transformers.Trainer` API
- âœ… **å®Œæ•´æµç¨‹**: åŠ è½½ â†’ å¾®è°ƒ â†’ è¯„ä¼° â†’ ä¿å­˜
- âœ… **æ”¯æŒ WandB**: å¯é€‰çš„è®­ç»ƒç›‘æ§
- âœ… **è‡ªåŠ¨è¯„ä¼°**: å¾®è°ƒåè‡ªåŠ¨è¿è¡Œè¯„ä¼°

**ä½¿ç”¨åœºæ™¯**:
```bash
# å¯¹å‰ªæåçš„æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python finetune_lora.py \
    --pruned_model results/taylor_only_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --lora_r 8 \
    --num_epochs 2 \
    --learning_rate 1e-4 \
    --batch_size 64
```

**ä¾èµ–**:
- `transformers` - HuggingFace Transformers
- `peft` - Parameter-Efficient Fine-Tuning åº“
- `datasets` - HuggingFace Datasets

**è¾“å‡º**:
- å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åœ¨ `results/<model_name>_finetuned/`
- åŒ…å« LoRA adapter æƒé‡
- è‡ªåŠ¨ç”Ÿæˆè¯„ä¼°ç»“æœ

---

### 2. `core/trainer/finetuner.py`ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰

**ç±»å‹**: å¯å¤ç”¨çš„ Python ç±»åº“

**ç”¨é€”**:
- ä½œä¸ºé¡¹ç›®å†…éƒ¨çš„å¾®è°ƒå·¥å…·
- è¢«å…¶ä»–è„šæœ¬ï¼ˆå¦‚å‰ªææµç¨‹ï¼‰å¯¼å…¥å’Œè°ƒç”¨

**ç‰¹ç‚¹**:
- âœ… **æ¨¡å—åŒ–è®¾è®¡**: `FineTuner` ç±»ï¼Œå¯å¯¼å…¥ä½¿ç”¨
- âœ… **åŒæ¨¡å¼æ”¯æŒ**: å…¨å‚æ•°å¾®è°ƒ + LoRA å¾®è°ƒ
- âœ… **çµæ´»é›†æˆ**: å¯é›†æˆåˆ°å‰ªææµç¨‹ä¸­
- âœ… **è‡ªå®šä¹‰æ•°æ®**: æ”¯æŒä½¿ç”¨é¡¹ç›®å†…éƒ¨çš„æ•°æ®åŠ è½½å™¨
- âœ… **ç®€å• API**: ä¸ä¾èµ–å¤æ‚çš„ Trainer API
- âœ… **è½»é‡çº§**: æ‰‹åŠ¨å®ç°è®­ç»ƒå¾ªç¯

**ä½¿ç”¨åœºæ™¯**:
```python
# åœ¨å‰ªæè„šæœ¬ä¸­ç›´æ¥è°ƒç”¨
from core.trainer.finetuner import FineTuner

# å‰ªæåç«‹å³å¾®è°ƒ
finetuner = FineTuner(
    model=pruned_model,
    tokenizer=tokenizer,
    device='cuda',
    use_lora=False  # ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒ
)

# æ‰§è¡Œå¾®è°ƒ
results = finetuner.finetune(
    dataset_name='wikitext',
    num_samples=500,
    lr=1e-5,
    epochs=1
)
```

**ä¾èµ–**:
- `torch` - PyTorchï¼ˆåŸºç¡€ä¾èµ–ï¼‰
- å¯é€‰ `peft` - å¦‚æœä½¿ç”¨ LoRA æ¨¡å¼

**ç‰¹æ€§**:
- æ”¯æŒå…¨å‚æ•°å¾®è°ƒï¼ˆæ¨èç”¨äºå‰ªæåçš„æ¨¡å‹ï¼‰
- æ”¯æŒ LoRA å¾®è°ƒï¼ˆä½æ˜¾å­˜ç¯å¢ƒï¼‰
- æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œæ›´çµæ´»
- å¯ä»¥é›†æˆåˆ°å…¶ä»–å·¥ä½œæµä¸­

---

## ğŸ”„ ä¸¤è€…çš„å…³ç³»

```
å‰ªææµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  run_global_pruning.py                          â”‚
â”‚  æˆ– layer_pruning.py                            â”‚
â”‚                                                 â”‚
â”‚  1. åŠ è½½åŸå§‹æ¨¡å‹                                â”‚
â”‚  2. æ‰§è¡Œå‰ªæ                                    â”‚
â”‚  3. ã€å¯é€‰ã€‘è°ƒç”¨ core/trainer/finetuner.py     â”‚  â† é›†æˆå¾®è°ƒ
â”‚     è¿›è¡Œå¿«é€Ÿå¾®è°ƒæ¢å¤æ€§èƒ½                        â”‚
â”‚  4. ä¿å­˜å‰ªææ¨¡å‹                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            ä¿å­˜ pruned_model.bin
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  finetune_lora.pyï¼ˆç‹¬ç«‹è„šæœ¬ï¼‰                   â”‚  â† ç‹¬ç«‹å¾®è°ƒ
â”‚                                                 â”‚
â”‚  1. åŠ è½½ pruned_model.bin                       â”‚
â”‚  2. é…ç½® LoRA å‚æ•°                              â”‚
â”‚  3. ä½¿ç”¨ Alpaca æ•°æ®é›†å¾®è°ƒ                      â”‚
â”‚  4. è¯„ä¼°æ€§èƒ½                                    â”‚
â”‚  5. ä¿å­˜åˆ° *_finetuned/                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š åŠŸèƒ½å¯¹æ¯”

| ç‰¹æ€§ | `finetune_lora.py` | `core/trainer/finetuner.py` |
|------|-------------------|---------------------------|
| **ç±»å‹** | ç‹¬ç«‹è„šæœ¬ | å¯å¯¼å…¥ç±» |
| **è¿è¡Œæ–¹å¼** | å‘½ä»¤è¡Œ | Python å¯¼å…¥ |
| **å¾®è°ƒæ–¹æ³•** | ä»… LoRA | å…¨å‚æ•° + LoRA |
| **è®­ç»ƒæ¡†æ¶** | HF Trainer | æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ |
| **æ•°æ®é›†** | Alpaca-cleaned | å¯è‡ªå®šä¹‰ï¼ˆwikitext ç­‰ï¼‰ |
| **WandB æ”¯æŒ** | âœ… æ˜¯ | âŒ å¦ |
| **è‡ªåŠ¨è¯„ä¼°** | âœ… æ˜¯ | âŒ å¦ |
| **ä»£ç è¡Œæ•°** | 470 è¡Œ | 429 è¡Œ |
| **ä¾èµ–** | transformers, peft, datasets | torch, å¯é€‰ peft |
| **é€‚ç”¨åœºæ™¯** | å•ç‹¬çš„å¾®è°ƒä»»åŠ¡ | é›†æˆåˆ°å‰ªææµç¨‹ |

## ğŸ¯ ä½¿ç”¨å»ºè®®

### ä½¿ç”¨ `finetune_lora.py` çš„åœºæ™¯

1. **å·²ç»å®Œæˆå‰ªæï¼Œéœ€è¦å•ç‹¬å¾®è°ƒ**
   ```bash
   python finetune_lora.py --pruned_model results/MyModel/pruned_model.bin
   ```

2. **éœ€è¦ä½¿ç”¨ Alpaca æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ**
   - è‡ªåŠ¨å¤„ç† Alpaca æ ¼å¼
   - åŒ…å«å®Œæ•´çš„æç¤ºè¯æ¨¡æ¿

3. **éœ€è¦ WandB ç›‘æ§è®­ç»ƒè¿‡ç¨‹**
   ```bash
   python finetune_lora.py --wandb_project my_finetune
   ```

4. **éœ€è¦å®Œæ•´çš„è¯„ä¼°æµç¨‹**
   - è‡ªåŠ¨è¿è¡Œ PPLã€Zero-shot ç­‰è¯„ä¼°

### ä½¿ç”¨ `core/trainer/finetuner.py` çš„åœºæ™¯

1. **åœ¨å‰ªæè„šæœ¬ä¸­é›†æˆå¾®è°ƒ**
   ```python
   # åœ¨ run_global_pruning.py ä¸­
   from core.trainer.finetuner import FineTuner

   # å‰ªæåç«‹å³å¾®è°ƒ
   finetuner = FineTuner(model, tokenizer)
   finetuner.finetune(num_samples=500)
   ```

2. **éœ€è¦å…¨å‚æ•°å¾®è°ƒ**
   - å‰ªæåçš„æ¨¡å‹æ¨èä½¿ç”¨å…¨å‚æ•°å¾®è°ƒ
   - ä¸ä½¿ç”¨ LoRA adapter

3. **éœ€è¦å¿«é€Ÿå®éªŒ**
   - æ›´è½»é‡çº§ï¼Œå¯åŠ¨æ›´å¿«
   - ä¸éœ€è¦ HuggingFace Trainer çš„å¤æ‚é…ç½®

4. **è‡ªå®šä¹‰è®­ç»ƒæµç¨‹**
   - å¯ä»¥ä¿®æ”¹è®­ç»ƒå¾ªç¯
   - æ›´çµæ´»çš„æ§åˆ¶

## ğŸ’¡ å®é™…å·¥ä½œæµç¨‹ç¤ºä¾‹

### æ–¹å¼ 1: å‰ªæ + é›†æˆå¾®è°ƒ

```bash
# è¿è¡Œå‰ªæï¼ˆå†…éƒ¨ä¼šè°ƒç”¨ finetuner.py è¿›è¡Œå¾®è°ƒï¼‰
python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --finetune True \
    --finetune_epochs 1
```

### æ–¹å¼ 2: å‰ªæ + ç‹¬ç«‹å¾®è°ƒ

```bash
# 1. åªå‰ªæï¼Œä¸å¾®è°ƒ
python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --finetune False

# 2. å•ç‹¬è¿›è¡Œ LoRA å¾®è°ƒ
python finetune_lora.py \
    --pruned_model results/MyModel/pruned_model.bin \
    --num_epochs 2 \
    --batch_size 64
```

## ğŸ—‘ï¸ æ¸…ç†å»ºè®®

åŸºäºä½ çš„é—®é¢˜ï¼Œæˆ‘çš„å»ºè®®æ˜¯ï¼š

### å¦‚æœä½ åªä½¿ç”¨ç‹¬ç«‹çš„ LoRA å¾®è°ƒ
- âœ… **ä¿ç•™** `finetune_lora.py`
- â“ **å¯é€‰ä¿ç•™** `core/trainer/finetuner.py`ï¼ˆå¦‚æœå‰ªææµç¨‹ä¸­ä¸ä½¿ç”¨ï¼‰

### å¦‚æœä½ åœ¨å‰ªææµç¨‹ä¸­é›†æˆå¾®è°ƒ
- âœ… **ä¿ç•™** `core/trainer/finetuner.py`
- â“ **å¯é€‰ä¿ç•™** `finetune_lora.py`ï¼ˆç”¨äºåç»­å•ç‹¬å¾®è°ƒï¼‰

### æ¨èåšæ³•
**ä¸¤ä¸ªéƒ½ä¿ç•™**ï¼Œå› ä¸ºï¼š
1. å®ƒä»¬ç”¨é€”ä¸åŒï¼Œäº’ä¸å†²çª
2. `finetune_lora.py` ç”¨äºå•ç‹¬çš„å¾®è°ƒä»»åŠ¡
3. `core/trainer/finetuner.py` ç”¨äºé›†æˆåˆ°å‰ªææµç¨‹
4. ä»£ç é‡éƒ½ä¸å¤§ï¼ˆå„ 400-500 è¡Œï¼‰

## ğŸ“ ä»£ç ä¼˜åŒ–å»ºè®®

å¦‚æœæƒ³ç»Ÿä¸€å¾®è°ƒæ¥å£ï¼Œå¯ä»¥ï¼š

1. **è®© `finetune_lora.py` è°ƒç”¨ `FineTuner` ç±»**
   - å‡å°‘ä»£ç é‡å¤
   - ç»Ÿä¸€å¾®è°ƒé€»è¾‘

2. **æˆ–è€…æ˜ç¡®åˆ†å·¥**
   - `finetune_lora.py` - ä¸“é—¨çš„ LoRA å¾®è°ƒè„šæœ¬
   - `core/trainer/finetuner.py` - é€šç”¨çš„å¾®è°ƒç±»åº“

ç›®å‰çš„è®¾è®¡æ˜¯åˆç†çš„ï¼Œä¸¤è€…å„å¸å…¶èŒã€‚
