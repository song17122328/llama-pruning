# Qwen 2.5 & Mistral 7B å‰ªæä¸å¾®è°ƒæŒ‡å—

æœ¬æ–‡æ¡£æä¾› Qwen2.5-7B å’Œ Mistral-7B-v0.3 æ¨¡å‹çš„å‰ªæä¸å¾®è°ƒå‘½ä»¤ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
prune_log/
â”œâ”€â”€ Qwen2.5-7B/           # Qwen å±‚çº§å‰ªæç»“æœï¼ˆlayer_pruning.pyï¼‰
â””â”€â”€ Mistral-7B-v0.3/      # Mistral å±‚çº§å‰ªæç»“æœï¼ˆlayer_pruning.pyï¼‰

results/
â”œâ”€â”€ Qwen2.5-7B/           # Qwen å…¨å±€å‰ªæç»“æœï¼ˆrun_global_pruning.pyï¼‰
â””â”€â”€ Mistral-7B-v0.3/      # Mistral å…¨å±€å‰ªæç»“æœï¼ˆrun_global_pruning.pyï¼‰
```

---

## ğŸ”§ Qwen 2.5 7B å‰ªæå‘½ä»¤

### æ¶æ„ç‰¹ç‚¹
- **Q Heads**: 28
- **KV Heads**: 4
- **GQA Ratio**: 7:1
- **Head Dim**: 128
- **æ€»å±‚æ•°**: 28

### 1. å±‚çº§å‰ªæï¼ˆLayer Pruningï¼‰- æ¨è

#### 20% å‰ªæç‡
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --save_ckpt_log_name Qwen2.5-7B/prune_20 \
  --pruning_ratio 0.2 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 1.0 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

#### 30% å‰ªæç‡ï¼ˆæ›´æ¿€è¿›ï¼‰
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --save_ckpt_log_name Qwen2.5-7B/prune_30 \
  --pruning_ratio 0.3 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 1.5 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

#### 50% å‰ªæç‡ï¼ˆæç«¯æµ‹è¯•ï¼‰
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --save_ckpt_log_name Qwen2.5-7B/prune_50 \
  --pruning_ratio 0.5 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 2.0 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

### 2. å…¨å±€å‰ªæï¼ˆGlobal Pruningï¼‰

#### 20% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/global_prune_20 \
  --target_sparsity 0.2 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model \
  --skip_evaluation
```

### 3. å¾®è°ƒå‘½ä»¤

#### LoRA å¾®è°ƒï¼ˆå‰ªæåæ¢å¤æ€§èƒ½ï¼‰
```bash
CUDA_VISIBLE_DEVICES=0 python finetune_lora.py \
  --pruned_model prune_log/Qwen2.5-7B/prune_20/best_model.bin \
  --data_path yahma/alpaca-cleaned \
  --output_dir prune_log/Qwen2.5-7B/prune_20_finetuned \
  --num_epochs 3 \
  --learning_rate 3e-4 \
  --batch_size 128 \
  --micro_batch_size 4 \
  --lora_r 8 \
  --lora_alpha 16 \
  --lora_dropout 0.05 \
  --device cuda:0
```

---

## ğŸ”§ Mistral 7B v0.3 å‰ªæå‘½ä»¤

### æ¶æ„ç‰¹ç‚¹
- **Q Heads**: 32
- **KV Heads**: 8
- **GQA Ratio**: 4:1
- **Head Dim**: 128
- **æ€»å±‚æ•°**: 32
- **æ³¨æ„**: v0.3 å·²ç§»é™¤æ»‘åŠ¨çª—å£ï¼Œä½¿ç”¨æ ‡å‡†å…¨æ³¨æ„åŠ›æœºåˆ¶

### 1. å±‚çº§å‰ªæï¼ˆLayer Pruningï¼‰- æ¨è

#### 20% å‰ªæç‡
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --save_ckpt_log_name Mistral-7B-v0.3/prune_20 \
  --pruning_ratio 0.2 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 1.0 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

#### 30% å‰ªæç‡
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --save_ckpt_log_name Mistral-7B-v0.3/prune_30 \
  --pruning_ratio 0.3 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 1.5 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

#### 50% å‰ªæç‡
```bash
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --save_ckpt_log_name Mistral-7B-v0.3/prune_50 \
  --pruning_ratio 0.5 \
  --pruning_distribution 5:5 \
  --pruning_strategy inverse \
  --layer_importance_weight 2.0 \
  --layer_importance_method removal \
  --layer_importance_samples 50 \
  --channel_importance_samples 10 \
  --taylor_seq_len 128 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model
```

### 2. å…¨å±€å‰ªæï¼ˆGlobal Pruningï¼‰

#### 20% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/global_prune_20 \
  --target_sparsity 0.2 \
  --nsamples 128 \
  --device cuda:0 \
  --save_model \
  --skip_evaluation
```

### 3. å¾®è°ƒå‘½ä»¤

#### LoRA å¾®è°ƒ
```bash
CUDA_VISIBLE_DEVICES=0 python finetune_lora.py \
  --pruned_model prune_log/Mistral-7B-v0.3/prune_20/best_model.bin \
  --data_path yahma/alpaca-cleaned \
  --output_dir prune_log/Mistral-7B-v0.3/prune_20_finetuned \
  --num_epochs 3 \
  --learning_rate 3e-4 \
  --batch_size 128 \
  --micro_batch_size 4 \
  --lora_r 8 \
  --lora_alpha 16 \
  --lora_dropout 0.05 \
  --device cuda:0
```

---

## ğŸ“Š ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•å»ºè®®

### ç›¸åŒå‰ªæç‡å¯¹æ¯”ï¼ˆéªŒè¯ç®—æ³•æ³›åŒ–æ€§ï¼‰

```bash
# LLaMA 3 8B (baseline)
python layer_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --save_ckpt_log_name LLaMA-3-8B/prune_20 \
  --pruning_ratio 0.2 ...

# Mistral 7B v0.3 (ç›¸åŒ GQA 4:1)
python layer_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --save_ckpt_log_name Mistral-7B-v0.3/prune_20 \
  --pruning_ratio 0.2 ...

# Qwen 2.5 7B (ä¸åŒ GQA 7:1)
python layer_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --save_ckpt_log_name Qwen2.5-7B/prune_20 \
  --pruning_ratio 0.2 ...
```

### è®ºæ–‡å®éªŒå»ºè®®

**æµ‹è¯•çŸ©é˜µï¼š**

| æ¨¡å‹ | å‰ªæç‡ | ç›®çš„ |
|------|--------|------|
| LLaMA 3 8B | 20%, 30%, 50% | åŸºå‡†å¯¹æ¯” |
| Mistral 7B v0.3 | 20%, 30% | éªŒè¯ç›¸åŒ GQA æ¯”ä¾‹ï¼ˆ4:1ï¼‰|
| Qwen 2.5 7B | 20%, 30% | éªŒè¯ä¸åŒ GQA æ¯”ä¾‹ï¼ˆ7:1ï¼‰|

---

## âš™ï¸ å‚æ•°è¯´æ˜

### æ ¸å¿ƒå‚æ•°
- `--pruning_ratio`: æ€»å‰ªæç‡ï¼ˆ0.2 = 20%ï¼‰
- `--pruning_distribution`: Attention:MLP å‰ªææ¯”ä¾‹ï¼ˆ5:5 è¡¨ç¤ºå„ä¸€åŠï¼‰
- `--pruning_strategy`: å‰ªæç­–ç•¥
  - `inverse`: é‡è¦å±‚å‰ªå°‘ï¼Œä¸é‡è¦å±‚å‰ªå¤šï¼ˆæ¨èï¼‰
  - `proportional`: é‡è¦å±‚å‰ªå¤š
  - `uniform`: å‡åŒ€å‰ªæ

### å±‚é‡è¦åº¦å‚æ•°
- `--layer_importance_method`:
  - `removal`: é€šè¿‡ç§»é™¤å±‚è¯„ä¼°é‡è¦åº¦ï¼ˆæ¨èï¼‰
  - `activation`: é€šè¿‡æ¿€æ´»å€¼è¯„ä¼°
- `--layer_importance_weight`: å±‚é—´å·®å¼‚ç³»æ•°ï¼ˆ1.0-2.0ï¼‰

### é€šé“/å¤´é‡è¦åº¦å‚æ•°
- `--channel_importance_samples`: Taylor é‡è¦æ€§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆ10-50ï¼‰
- `--taylor_seq_len`: åºåˆ—é•¿åº¦ï¼ˆ128ï¼‰
- `--nsamples`: æ ¡å‡†æ ·æœ¬æ•°ï¼ˆ128ï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€å°æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
```bash
# Qwen å¿«é€Ÿæµ‹è¯•ï¼ˆ10% å‰ªæï¼‰
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --save_ckpt_log_name Qwen2.5-7B/test \
  --pruning_ratio 0.1 \
  --nsamples 32 \
  --layer_importance_samples 10 \
  --channel_importance_samples 5 \
  --device cuda:0

# Mistral å¿«é€Ÿæµ‹è¯•
CUDA_VISIBLE_DEVICES=0 python layer_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --save_ckpt_log_name Mistral-7B-v0.3/test \
  --pruning_ratio 0.1 \
  --nsamples 32 \
  --layer_importance_samples 10 \
  --channel_importance_samples 5 \
  --device cuda:0
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **è‡ªåŠ¨é…ç½®æ£€æµ‹**ï¼šä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹ GQA é…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š `--gqa_ratio`
2. **æ˜¾å­˜è¦æ±‚**ï¼š
   - Qwen 2.5 7B: ~14GB
   - Mistral 7B v0.3: ~14GB
   - å»ºè®®ä½¿ç”¨ A100/V100/3090 ä»¥ä¸Š
3. **ä¸‹è½½æ¨¡å‹**ï¼š
   - Qwenï¼šå¯ä½¿ç”¨ ModelScope é•œåƒåŠ é€Ÿ
   - Mistralï¼šç›´æ¥ä» HuggingFace ä¸‹è½½
4. **ç»“æœä¿å­˜**ï¼š
   - æ¨¡å‹æƒé‡ï¼š`prune_log/{model}/prune_{ratio}/best_model.bin`
   - æ—¥å¿—æ–‡ä»¶ï¼š`prune_log/{model}/prune_{ratio}/log.txt`
   - é…ç½®æ–‡ä»¶ï¼š`prune_log/{model}/prune_{ratio}/config.json`

---

## ğŸ“ˆ è¯„ä¼°å‘½ä»¤

### è¯„ä¼°å‰ªæåçš„æ¨¡å‹
```bash
# è¯„ä¼° Qwen å‰ªææ¨¡å‹
python evaluation/run_evaluation.py \
  --model_path prune_log/Qwen2.5-7B/prune_20/best_model.bin \
  --tasks wikitext,c4,lambada \
  --device cuda:0

# è¯„ä¼° Mistral å‰ªææ¨¡å‹
python evaluation/run_evaluation.py \
  --model_path prune_log/Mistral-7B-v0.3/prune_20/best_model.bin \
  --tasks wikitext,c4,lambada \
  --device cuda:0
```
