# å¤šæ¨¡å‹å‰ªææŒ‡å—ï¼ˆLLaMA / Qwen / Mistralï¼‰

æœ¬æ–‡æ¡£æä¾› LLaMA-3-8Bã€Qwen2.5-7B å’Œ Mistral-7B-v0.3 æ¨¡å‹çš„å…¨å±€å‰ªæå‘½ä»¤å’Œä½¿ç”¨æŒ‡å—ã€‚

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹æ¶æ„

| æ¨¡å‹ | Q Heads | KV Heads | GQA Ratio | å‚æ•°é‡ | ç‰¹æ®Šæœºåˆ¶ |
|------|---------|----------|-----------|--------|----------|
| **LLaMA-3-8B** | 32 | 8 | **4:1** | 8B | æ ‡å‡† Attention |
| **Mistral-7B-v0.3** | 32 | 8 | **4:1** | 7B | æ ‡å‡† Attentionï¼ˆv0.2+ å·²å–æ¶ˆæ»‘åŠ¨çª—å£ï¼‰ |
| **Qwen2.5-7B** | 28 | 4 | **7:1** | 7B | æ ‡å‡† Attention |

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… è‡ªåŠ¨æ£€æµ‹ GQA é…ç½®ï¼ˆæ— éœ€æ‰‹åŠ¨æŒ‡å®šå‚æ•°ï¼‰
- âœ… æ”¯æŒä¸åŒ GQA æ¯”ä¾‹ï¼ˆ4:1 å’Œ 7:1ï¼‰
- âœ… ç»Ÿä¸€çš„å‰ªææ¥å£

---

## ğŸ“ ç›®å½•ç»“æ„

```
results/
â”œâ”€â”€ LLaMA-3-8B/           # LLaMA å…¨å±€å‰ªæç»“æœ
â”œâ”€â”€ Qwen2.5-7B/           # Qwen å…¨å±€å‰ªæç»“æœ
â””â”€â”€ Mistral-7B-v0.3/      # Mistral å…¨å±€å‰ªæç»“æœ
```

---

## ğŸ”§ å…¨å±€å‰ªæå‘½ä»¤

### LLaMA-3-8Bï¼ˆåŸºå‡†æ¨¡å‹ï¼‰

#### 20% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --output_name LLaMA-3-8B/global_prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --device cuda:0
```

#### 30% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --output_name LLaMA-3-8B/global_prune_30 \
  --pruning_ratio 0.3 \
  --temperature 0.0 \
  --device cuda:0
```

#### 50% ç¨€ç–åº¦ï¼ˆæç«¯æµ‹è¯•ï¼‰
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --output_name LLaMA-3-8B/global_prune_50 \
  --pruning_ratio 0.5 \
  --temperature 0.0 \
  --device cuda:0
```

---

### Qwen2.5-7B

**æ¶æ„ç‰¹ç‚¹**ï¼š
- GQA Ratio: **7:1**ï¼ˆä¸åŒäº LLaMA/Mistral çš„ 4:1ï¼‰
- æ€»å±‚æ•°: 28
- è‡ªåŠ¨æ£€æµ‹é…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š

#### 20% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/global_prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --device cuda:0
```

#### 30% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/global_prune_30 \
  --pruning_ratio 0.3 \
  --temperature 0.0 \
  --device cuda:0
```

#### 50% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/global_prune_50 \
  --pruning_ratio 0.5 \
  --temperature 0.0 \
  --device cuda:0
```

---

### Mistral-7B-v0.3

**æ¶æ„ç‰¹ç‚¹**ï¼š
- GQA Ratio: **4:1**ï¼ˆä¸ LLaMA ç›¸åŒï¼‰
- æ€»å±‚æ•°: 32
- v0.3 å·²ç§»é™¤æ»‘åŠ¨çª—å£ï¼Œä½¿ç”¨æ ‡å‡†å…¨æ³¨æ„åŠ›

#### 20% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/global_prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --device cuda:0
```

#### 30% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/global_prune_30 \
  --pruning_ratio 0.3 \
  --temperature 0.0 \
  --device cuda:0
```

#### 50% ç¨€ç–åº¦
```bash
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/global_prune_50 \
  --pruning_ratio 0.5 \
  --temperature 0.0 \
  --device cuda:0
```

---

## ğŸ”„ å¾®è°ƒå‘½ä»¤ï¼ˆå‰ªæåæ€§èƒ½æ¢å¤ï¼‰

### é›†æˆå¾®è°ƒï¼ˆæ¨èï¼‰

å‰ªææ—¶ç›´æ¥å¯ç”¨å¾®è°ƒï¼š

```bash
# LLaMA 3 8B - 20% å‰ªæ + å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --output_name LLaMA-3-8B/prune_20_finetune \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --finetune \
  --finetune_data_path yahma/alpaca-cleaned \
  --finetune_epochs 3 \
  --finetune_lr 3e-4 \
  --finetune_batch_size 128 \
  --finetune_micro_batch_size 4 \
  --lora_r 8 \
  --lora_alpha 16 \
  --device cuda:0

# Qwen 2.5 7B - 20% å‰ªæ + å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/prune_20_finetune \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --finetune \
  --finetune_data_path yahma/alpaca-cleaned \
  --finetune_epochs 3 \
  --finetune_lr 3e-4 \
  --device cuda:0

# Mistral 7B v0.3 - 20% å‰ªæ + å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/prune_20_finetune \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --finetune \
  --finetune_data_path yahma/alpaca-cleaned \
  --finetune_epochs 3 \
  --finetune_lr 3e-4 \
  --device cuda:0
```

### ç‹¬ç«‹å¾®è°ƒ

å¦‚æœå·²æœ‰å‰ªææ¨¡å‹ï¼Œå¯ä»¥å•ç‹¬è¿è¡Œå¾®è°ƒï¼š

```bash
# ä½¿ç”¨ finetune_lora.py å•ç‹¬å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python finetune_lora.py \
  --pruned_model results/Qwen2.5-7B/global_prune_20/pruned_model.bin \
  --data_path yahma/alpaca-cleaned \
  --output_dir results/Qwen2.5-7B/prune_20_finetuned \
  --num_epochs 3 \
  --learning_rate 3e-4 \
  --batch_size 128 \
  --micro_batch_size 4 \
  --lora_r 8 \
  --lora_alpha 16 \
  --device cuda:0
```

---

## ğŸ“Š ä¸‰æ¨¡å‹å¯¹æ¯”å®éªŒè®¾è®¡

### å®éªŒç›®æ ‡
- **ç›¸åŒ GQA æ¯”ä¾‹éªŒè¯**ï¼šLLaMA-3 (4:1) vs Mistral (4:1)
- **ä¸åŒ GQA æ¯”ä¾‹éªŒè¯**ï¼šLLaMA-3 (4:1) vs Qwen (7:1)
- **ç®—æ³•æ³›åŒ–æ€§éªŒè¯**ï¼šä¸‰æ¨¡å‹åœ¨ä¸åŒç¨€ç–åº¦ä¸‹çš„è¡¨ç°

### æµ‹è¯•çŸ©é˜µ

| æ¨¡å‹ | ç¨€ç–åº¦ | ç›®çš„ | ä¼˜å…ˆçº§ |
|------|--------|------|--------|
| LLaMA-3-8B | 20%, 30%, 50% | åŸºå‡†å¯¹æ¯” | â­â­â­â­â­ |
| Mistral-7B-v0.3 | 20%, 30% | éªŒè¯ç›¸åŒ GQA (4:1) | â­â­â­â­â­ |
| Qwen2.5-7B | 20%, 30% | éªŒè¯ä¸åŒ GQA (7:1) | â­â­â­â­â­ |

### æ‰¹é‡è¿è¡Œè„šæœ¬

åˆ›å»º `scripts/run_all_experiments.sh`ï¼š

```bash
#!/bin/bash
# æ‰¹é‡è¿è¡Œä¸‰æ¨¡å‹å¯¹æ¯”å®éªŒ

# LLaMA-3-8B
for sparsity in 0.2 0.3 0.5; do
  CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
    --base_model meta-llama/Meta-Llama-3-8B \
    --output_name LLaMA-3-8B/prune_$(echo "$sparsity * 100" | bc | cut -d. -f1) \
    --pruning_ratio $sparsity \
    
    --device cuda:0
    
done

# Mistral-7B-v0.3
for sparsity in 0.2 0.3; do
  CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
    --base_model mistralai/Mistral-7B-v0.3 \
    --output_name Mistral-7B-v0.3/prune_$(echo "$sparsity * 100" | bc | cut -d. -f1) \
    --pruning_ratio $sparsity \
    
    --device cuda:0
    
done

# Qwen2.5-7B
for sparsity in 0.2 0.3; do
  CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
    --base_model Qwen/Qwen2.5-7B \
    --output_name Qwen2.5-7B/prune_$(echo "$sparsity * 100" | bc | cut -d. -f1) \
    --pruning_ratio $sparsity \
    
    --device cuda:0
    
done
```

---

## âš™ï¸ æ ¸å¿ƒå‚æ•°è¯´æ˜

### å¿…éœ€å‚æ•°
- `--base_model`: æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID
- `--output_name`: è¾“å‡ºç›®å½•åï¼ˆä¿å­˜åœ¨ `results/{output_name}/`ï¼‰
- `--pruning_ratio`: ç›®æ ‡ç¨€ç–åº¦ï¼ˆ0.2 = 20%ï¼‰

### å‰ªæå‚æ•°
- `--importance_method`: é‡è¦æ€§åº¦é‡
  - `taylor`: Taylor ä¸€é˜¶ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
  - `taylor_2nd`: Taylor äºŒé˜¶ï¼ˆæ›´ç²¾ç¡®ï¼Œæ›´æ…¢ï¼‰
  - `wanda`: Wanda æ–¹æ³•
  - `magnitude`: æƒé‡å¤§å°
- `--dataset`: æ ¡å‡†æ•°æ®é›†ï¼ˆwikitext2 / ptb / c4ï¼Œé»˜è®¤ wikitext2ï¼‰
- `--temperature`: H-GSP æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ 1.0ï¼‰
  - **è®¾ä¸º 0.0**: åªä½¿ç”¨å…¨å±€ Taylorï¼Œè·³è¿‡å±‚çº§/å—çº§é‡è¦æ€§ï¼ˆæ¨èï¼Œé¿å…æ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼‰
  - **è®¾ä¸º 1.0**: ä½¿ç”¨å®Œæ•´ H-GSP å±‚æ¬¡åŒ–å‰ªæç­–ç•¥
- `--epsilon`: H-GSP åç¼©é˜ˆå€¼ï¼ˆé»˜è®¤ 0.15ï¼‰

### å¾®è°ƒå‚æ•°
- `--finetune`: å¯ç”¨å¾®è°ƒ
- `--finetune_data_path`: å¾®è°ƒæ•°æ®é›†
- `--finetune_epochs`: å¾®è°ƒè½®æ•°ï¼ˆæ¨è 3-5ï¼‰
- `--finetune_lr`: å­¦ä¹ ç‡ï¼ˆæ¨è 3e-4ï¼‰
- `--lora_r`: LoRA ç§©ï¼ˆæ¨è 8-16ï¼‰
- `--lora_alpha`: LoRA ç¼©æ”¾ï¼ˆæ¨è 2Ã—rï¼‰

### è¯„ä¼°å‚æ•°
- `--skip_evaluation`: è·³è¿‡è‡ªåŠ¨è¯„ä¼°ï¼ˆèŠ‚çœæ—¶é—´ï¼‰
- `--eval_tasks`: è¯„ä¼°ä»»åŠ¡ï¼ˆé»˜è®¤ï¼šwikitext,c4ï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¿«é€ŸéªŒè¯ï¼ˆ10% ç¨€ç–åº¦ï¼‰

```bash
# å¿«é€Ÿæµ‹è¯• Qwenï¼ˆéªŒè¯ç¯å¢ƒï¼‰
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/quick_test \
  --pruning_ratio 0.1 \
  --temperature 0.0 \
  --temperature 0.0 \
  --device cuda:0

# å¿«é€Ÿæµ‹è¯• Mistral
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/quick_test \
  --pruning_ratio 0.1 \
  --temperature 0.0 \
  --temperature 0.0 \
  --device cuda:0
```

**æ¨èé…ç½®è¯´æ˜**ï¼š
- `--temperature 0.0`ï¼šåªä½¿ç”¨å…¨å±€ Taylor é‡è¦æ€§ï¼Œè·³è¿‡å±‚çº§/å—çº§é‡è¦æ€§æµ‹è¯•
- âœ… **ä¼˜åŠ¿**ï¼šé¿å…æ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼ˆå¦‚ Qwen/Mistral çš„å±‚æ’ç­‰æ˜ å°„ï¼‰
- âœ… **æ€§èƒ½**ï¼šå…¨å±€ Taylor æ–¹æ³•å·²è¢«å®éªŒè¯æ˜æ•ˆæœæœ€å¥½
- âœ… **é€Ÿåº¦**ï¼šè·³è¿‡å±‚çº§åˆ†æï¼ŒåŠ å¿«å‰ªæé€Ÿåº¦

### 2. æ ‡å‡†å‰ªæï¼ˆ20% ç¨€ç–åº¦ï¼Œæ¨èé…ç½®ï¼‰

```bash
# Qwenï¼ˆæ¨èï¼štemperature=0ï¼‰
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model Qwen/Qwen2.5-7B \
  --output_name Qwen2.5-7B/prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --temperature 0.0 \
  --device cuda:0

# Mistralï¼ˆæ¨èï¼štemperature=0ï¼‰
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model mistralai/Mistral-7B-v0.3 \
  --output_name Mistral-7B-v0.3/prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --temperature 0.0 \
  --device cuda:0

# LLaMAï¼ˆå¯ä½¿ç”¨å®Œæ•´ H-GSPï¼‰
CUDA_VISIBLE_DEVICES=0 python run_global_pruning.py \
  --base_model meta-llama/Meta-Llama-3-8B \
  --output_name LLaMA-3-8B/prune_20 \
  --pruning_ratio 0.2 \
  --temperature 0.0 \
  --temperature 1.0 \
  --device cuda:0
```

---

## ğŸ“ˆ è¯„ä¼°å‰ªææ¨¡å‹

### ä½¿ç”¨å†…ç½®è¯„ä¼°
```bash
python evaluation/run_evaluation.py \
  --model_path results/Qwen2.5-7B/prune_20/pruned_model.bin \
  --base_model Qwen/Qwen2.5-7B \
  --tasks wikitext,c4,lambada \
  --device cuda:0
```

### Python API
```python
from evaluation.metrics.ppl import PPLMetric
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½å‰ªææ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('results/Qwen2.5-7B/prune_20')
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B')

# è¯„ä¼°
ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device='cuda')
print(f"WikiText-2 PPL: {ppl['wikitext2']}")
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

### æ˜¾å­˜è¦æ±‚
- **Qwen2.5-7B**: ~14GBï¼ˆFP16ï¼‰
- **Mistral-7B-v0.3**: ~14GBï¼ˆFP16ï¼‰
- **LLaMA-3-8B**: ~16GBï¼ˆFP16ï¼‰
- æ¨è GPU: A100 / V100 / 3090 / 4090

### æ¨¡å‹ä¸‹è½½
```bash
# Qwenï¼ˆå›½å†…ç”¨æˆ·æ¨è ModelScopeï¼‰
pip install modelscope
python -c "from modelscope import snapshot_download; snapshot_download('qwen/Qwen2.5-7B')"

# æˆ–ä½¿ç”¨ HuggingFace
huggingface-cli download Qwen/Qwen2.5-7B

# Mistral
huggingface-cli download mistralai/Mistral-7B-v0.3
```

### è¾“å‡ºæ–‡ä»¶
```
results/{output_name}/
â”œâ”€â”€ pruned_model.bin              # å‰ªæåçš„æ¨¡å‹æƒé‡
â”œâ”€â”€ config.json                   # æ¨¡å‹é…ç½®
â”œâ”€â”€ pruning_analysis.json         # å‰ªæåˆ†ææŠ¥å‘Š
â”œâ”€â”€ global_group_table.csv        # å…¨å±€åˆ†ç»„è¡¨
â””â”€â”€ logs/
    â””â”€â”€ training.log              # è¯¦ç»†æ—¥å¿—
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è‡ªåŠ¨æ£€æµ‹ GQA é…ç½®å¤±è´¥ï¼Ÿ
**A**: æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦åŒ…å« `num_key_value_heads` å­—æ®µã€‚ä»£ç ä¼šè‡ªåŠ¨å¤„ç†ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥ä¼šç»™å‡ºè­¦å‘Šã€‚

### Q2: OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰ï¼Ÿ
**A**: å‡å°‘ `--nsamples`ï¼ˆå¦‚ 64 æˆ– 32ï¼‰ï¼Œæˆ–ä½¿ç”¨ `--use_gradient_checkpointing`ã€‚

### Q3: å¦‚ä½•é€‰æ‹©ç¨€ç–åº¦ï¼Ÿ
**A**:
- 20%: å¹³è¡¡æ€§èƒ½å’Œå‹ç¼©ç‡ï¼Œæ¨èèµ·ç‚¹
- 30%: éœ€è¦å¾®è°ƒæ¢å¤æ€§èƒ½
- 50%: æç«¯æµ‹è¯•ï¼Œå¿…é¡»å¾®è°ƒ

### Q4: Qwen å’Œ Mistral çš„å‰ªææ•ˆæœå·®å¼‚ï¼Ÿ
**A**: Qwen ä½¿ç”¨ 7:1 GQAï¼Œç†è®ºä¸Šæ¯ä¸ª KV head æ‰¿è½½æ›´å¤šä¿¡æ¯ï¼Œå‰ªææ—¶éœ€è¦æ›´è°¨æ…ã€‚å»ºè®®ä» 20% å¼€å§‹æµ‹è¯•ã€‚

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é¡¹ç›®æ€»è§ˆ](../README.md)
- [ä½¿ç”¨æŒ‡å—](../USAGE.md)
- [å¾®è°ƒæ–‡æ¡£](./FINETUNING.md)
- [é¡¹ç›®ç»“æ„](../PROJECT_STRUCTURE.md)

---

**æœ€åæ›´æ–°**: 2024-11-29
