# LoRA å¾®è°ƒæŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨ LoRA å¯¹å‰ªæåçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¢å¤æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [å‚æ•°è¯´æ˜](#å‚æ•°è¯´æ˜)
- [è¾“å‡ºè¯´æ˜](#è¾“å‡ºè¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install peft datasets transformers

# 2. å¾®è°ƒå‰ªæåçš„æ¨¡å‹
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --lora_r 8 \
    --num_epochs 2 \
    --learning_rate 1e-4 \
    --batch_size 64 \
    --micro_batch_size 4

# 3. æŸ¥çœ‹ç»“æœ
# å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åœ¨: results/HGSP_2000_finetuned/
# è¯„ä¼°ç»“æœ: results/HGSP_2000_finetuned/evaluation/evaluation_results.json
```

---

## ç¯å¢ƒè¦æ±‚

### å¿…éœ€ä¾èµ–

```bash
pip install peft>=0.5.0 datasets>=2.14.0 transformers>=4.33.0
```

### å¯é€‰ä¾èµ–

```bash
# WandB (ç”¨äºè®­ç»ƒç›‘æ§)
pip install wandb

# è¯„ä¼°å·¥å…·
pip install lm-eval>=0.4.0
```

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹å¤§å° | æ¨èæ˜¾å­˜ | æ¨èé…ç½® |
|---------|---------|---------|
| 2B å‚æ•° | 16GB | 1x RTX 3090 |
| 4B å‚æ•° | 24GB | 1x RTX 3090 Ti |
| 7B å‚æ•° | 40GB | 1x A100 40GB |

**æ³¨æ„**: LoRA å¾®è°ƒæ¯”å…¨å‚æ•°å¾®è°ƒèŠ‚çœçº¦ 70% æ˜¾å­˜

---

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ç”¨æ³•

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned
```

### 2. è‡ªå®šä¹‰ LoRA å‚æ•°

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --lora_r 16 \              # å¢å¤§ LoRA ç§©
    --lora_alpha 32 \          # è°ƒæ•´ç¼©æ”¾ç³»æ•°
    --lora_dropout 0.1         # å¢åŠ  dropout
```

### 3. è°ƒæ•´è®­ç»ƒå‚æ•°

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --num_epochs 3 \           # å¢åŠ è®­ç»ƒè½®æ•°
    --learning_rate 2e-4 \     # æé«˜å­¦ä¹ ç‡
    --batch_size 128 \         # å¢å¤§batch size
    --micro_batch_size 8       # å¢å¤§micro batch size
```

### 4. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path /path/to/your/dataset \
    --prompt_template_name custom
```

### 5. è·³è¿‡è‡ªåŠ¨è¯„ä¼°

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --skip_evaluation
```

### 6. ä½¿ç”¨ WandB ç›‘æ§

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --wandb_project "llama-pruning-finetune"
```

---

## å‚æ•°è¯´æ˜

### å¿…éœ€å‚æ•°

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `--pruned_model` | å‰ªææ¨¡å‹è·¯å¾„ | `results/HGSP_2000/pruned_model.bin` |

### æ•°æ®ç›¸å…³

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--data_path` | `yahma/alpaca-cleaned` | è®­ç»ƒæ•°æ®é›†è·¯å¾„ |
| `--val_set_size` | `2000` | éªŒè¯é›†å¤§å° |
| `--cutoff_len` | `256` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--prompt_template_name` | `alpaca` | æç¤ºè¯æ¨¡æ¿ |

### è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | å»ºè®®èŒƒå›´ |
|------|--------|------|----------|
| `--batch_size` | `64` | æ€»batch size | 32-128 |
| `--micro_batch_size` | `4` | æ¯GPUçš„batch size | 1-8 |
| `--num_epochs` | `2` | è®­ç»ƒè½®æ•° | 1-5 |
| `--learning_rate` | `1e-4` | å­¦ä¹ ç‡ | 5e-5 ~ 3e-4 |

**æ¢¯åº¦ç´¯ç§¯æ­¥æ•°** = `batch_size / micro_batch_size`

### LoRA é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | å»ºè®®èŒƒå›´ |
|------|--------|------|----------|
| `--lora_r` | `8` | LoRA ç§© | 4-32 |
| `--lora_alpha` | `16` | LoRA ç¼©æ”¾ç³»æ•° | 8-64 |
| `--lora_dropout` | `0.05` | LoRA dropout | 0.0-0.1 |
| `--lora_target_modules` | `q_proj,k_proj,v_proj,o_proj,gate_proj,down_proj,up_proj` | ç›®æ ‡æ¨¡å— | - |

**LoRA å‚æ•°å»ºè®®**:
- æ›´å¤§çš„æ¨¡å‹ â†’ æ›´å¤§çš„ `lora_r` (16-32)
- æ›´å°çš„æ¨¡å‹ â†’ æ›´å°çš„ `lora_r` (4-8)
- `lora_alpha` é€šå¸¸è®¾ä¸º `lora_r * 2`

### è¾“å‡ºç›¸å…³

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--output_dir` | `results/<model_name>_finetuned` | è¾“å‡ºç›®å½• |
| `--skip_evaluation` | `False` | è·³è¿‡è‡ªåŠ¨è¯„ä¼° |

### å…¶ä»–é€‰é¡¹

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--train_on_inputs` | `False` | åœ¨è¾“å…¥éƒ¨åˆ†è®¡ç®—loss |
| `--add_eos_token` | `False` | æ·»åŠ EOS token |
| `--group_by_length` | `False` | æŒ‰é•¿åº¦åˆ†ç»„ |
| `--wandb_project` | `""` | WandB é¡¹ç›®åç§° |
| `--resume_from_checkpoint` | `None` | ä»æ£€æŸ¥ç‚¹æ¢å¤ |

---

## è¾“å‡ºè¯´æ˜

### ç›®å½•ç»“æ„

å¾®è°ƒå®Œæˆåï¼Œè¾“å‡ºç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
results/HGSP_2000_finetuned/
â”œâ”€â”€ pruned_model.bin           # å¾®è°ƒåçš„å®Œæ•´æ¨¡å‹
â”œâ”€â”€ lora_adapter/              # LoRA adapter (å¯å•ç‹¬ä½¿ç”¨)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ tokenizer ç›¸å…³æ–‡ä»¶
â”œâ”€â”€ evaluation/                # è¯„ä¼°ç»“æœ
â”‚   â””â”€â”€ evaluation_results.json
â””â”€â”€ checkpoint-*/              # è®­ç»ƒæ£€æŸ¥ç‚¹ (å¯é€‰)
```

### ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

```python
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model_dict = torch.load('results/HGSP_2000_finetuned/pruned_model.bin')
model = model_dict['model']
tokenizer = model_dict['tokenizer']

# ä½¿ç”¨æ¨¡å‹
inputs = tokenizer("Hello, world!", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))
```

### è¯„ä¼°ç»“æœ

`evaluation/evaluation_results.json` åŒ…å«:
- PPL (WikiText-2, PTB)
- Zero-shot å‡†ç¡®ç‡ (BoolQ, PIQA, HellaSwag, ç­‰)
- æ¨ç†é€Ÿåº¦å’Œå»¶è¿Ÿ
- æ˜¾å­˜å ç”¨

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³

**é—®é¢˜**: CUDA out of memory

**è§£å†³æ–¹æ³•**:
```bash
# 1. å‡å° micro_batch_size
python finetune_lora.py ... --micro_batch_size 2

# 2. å‡å° cutoff_len
python finetune_lora.py ... --cutoff_len 128

# 3. å‡å° LoRA ç§©
python finetune_lora.py ... --lora_r 4
```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**è§£å†³æ–¹æ³•**:
```bash
# 1. å¢å¤§ micro_batch_size (å¦‚æœæ˜¾å­˜å…è®¸)
python finetune_lora.py ... --micro_batch_size 8

# 2. å¯ç”¨ group_by_length
python finetune_lora.py ... --group_by_length

# 3. å‡å°éªŒè¯é›†å¤§å°
python finetune_lora.py ... --val_set_size 1000
```

### Q3: Loss ä¸ä¸‹é™

**é—®é¢˜**: è®­ç»ƒæ—¶ loss ä¸ä¸‹é™

**è§£å†³æ–¹æ³•**:
```bash
# 1. æé«˜å­¦ä¹ ç‡
python finetune_lora.py ... --learning_rate 2e-4

# 2. å¢å¤§ LoRA ç§©
python finetune_lora.py ... --lora_r 16

# 3. å¢åŠ è®­ç»ƒè½®æ•°
python finetune_lora.py ... --num_epochs 5
```

### Q4: å¦‚ä½•åªä½¿ç”¨ LoRA adapter

**é—®é¢˜**: ä¸æƒ³ä¿å­˜å®Œæ•´æ¨¡å‹ï¼Œåªæƒ³è¦ LoRA adapter

**è§£å†³æ–¹æ³•**:

LoRA adapter å·²ç»ä¿å­˜åœ¨ `<output_dir>/lora_adapter/` ç›®å½•ä¸‹ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š

```python
from peft import PeftModel

# åŠ è½½åŸå§‹å‰ªææ¨¡å‹
pruned_dict = torch.load('results/HGSP_2000/pruned_model.bin')
base_model = pruned_dict['model']

# åŠ è½½ LoRA adapter
model = PeftModel.from_pretrained(base_model, 'results/HGSP_2000_finetuned/lora_adapter')
```

### Q5: è¯„ä¼°å¤±è´¥

**é—®é¢˜**: è‡ªåŠ¨è¯„ä¼°å¤±è´¥

**è§£å†³æ–¹æ³•**:

å¾®è°ƒå®Œæˆåæ‰‹åŠ¨è¿è¡Œè¯„ä¼°ï¼š

```bash
python evaluation/run_evaluation.py \
    --model_path results/HGSP_2000_finetuned/pruned_model.bin \
    --metrics all \
    --output results/HGSP_2000_finetuned/evaluation/evaluation_results.json
```

### Q6: ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†

**é—®é¢˜**: å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

**è§£å†³æ–¹æ³•**:

æ•°æ®é›†éœ€è¦ Alpaca æ ¼å¼çš„ JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- `instruction`: æŒ‡ä»¤
- `input`: è¾“å…¥ï¼ˆå¯é€‰ï¼‰
- `output`: æœŸæœ›è¾“å‡º

ç¤ºä¾‹:
```json
[
  {
    "instruction": "æ€»ç»“ä»¥ä¸‹æ–‡æœ¬",
    "input": "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬...",
    "output": "è¿™æ˜¯æ€»ç»“..."
  }
]
```

ç„¶åä½¿ç”¨ Hugging Face datasets åŠ è½½æˆ–æœ¬åœ°è·¯å¾„ã€‚

---

## é«˜çº§ç”¨æ³•

### å¤šGPUè®­ç»ƒ

```bash
# ä½¿ç”¨ torchrun (æ¨è)
torchrun --nproc_per_node=4 finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --batch_size 256 \
    --micro_batch_size 4

# æˆ–ä½¿ç”¨ accelerate
accelerate launch finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --resume_from_checkpoint results/HGSP_2000_finetuned/checkpoint-200
```

### å®Œæ•´çš„ç”Ÿäº§çº§å‘½ä»¤

```bash
python finetune_lora.py \
    --pruned_model results/HGSP_2000/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --output_dir results/HGSP_2000_finetuned_production \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --num_epochs 3 \
    --learning_rate 1e-4 \
    --batch_size 128 \
    --micro_batch_size 8 \
    --cutoff_len 512 \
    --val_set_size 2000 \
    --group_by_length \
    --wandb_project "llama-pruning-production"
```

---

## å‚è€ƒèµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)
- [LLM-Pruner](https://github.com/horseee/LLM-Pruner)

---

**ç»´æŠ¤è€…**: LLaMA Pruning Research Team
**æœ€åæ›´æ–°**: 2025-11-23
