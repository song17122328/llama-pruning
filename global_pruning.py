#!/usr/bin/env python3
"""
åŸºäºå…¨å±€æ€§ä»·æ¯”æ’åºçš„æ··åˆç»“æ„åŒ–å‰ªæ

æ ¸å¿ƒæ€æƒ³ï¼š
- å°†å‰ªæé—®é¢˜å»ºæ¨¡ä¸ºåˆ†æ•°èƒŒåŒ…é—®é¢˜
- Score = Importance / Cost
- å…¨å±€æ’åºï¼Œä¼˜å…ˆå‰ªé™¤"æ€§ä»·æ¯”"æœ€ä½çš„ groups
- è‡ªåŠ¨å®ç°æ·±åº¦å‰ªæï¼ˆå±‚ç§»é™¤ï¼‰+ å®½åº¦å‰ªæï¼ˆç¥ç»å…ƒå‰ªé™¤ï¼‰çš„æ··åˆç­–ç•¥
"""

import os
import torch
import argparse
import time
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

from core.methods.global_pruning import (
    build_global_group_table,
    select_groups_to_prune
)
from core.methods.gqa_aware import prune_attention_by_gqa_groups
from core.datasets.example_samples import get_examples
from evaluation.metrics.ppl import PPLMetric
from core.trainer.finetuner import FineTuner
from core.utils.logger import LoggerWithDepth


def collect_layer_activations(model, input_ids, device='cuda'):
    """
    æ”¶é›†æ¯å±‚çš„æ¿€æ´»å€¼ç”¨äº Wanda æ–¹æ³•

    Returns:
        activations: Dict[layer_idx -> Dict[name -> Tensor]]
    """
    activations = {}
    hooks = []

    def get_activation_hook(layer_idx, name):
        def hook(module, input, output):
            if layer_idx not in activations:
                activations[layer_idx] = {}
            # å­˜å‚¨è¾“å…¥æ¿€æ´»å€¼çš„å¹³å‡å€¼ï¼ˆç”¨äº Wandaï¼‰
            if isinstance(input, tuple):
                act = input[0].detach()
            else:
                act = input.detach()
            # è®¡ç®—æ‰€æœ‰ç»´åº¦çš„å¹³å‡ï¼ˆé™¤äº†æœ€åçš„ç‰¹å¾ç»´åº¦ï¼‰
            if act.dim() > 1:
                act = act.abs().mean(dim=tuple(range(act.dim() - 1)))
            activations[layer_idx][name] = act.cpu()
        return hook

    # ä¸ºæ¯å±‚çš„å…³é”®æ¨¡å—æ³¨å†Œ hooks
    for layer_idx, layer in enumerate(model.model.layers):
        # Attention çš„è¾“å…¥æ¿€æ´»
        hooks.append(layer.self_attn.q_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'q_proj')))
        hooks.append(layer.self_attn.k_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'k_proj')))
        hooks.append(layer.self_attn.v_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'v_proj')))
        hooks.append(layer.self_attn.o_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'o_proj')))

        # MLP çš„è¾“å…¥æ¿€æ´»
        hooks.append(layer.mlp.gate_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'mlp_input')))

        # MLP ä¸­é—´æ¿€æ´»ï¼ˆç”¨äº down_projï¼‰
        def get_mlp_intermediate_hook(layer_idx):
            def hook(module, input, output):
                if layer_idx not in activations:
                    activations[layer_idx] = {}
                act = output.detach().abs().mean(dim=tuple(range(output.dim() - 1)))
                activations[layer_idx]['intermediate'] = act.cpu()
            return hook

        hooks.append(layer.mlp.up_proj.register_forward_hook(
            get_mlp_intermediate_hook(layer_idx)))

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    with torch.no_grad():
        model(input_ids)

    # ç§»é™¤æ‰€æœ‰ hooks
    for hook in hooks:
        hook.remove()

    return activations


def apply_global_pruning(model, groups_to_prune_df, head_dim=128, gqa_ratio=4, logger=None):
    """
    æ ¹æ®å…¨å±€åˆ†æè¡¨æ‰§è¡Œå®é™…å‰ªæ

    Args:
        model: æ¨¡å‹
        groups_to_prune_df: è¦å‰ªæçš„ groups DataFrame
        head_dim: attention head ç»´åº¦
        gqa_ratio: Q:KV æ¯”ä¾‹
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        pruned_layers: è¢«å®Œå…¨å‰ªç©ºçš„å±‚åˆ—è¡¨
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log("\n" + "="*60)
    log("æ‰§è¡Œå…¨å±€å‰ªæ")
    log("="*60)

    num_layers = len(model.model.layers)
    pruning_stats = {
        'attention': {},  # {layer_idx: (old_kv, new_kv)}
        'mlp': {},        # {layer_idx: (old_channels, new_channels)}
        'empty_layers': []
    }

    # æŒ‰å±‚ç»„ç»‡è¦å‰ªæçš„ groups
    layer_prune_info = {}
    for layer_idx in range(num_layers):
        layer_data = groups_to_prune_df[groups_to_prune_df['layer_idx'] == layer_idx]

        attn_groups = layer_data[layer_data['group_type'] == 'attention']['group_idx'].tolist()
        mlp_groups = layer_data[layer_data['group_type'] == 'mlp']['group_idx'].tolist()

        layer_prune_info[layer_idx] = {
            'attention': attn_groups,
            'mlp': mlp_groups
        }

    # æ‰§è¡Œå‰ªæ
    for layer_idx in range(num_layers):
        layer = model.model.layers[layer_idx]
        prune_info = layer_prune_info[layer_idx]

        log(f"\nå¤„ç† Layer {layer_idx}:")

        # ========== Attention å‰ªæ ==========
        attn_prune_indices = prune_info['attention']

        if len(attn_prune_indices) > 0:
            # è·å–å½“å‰ KV heads æ•°é‡ï¼ˆä»æƒé‡å½¢çŠ¶æ¨æ–­ï¼‰
            k_proj_out_features = layer.self_attn.k_proj.out_features
            num_kv_heads = k_proj_out_features // head_dim

            # è®¡ç®—ä¿ç•™çš„ indices
            all_kv_indices = set(range(num_kv_heads))
            keep_kv_indices = sorted(list(all_kv_indices - set(attn_prune_indices)))

            # ä»æƒé‡å½¢çŠ¶è·å– Q heads æ•°é‡
            q_proj_out_features = layer.self_attn.q_proj.out_features
            old_q = q_proj_out_features // head_dim
            old_kv = num_kv_heads

            if len(keep_kv_indices) > 0:
                # æ‰§è¡Œå‰ªæ
                new_q, new_kv = prune_attention_by_gqa_groups(
                    layer,
                    keep_kv_indices,
                    head_dim=head_dim,
                    gqa_ratio=gqa_ratio
                )
                log(f"  Attention: {old_q}Q:{old_kv}KV â†’ {new_q}Q:{new_kv}KV")
                pruning_stats['attention'][layer_idx] = (old_kv, new_kv)
            else:
                # è¯¥å±‚ Attention è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ Attention è¢«å®Œå…¨å‰ªç©ºï¼ˆ{old_kv} â†’ 0 KV headsï¼‰")
                pruning_stats['attention'][layer_idx] = (old_kv, 0)

        # ========== MLP å‰ªæ ==========
        mlp_prune_indices = prune_info['mlp']

        if len(mlp_prune_indices) > 0:
            intermediate_size = layer.mlp.gate_proj.out_features

            # è®¡ç®—ä¿ç•™çš„ indices
            all_mlp_indices = set(range(intermediate_size))
            keep_mlp_indices = sorted(list(all_mlp_indices - set(mlp_prune_indices)))

            if len(keep_mlp_indices) > 0:
                # æ‰§è¡Œ MLP å‰ªæ
                keep_mlp_indices_tensor = torch.tensor(keep_mlp_indices, device=layer.mlp.gate_proj.weight.device)

                # å‰ªæ gate_proj å’Œ up_projï¼ˆä¿ç•™å¯¹åº”çš„è¡Œï¼‰
                layer.mlp.gate_proj.weight = torch.nn.Parameter(
                    layer.mlp.gate_proj.weight[keep_mlp_indices_tensor, :]
                )
                layer.mlp.up_proj.weight = torch.nn.Parameter(
                    layer.mlp.up_proj.weight[keep_mlp_indices_tensor, :]
                )

                # å‰ªæ down_projï¼ˆä¿ç•™å¯¹åº”çš„åˆ—ï¼‰
                layer.mlp.down_proj.weight = torch.nn.Parameter(
                    layer.mlp.down_proj.weight[:, keep_mlp_indices_tensor]
                )

                # æ›´æ–° intermediate_size
                new_intermediate_size = len(keep_mlp_indices)
                layer.mlp.gate_proj.out_features = new_intermediate_size
                layer.mlp.up_proj.out_features = new_intermediate_size
                layer.mlp.down_proj.in_features = new_intermediate_size

                log(f"  MLP: {intermediate_size} â†’ {new_intermediate_size} channels")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, new_intermediate_size)
            else:
                # è¯¥å±‚ MLP è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ MLP è¢«å®Œå…¨å‰ªç©ºï¼ˆ{intermediate_size} â†’ 0 channelsï¼‰")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, 0)

        # æ£€æŸ¥æ˜¯å¦æ•´å±‚è¢«å‰ªç©º
        attn_empty = (layer_idx in pruning_stats['attention'] and
                     pruning_stats['attention'][layer_idx][1] == 0)
        mlp_empty = (layer_idx in pruning_stats['mlp'] and
                    pruning_stats['mlp'][layer_idx][1] == 0)

        if attn_empty and mlp_empty:
            log(f"  ğŸ”´ Layer {layer_idx} è¢«å®Œå…¨å‰ªç©ºï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰")
            pruning_stats['empty_layers'].append(layer_idx)

    return pruning_stats


def remove_empty_layers(model, empty_layers, logger=None):
    """
    ç§»é™¤è¢«å®Œå…¨å‰ªç©ºçš„å±‚

    Args:
        model: æ¨¡å‹
        empty_layers: è¦ç§»é™¤çš„å±‚ç´¢å¼•åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    if len(empty_layers) == 0:
        log("\nâœ“ æ²¡æœ‰å±‚è¢«å®Œå…¨å‰ªç©ºï¼Œè·³è¿‡å±‚ç§»é™¤")
        return

    log(f"\n{'='*60}")
    log(f"ç§»é™¤å®Œå…¨å‰ªç©ºçš„å±‚")
    log(f"{'='*60}")
    log(f"è¦ç§»é™¤çš„å±‚: {empty_layers}")

    # åˆ›å»ºä¿ç•™çš„å±‚åˆ—è¡¨
    num_layers = len(model.model.layers)
    keep_layers = [i for i in range(num_layers) if i not in empty_layers]

    # é‡å»º layers åˆ—è¡¨
    new_layers = torch.nn.ModuleList([model.model.layers[i] for i in keep_layers])
    model.model.layers = new_layers

    # æ›´æ–°é…ç½®
    model.config.num_hidden_layers = len(keep_layers)

    log(f"âœ“ å±‚æ•°: {num_layers} â†’ {len(keep_layers)}")


def main():
    parser = argparse.ArgumentParser(description='åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_ckpt_log_name', type=str, default='llama_global_prune',
                       help='å®éªŒåç§°')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆç›¸å¯¹äºæ¨¡å‹æ€»å‚æ•°ï¼‰')
    parser.add_argument('--importance_method', type=str, default='taylor',
                       choices=['taylor', 'wanda', 'taylor_2nd'],
                       help='é‡è¦æ€§è®¡ç®—æ–¹æ³•: taylor(ä¸€é˜¶), wanda(æƒé‡Ã—æ¿€æ´»), taylor_2nd(äºŒé˜¶)')
    parser.add_argument('--num_samples', type=int, default=128,
                       help='ç”¨äºè®¡ç®—é‡è¦æ€§çš„æ ·æœ¬æ•°')
    parser.add_argument('--gradient_batch_size', type=int, default=4,
                       help='æ¢¯åº¦è®¡ç®—æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼ˆç”¨äºèŠ‚çœå†…å­˜ï¼‰')
    parser.add_argument('--seq_len', type=int, default=128,
                       help='æ ·æœ¬åºåˆ—é•¿åº¦ï¼ˆå‡å°å¯èŠ‚çœæ˜¾å­˜ï¼‰')
    parser.add_argument('--use_gradient_checkpointing', action='store_true',
                       help='ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ä½†ä¼šæ…¢ä¸€äº›ï¼‰')
    parser.add_argument('--remove_empty_layers', action='store_true',
                       help='æ˜¯å¦ç§»é™¤è¢«å®Œå…¨å‰ªç©ºçš„å±‚ï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰')
    parser.add_argument('--use_layer_weighting', action='store_true',
                       help='ä½¿ç”¨å±‚é‡è¦æ€§åŠ æƒè¯„åˆ†: Final_Score = Taylor_Score Ã— ln(1 + Removal_PPL)')
    parser.add_argument('--layer_weighting_samples', type=int, default=32,
                       help='ç”¨äºè®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦çš„æ ·æœ¬æ•°ï¼ˆä»…å½“ use_layer_weighting=True æ—¶æœ‰æ•ˆï¼‰')

    # GQA é…ç½®
    parser.add_argument('--head_dim', type=int, default=128,
                       help='Attention head ç»´åº¦')
    parser.add_argument('--gqa_ratio', type=int, default=4,
                       help='Q:KV æ¯”ä¾‹')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--test_before_prune', action='store_true',
                       help='å‰ªæå‰è¯„ä¼°åŸºçº¿ PPL')
    parser.add_argument('--test_after_prune', action='store_true',
                       help='å‰ªæåè¯„ä¼° PPL')

    # å¾®è°ƒå‚æ•°
    parser.add_argument('--finetune', action='store_true',
                       help='å‰ªæåè¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--finetune_method', type=str, default='lora',
                       choices=['full', 'lora'],
                       help='å¾®è°ƒæ–¹æ³•')
    parser.add_argument('--finetune_samples', type=int, default=500,
                       help='å¾®è°ƒæ ·æœ¬æ•°')
    parser.add_argument('--finetune_lr', type=float, default=1e-4,
                       help='å¾®è°ƒå­¦ä¹ ç‡')
    parser.add_argument('--finetune_epochs', type=int, default=1,
                       help='å¾®è°ƒè½®æ•°')
    parser.add_argument('--lora_r', type=int, default=8,
                       help='LoRA rank')
    parser.add_argument('--lora_alpha', type=int, default=16,
                       help='LoRA alpha')

    # ä¿å­˜å‚æ•°
    parser.add_argument('--save_model', action='store_true',
                       help='ä¿å­˜å‰ªæåçš„æ¨¡å‹')

    # å…¶ä»–
    from core.utils.get_best_gpu import get_best_gpu
    bestDevice = "cuda:"+str(get_best_gpu())  # è‡ªåŠ¨é€‰æ‹©æ˜¾å­˜æœ€å¤§çš„GPU
    # bestDevice = "cpu"  # å¦‚æœè¦ç”¨CPUï¼Œå–æ¶ˆæ³¨é‡Šè¿™è¡Œ
    parser.add_argument('--device', type=str, default=bestDevice,
                       help='è®¾å¤‡')
    parser.add_argument('--layer_start', type=int, default=0,
                       help='èµ·å§‹å±‚ï¼ˆdebugç”¨ï¼‰')
    parser.add_argument('--layer_end', type=int, default=None,
                       help='ç»“æŸå±‚ï¼ˆdebugç”¨ï¼‰')

    args = parser.parse_args()

    # è®¾ç½® logger
    logger = LoggerWithDepth(
        env_name=args.save_ckpt_log_name,
        config=args.__dict__,
        root_dir='prune_log'
    )

    logger.log("="*60)
    logger.log("åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ")
    logger.log("="*60)
    logger.log(f"æ¨¡å‹: {args.base_model}")
    logger.log(f"å‰ªæç‡: {args.pruning_ratio:.1%}")
    logger.log(f"é‡è¦æ€§æ–¹æ³•: {args.importance_method}")

    # ========== Step 1: åŠ è½½æ¨¡å‹ ==========
    logger.log("\n[Step 1] åŠ è½½æ¨¡å‹...")

    # æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
    if 'cpu' in args.device.lower():
        device_map = args.device
    else:
        # å•GPUï¼šç›´æ¥æŒ‡å®šè®¾å¤‡ï¼Œé¿å…å¤šGPUåˆ†å¸ƒ
        device_map = args.device

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map=device_map,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    if args.use_gradient_checkpointing:
        logger.log("  å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰...")
        model.gradient_checkpointing_enable()

    # è·å–å®é™…ä½¿ç”¨çš„è®¾å¤‡
    if hasattr(model, 'hf_device_map'):
        logger.log(f"  æ¨¡å‹åˆ†å¸ƒ: {model.hf_device_map}")
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—çš„è®¾å¤‡ï¼ˆè¾“å…¥æ•°æ®åº”è¯¥å‘é€åˆ°è¿™é‡Œï¼‰
        first_device = next(iter(model.hf_device_map.values()))
        args.device = f'cuda:{first_device}' if isinstance(first_device, int) else first_device
        logger.log(f"  è¾“å…¥è®¾å¤‡: {args.device}")
    else:
        args.device = next(model.parameters()).device

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    logger.log(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    logger.log(f"  æ€»å‚æ•°é‡: {total_params:,}")

    # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available() and 'cuda' in str(args.device).lower():
        device_str = str(args.device)
        gpu_id = int(device_str.split(':')[-1]) if ':' in device_str else 0
        allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
        reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3
        total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
        logger.log(f"  GPU æ˜¾å­˜: {allocated:.2f}GB / {total_mem:.2f}GB (å·²åˆ†é…)")
        logger.log(f"  GPU æ˜¾å­˜: {reserved:.2f}GB / {total_mem:.2f}GB (å·²é¢„ç•™)")

    # ========== Step 2: è¯„ä¼°åŸºçº¿ ==========
    if args.test_before_prune:
        logger.log("\n[Step 2] è¯„ä¼°åŸºçº¿ PPL...")
        baseline_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
        logger.log(f"âœ“ åŸºçº¿ PPL: {baseline_ppl}")

    # ========== Step 3: è®¡ç®—é‡è¦æ€§ï¼ˆæ¢¯åº¦æˆ–æ¿€æ´»ï¼‰ ==========
    activations = None
    hessian_diag = None

    if args.importance_method in ['taylor', 'taylor_2nd']:
        logger.log(f"\n[Step 3] è®¡ç®—æ¢¯åº¦ï¼ˆ{'ä¸€é˜¶' if args.importance_method == 'taylor' else 'äºŒé˜¶'} Taylor importanceï¼‰...")
        logger.log(f"  åŠ è½½ {args.num_samples} ä¸ªæ ·æœ¬...")

        # åˆ†æ‰¹è®¡ç®—æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜
        batch_size = args.gradient_batch_size
        num_batches = (args.num_samples + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        model.zero_grad()
        total_loss = 0.0
        start_time = time.time()

        # å¦‚æœåœ¨ CPU ä¸Šè¿è¡Œï¼Œç»™å‡ºæç¤º
        if 'cpu' in str(args.device).lower():
            logger.log(f"  âš ï¸ åœ¨ CPU ä¸Šè¿è¡Œï¼Œé€Ÿåº¦ä¼šéå¸¸æ…¢ï¼")
            logger.log(f"  é¢„è®¡æ¯ä¸ªæ‰¹æ¬¡éœ€è¦ 5-10 åˆ†é’Ÿï¼ˆå–å†³äº CPU æ€§èƒ½ï¼‰")
            logger.log(f"  æ€»é¢„è®¡æ—¶é—´: {num_batches * 7:.0f} åˆ†é’Ÿå·¦å³")
            logger.log("")

        # äºŒé˜¶æ³°å‹’éœ€è¦ç´¯ç§¯ Hessian å¯¹è§’çº¿è¿‘ä¼¼
        # âš ï¸ å­˜å‚¨åœ¨CPUä¸Šä»¥é¿å…GPU OOM
        if args.importance_method == 'taylor_2nd':
            hessian_diag = {}
            logger.log("  åˆå§‹åŒ– Hessian å¯¹è§’çº¿å­˜å‚¨ï¼ˆåœ¨CPUä¸Šä»¥èŠ‚çœGPUæ˜¾å­˜ï¼‰...")
            for name, param in model.named_parameters():
                if param.requires_grad:
                    # å­˜å‚¨åœ¨CPUä¸Šï¼Œé¿å…å ç”¨GPUæ˜¾å­˜
                    hessian_diag[name] = torch.zeros_like(param.data, device='cpu')

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(range(num_batches), desc="è®¡ç®—æ¢¯åº¦", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, args.num_samples)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # åŠ è½½å½“å‰æ‰¹æ¬¡
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åŠ è½½æ•°æ®...")
            input_ids = get_examples('c4', tokenizer, num_samples=current_batch_size, seq_len=args.seq_len)
            # input_ids = get_examples('wikitext', tokenizer, num_samples=current_batch_size, seq_len=args.seq_len)
            input_ids = input_ids.to(args.device)

            # å‰å‘ä¼ æ’­
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å‰å‘ä¼ æ’­...")
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss / num_batches  # å½’ä¸€åŒ–

            # åå‘ä¼ æ’­
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åå‘ä¼ æ’­...")
            loss.backward()

            # äºŒé˜¶æ³°å‹’ï¼šç´¯ç§¯ Hessian å¯¹è§’çº¿ï¼ˆä½¿ç”¨æ¢¯åº¦å¹³æ–¹è¿‘ä¼¼ï¼‰
            if args.importance_method == 'taylor_2nd':
                for name, param in model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        # å°†æ¢¯åº¦å¹³æ–¹ç§»åŠ¨åˆ°CPUåç´¯åŠ ï¼Œé¿å…GPU OOM
                        hessian_diag[name] += (param.grad ** 2).cpu() / num_batches

            batch_time = time.time() - batch_start_time
            total_loss += loss.item() * num_batches

            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å®Œæˆï¼è€—æ—¶: {batch_time:.2f}s, loss: {loss.item() * num_batches:.4f}")

            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({
                'loss': f'{loss.item() * num_batches:.4f}',
                'batch_time': f'{batch_time:.2f}s'
            })

            # æ¸…ç†å†…å­˜
            del input_ids, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¢¯åº¦è®¡ç®—å®Œæˆ")
        logger.log(f"  å¹³å‡ loss: {total_loss:.4f}")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

        if args.importance_method == 'taylor_2nd':
            logger.log(f"  âœ“ Hessian å¯¹è§’çº¿è¿‘ä¼¼è®¡ç®—å®Œæˆ")

    elif args.importance_method == 'wanda':
        logger.log(f"\n[Step 3] æ”¶é›†æ¿€æ´»å€¼ï¼ˆWanda importanceï¼‰...")
        logger.log(f"  åŠ è½½ {args.num_samples} ä¸ªæ ·æœ¬...")

        # åˆ†æ‰¹æ”¶é›†æ¿€æ´»
        batch_size = args.gradient_batch_size
        num_batches = (args.num_samples + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        all_activations = {}
        start_time = time.time()

        pbar = tqdm(range(num_batches), desc="æ”¶é›†æ¿€æ´»", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, args.num_samples)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # åŠ è½½å½“å‰æ‰¹æ¬¡
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åŠ è½½æ•°æ®...")
            input_ids = get_examples('c4', tokenizer, num_samples=current_batch_size, seq_len=args.seq_len)
            # input_ids = get_examples('wikitext', tokenizer, num_samples=current_batch_size, seq_len=args.seq_len)
            input_ids = input_ids.to(args.device)

            # æ”¶é›†æ¿€æ´»
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] æ”¶é›†æ¿€æ´»...")
            batch_activations = collect_layer_activations(model, input_ids, args.device)

            # ç´¯åŠ æ¿€æ´»å€¼
            for layer_idx, layer_acts in batch_activations.items():
                if layer_idx not in all_activations:
                    all_activations[layer_idx] = {}
                for name, act in layer_acts.items():
                    if name not in all_activations[layer_idx]:
                        all_activations[layer_idx][name] = act.to(args.device)
                    else:
                        all_activations[layer_idx][name] += act.to(args.device)

            batch_time = time.time() - batch_start_time
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å®Œæˆï¼è€—æ—¶: {batch_time:.2f}s")

            pbar.set_postfix({'batch_time': f'{batch_time:.2f}s'})

            del input_ids, batch_activations
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        # å¹³å‡æ¿€æ´»å€¼
        for layer_idx in all_activations:
            for name in all_activations[layer_idx]:
                all_activations[layer_idx][name] /= num_batches

        activations = all_activations

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¿€æ´»å€¼æ”¶é›†å®Œæˆ")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

    # ========== Step 3.5: è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰==========
    layer_removal_ppl = None
    if args.use_layer_weighting:
        logger.log(f"\n[Step 3.5] è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆç”¨äºåŠ æƒè¯„åˆ†ï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {args.layer_weighting_samples}")

        from core.importance.layer_analyzer import LayerImportanceAnalyzer

        # åŠ è½½ç”¨äºå±‚é‡è¦æ€§åˆ†æçš„æ ·æœ¬
        layer_texts = get_examples('wikitext', tokenizer, num_samples=args.layer_weighting_samples, seq_len=args.seq_len)

        # è½¬æ¢ä¸ºæ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äº LayerImportanceAnalyzerï¼‰
        layer_texts_list = []
        for i in range(layer_texts.size(0)):
            text = tokenizer.decode(layer_texts[i], skip_special_tokens=True)
            layer_texts_list.append(text)

        # åˆ›å»ºåˆ†æå™¨
        analyzer = LayerImportanceAnalyzer(model, tokenizer, device=args.device)

        # è®¡ç®—æ¯å±‚çš„ç§»é™¤å›°æƒ‘åº¦
        num_layers = len(model.model.layers)
        layer_removal_ppl = analyzer.measure_layer_importance_by_removal(
            texts=layer_texts_list,
            num_layers=num_layers
        )

        logger.log(f"âœ“ å±‚ç§»é™¤å›°æƒ‘åº¦è®¡ç®—å®Œæˆ")
        logger.log(f"  ç¤ºä¾‹ - Layer 0: Removal PPL = {layer_removal_ppl[0]:.4f}")
        logger.log(f"  ç¤ºä¾‹ - Layer {num_layers//2}: Removal PPL = {layer_removal_ppl[num_layers//2]:.4f}")
        logger.log(f"  ç¤ºä¾‹ - Layer {num_layers-1}: Removal PPL = {layer_removal_ppl[num_layers-1]:.4f}")

        # ä¿å­˜å±‚ç§»é™¤å›°æƒ‘åº¦åˆ°æ–‡ä»¶
        import json
        if not hasattr(logger, 'env_name'):
            logger.env_name = 'global_results'
        if not os.path.exists(logger.env_name):
            os.makedirs(logger.env_name, exist_ok=True)

        layer_ppl_path = os.path.join(logger.env_name, 'layer_removal_ppl.json')
        with open(layer_ppl_path, 'w') as f:
            json.dump(layer_removal_ppl, f, indent=2)
        logger.log(f"âœ“ å±‚ç§»é™¤å›°æƒ‘åº¦å·²ä¿å­˜: {layer_ppl_path}")

    # ========== Step 4: æ„å»ºå…¨å±€åˆ†æè¡¨ ==========
    logger.log("\n[Step 4] æ„å»ºå…¨å±€ Group åˆ†æè¡¨...")

    layer_end = args.layer_end if args.layer_end else len(model.model.layers)

    # ä¼ é€’é‡è¦æ€§ä¿¡æ¯
    importance_info = {}
    if args.importance_method in ['taylor', 'taylor_2nd']:
        importance_info['gradients'] = {name: param.grad for name, param in model.named_parameters() if param.grad is not None}
        if args.importance_method == 'taylor_2nd':
            importance_info['hessian_diag'] = hessian_diag
    elif args.importance_method == 'wanda':
        importance_info['activations'] = activations

    df = build_global_group_table(
        model=model,
        importance_method=args.importance_method,
        importance_info=importance_info,
        layer_start=args.layer_start,
        layer_end=layer_end,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        device=args.device,
        layer_removal_ppl=layer_removal_ppl  # ä¼ é€’å±‚ç§»é™¤å›°æƒ‘åº¦
    )

    logger.log(f"âœ“ åˆ†æè¡¨æ„å»ºå®Œæˆ")

    # ========== Step 5: é€‰æ‹©è¦å‰ªæçš„ groups ==========
    logger.log(f"\n[Step 5] æ ¹æ®å‰ªæç‡é€‰æ‹©è¦å‰ªæçš„ groups...")

    groups_to_prune = select_groups_to_prune(
        df=df,
        pruning_ratio=args.pruning_ratio,
        total_params=total_params
    )

    logger.log(f"âœ“ é€‰ä¸­ {len(groups_to_prune)} ä¸ª groups è¿›è¡Œå‰ªæ")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = 'global_results'
    if not hasattr(logger, 'env_name'):
        logger.env_name = output_dir
    if not os.path.exists(logger.env_name):
        os.makedirs(logger.env_name, exist_ok=True)

    # ä¿å­˜åˆ†æè¡¨ï¼ˆæŒ‰scoreæ’åºï¼‰
    table_path = os.path.join(logger.env_name, 'global_group_table.csv')
    df.to_csv(table_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {table_path}")

    prune_table_path = os.path.join(logger.env_name, 'groups_to_prune.csv')
    groups_to_prune.to_csv(prune_table_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {prune_table_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„åˆ†æè¡¨
    df_by_layer = df.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    table_by_layer_path = os.path.join(logger.env_name, 'global_group_table_by_layer.csv')
    df_by_layer.to_csv(table_by_layer_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {table_by_layer_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„å‰ªæåˆ—è¡¨
    prune_by_layer = groups_to_prune.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    prune_by_layer_path = os.path.join(logger.env_name, 'groups_to_prune_by_layer.csv')
    prune_by_layer.to_csv(prune_by_layer_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {prune_by_layer_path}")

    # ç”Ÿæˆå±‚çº§ç»Ÿè®¡æ‘˜è¦
    summary_lines = []
    summary_lines.append("="*80)
    summary_lines.append("å„å±‚å‰ªæç»Ÿè®¡æ‘˜è¦")
    summary_lines.append("="*80)
    summary_lines.append(f"{'Layer':<8} {'Attentionå‰ªæ':<20} {'MLPå‰ªæ':<20} {'æ€»å‚æ•°å‰ªæ':<20}")
    summary_lines.append("-"*80)

    for layer_idx in sorted(groups_to_prune['layer_idx'].unique()):
        layer_data = groups_to_prune[groups_to_prune['layer_idx'] == layer_idx]
        attn_data = layer_data[layer_data['group_type'] == 'attention']
        mlp_data = layer_data[layer_data['group_type'] == 'mlp']

        attn_count = len(attn_data)
        mlp_count = len(mlp_data)
        attn_params = attn_data['cost'].sum() if len(attn_data) > 0 else 0
        mlp_params = mlp_data['cost'].sum() if len(mlp_data) > 0 else 0
        total_params_prune = attn_params + mlp_params

        summary_lines.append(
            f"{layer_idx:<8} "
            f"{attn_count} groups ({attn_params:,} params)".ljust(20) + " "
            f"{mlp_count} channels ({mlp_params:,} params)".ljust(20) + " "
            f"{total_params_prune:,} params".ljust(20)
        )

    summary_lines.append("-"*80)
    summary_lines.append(f"æ€»è®¡: {len(groups_to_prune)} groups, "
                        f"{groups_to_prune['cost'].sum():,} params")
    summary_lines.append("="*80)

    # ä¿å­˜æ‘˜è¦æ–‡ä»¶
    summary_path = os.path.join(logger.env_name, 'pruning_summary_by_layer.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary_lines))
    logger.log(f"âœ“ å±‚çº§ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    # ä¹Ÿåœ¨æ—¥å¿—ä¸­æ˜¾ç¤º
    logger.log("\n" + '\n'.join(summary_lines))

    # ========== Step 6: æ‰§è¡Œå…¨å±€å‰ªæ ==========
    logger.log(f"\n[Step 6] æ‰§è¡Œå…¨å±€å‰ªæ...")

    pruning_stats = apply_global_pruning(
        model=model,
        groups_to_prune_df=groups_to_prune,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        logger=logger
    )

    logger.log("\nâœ“ å…¨å±€å‰ªæå®Œæˆ")

    # ========== Step 7: ç§»é™¤ç©ºå±‚ï¼ˆå¯é€‰ï¼‰==========
    if args.remove_empty_layers and len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\n[Step 7] ç§»é™¤ç©ºå±‚...")
        remove_empty_layers(model, pruning_stats['empty_layers'], logger)

    # ========== Step 8: ç»Ÿè®¡å‰ªæç»“æœ ==========
    logger.log(f"\n{'='*60}")
    logger.log(f"å‰ªæç»Ÿè®¡")
    logger.log(f"{'='*60}")

    after_params = sum(p.numel() for p in model.parameters())
    actual_ratio = (total_params - after_params) / total_params

    logger.log(f"å‚æ•°ç»Ÿè®¡:")
    logger.log(f"  å‰ªæå‰: {total_params:,}")
    logger.log(f"  å‰ªæå: {after_params:,}")
    logger.log(f"  å®é™…å‰ªæç‡: {actual_ratio:.2%}")

    if len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\nè‡ªåŠ¨æ·±åº¦å‰ªæ:")
        logger.log(f"  ç§»é™¤çš„å±‚: {pruning_stats['empty_layers']}")
        logger.log(f"  å‰©ä½™å±‚æ•°: {len(model.model.layers)}")

    # ========== Step 9: è¯„ä¼°å‰ªæå PPL ==========
    if args.test_after_prune:
        logger.log(f"\n[Step 9] è¯„ä¼°å‰ªæå PPL...")
        pruned_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
        logger.log(f"âœ“ å‰ªæå PPL: {pruned_ppl}")

        if args.test_before_prune:
            degradation = (pruned_ppl['wikitext2 (wikitext-2-raw-v1)'] /
                          baseline_ppl['wikitext2 (wikitext-2-raw-v1)'] - 1) * 100
            logger.log(f"  PPL é€€åŒ–: {degradation:.2f}%")

    # ========== Step 10: å¾®è°ƒæ¢å¤ï¼ˆå¯é€‰ï¼‰==========
    if args.finetune:
        logger.log(f"\n[Step 10] å¾®è°ƒæ¢å¤...")

        finetuner = FineTuner(model, tokenizer, device=args.device, logger=logger)

        finetuner.finetune(
            dataset_name='wikitext',
            num_samples=args.finetune_samples,
            lr=args.finetune_lr,
            epochs=args.finetune_epochs,
            method=args.finetune_method,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha
        )

        logger.log(f"âœ“ å¾®è°ƒå®Œæˆ")

        # è¯„ä¼°å¾®è°ƒå PPL
        if args.test_after_prune:
            logger.log(f"\nè¯„ä¼°å¾®è°ƒå PPL...")
            finetuned_ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], device=args.device)
            logger.log(f"âœ“ å¾®è°ƒå PPL: {finetuned_ppl}")

            if args.test_before_prune:
                final_degradation = (finetuned_ppl['wikitext2 (wikitext-2-raw-v1)'] /
                                    baseline_ppl['wikitext2 (wikitext-2-raw-v1)'] - 1) * 100
                logger.log(f"  æœ€ç»ˆ PPL é€€åŒ–: {final_degradation:.2f}%")

    # ========== Step 11: ä¿å­˜æ¨¡å‹ ==========
    if args.save_model:
        logger.log(f"\n[Step 11] ä¿å­˜æ¨¡å‹...")

        save_path = os.path.join(logger.env_name, 'pytorch_model.bin')

        save_dict = {
            'model': model,
            'tokenizer': tokenizer,
            'pruning_stats': pruning_stats,
            'pruning_ratio': args.pruning_ratio,
            'actual_ratio': actual_ratio,
            'method': 'global_pruning',
            'config': args.__dict__
        }

        torch.save(save_dict, save_path)
        logger.log(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {save_path}")

    logger.log(f"\n{'='*60}")
    logger.log(f"âœ“ å…¨éƒ¨å®Œæˆï¼")
    logger.log(f"{'='*60}")


if __name__ == '__main__':
    main()
