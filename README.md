# LLaMA Pruning Toolkit

é«˜æ•ˆçš„LLaMAæ¨¡å‹ç»“æ„åŒ–å‰ªæå·¥å…·ï¼Œæ”¯æŒå…¨å±€å‰ªæå’Œå±‚çº§å‰ªæä¸¤ç§æ–¹æ³•ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç»“æ„åŒ–åˆ†ç»„å‰ªæ**ï¼šåŸºäºé€šé“åˆ†ç»„çš„ç«¯åˆ°ç«¯å‰ªæç­–ç•¥
- ğŸ”¬ **å¤šç§é‡è¦æ€§åº¦é‡**ï¼šTaylorä¸€é˜¶/äºŒé˜¶ã€Wanda
- ğŸš€ **å…¨å±€ä¼˜åŒ–**ï¼šåŸºäºæ€§ä»·æ¯”çš„åˆ†æ•°èƒŒåŒ…å‰ªæ
- ğŸ”§ **å±‚çº§æ§åˆ¶**ï¼šéå‡è¡¡å‰ªæç­–ç•¥ï¼Œä¿æŠ¤é‡è¦å±‚
- ğŸ’ª **å¾®è°ƒæ¢å¤**ï¼šæ”¯æŒå…¨å‚æ•°å’ŒLoRAå¾®è°ƒ

## ğŸ§  æ ¸å¿ƒè®¾è®¡ï¼šåˆ†ç»„å‰ªæé€»è¾‘

æœ¬å·¥å…·é‡‡ç”¨**ç»“æ„åŒ–åˆ†ç»„å‰ªæ**ç­–ç•¥ï¼Œç¡®ä¿å‰ªæåæ¨¡å‹çš„ç»´åº¦ä¸€è‡´æ€§å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚

### 1ï¸âƒ£ Attention åˆ†ç»„ï¼ˆGQA-Awareï¼‰

åœ¨ Grouped Query Attention (GQA) æ¶æ„ä¸­ï¼Œå°†ç›¸å…³çš„ Q/K/V/O heads ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå‰ªæï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç¬¬ i ä¸ª GQA å‰ªæç»„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ 1 ä¸ª KV head (åŒ…å« K head + V head)          â”‚
â”‚  â€¢ å¯¹åº”çš„ 4 ä¸ª Q heads                          â”‚
â”‚  â€¢ å¯¹åº”çš„ 4 ä¸ª O heads                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¿æŒ 4:1 çš„ Q:KV æ¯”ä¾‹ä¸å˜
```

**å®ç°ç»†èŠ‚**ï¼š
- `q_proj`: å‰ªæè¾“å‡ºé€šé“ `[4Ã—head_dim]`
- `k_proj`: å‰ªæè¾“å‡ºé€šé“ `[head_dim]`
- `v_proj`: å‰ªæè¾“å‡ºé€šé“ `[head_dim]`
- `o_proj`: å‰ªæè¾“å…¥é€šé“ `[4Ã—head_dim]`ï¼ˆå¯¹åº” Q heads concat çš„ç»“æœï¼‰

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼Ÿ
- ä¿æŒ GQA çš„ 4:1 ç»“æ„çº¦æŸ
- ç¡®ä¿ Q heads å’Œ KV heads çš„è¯­ä¹‰å¯¹åº”å…³ç³»
- é¿å…ç»´åº¦ä¸åŒ¹é…å¯¼è‡´çš„æ¨ç†é”™è¯¯

### 2ï¸âƒ£ MLP åˆ†ç»„ï¼ˆé€šé“çº§ï¼‰

åœ¨ SwiGLU MLP ç»“æ„ä¸­ï¼Œå°† gate/up/down çš„å¯¹åº”é€šé“ä½œä¸ºä¸€ç»„å‰ªæï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç¬¬ i ä¸ª MLP å‰ªæç»„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  gate_proj[i, :]  hidden_dim â†’ ç¬¬iä¸ªè¾“å‡º        â”‚
â”‚  up_proj[i, :]    hidden_dim â†’ ç¬¬iä¸ªè¾“å‡º        â”‚
â”‚  down_proj[:, i]  ç¬¬iä¸ªè¾“å…¥ â†’ hidden_dim        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‰å‘ä¼ æ’­: x â†’ SwiGLU(gate[i], up[i]) â†’ down[:, i] â†’ out
```

**å®ç°ç»†èŠ‚**ï¼š
- `gate_proj.weight[i, :]`: ä¿ç•™/åˆ é™¤ç¬¬ i è¡Œï¼ˆè¾“å‡ºé€šé“ï¼‰
- `up_proj.weight[i, :]`: ä¿ç•™/åˆ é™¤ç¬¬ i è¡Œï¼ˆè¾“å‡ºé€šé“ï¼‰
- `down_proj.weight[:, i]`: ä¿ç•™/åˆ é™¤ç¬¬ i åˆ—ï¼ˆè¾“å…¥é€šé“ï¼‰

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼Ÿ
- ç¡®ä¿ `gate` å’Œ `up` çš„å¯¹åº”é€šé“ä¸€èµ·å‚ä¸ SwiGLU æ¿€æ´»
- ä¿è¯ `down` çš„è¾“å…¥ç»´åº¦ä¸å‰é¢çš„è¾“å‡ºå¯¹é½
- ç»´æŒå®Œæ•´çš„ç«¯åˆ°ç«¯è®¡ç®—è·¯å¾„

### ğŸ“ æ•°å­¦å½¢å¼

**Attention ç»„é‡è¦æ€§**ï¼š
```
I_attention(group_i) = I(Q_heads[4i:4i+4]) + I(K_head[i]) + I(V_head[i]) + I(O_heads[4i:4i+4])
```

**MLP ç»„é‡è¦æ€§**ï¼š
```
I_mlp(channel_i) = I(gate[i, :]) + I(up[i, :]) + I(down[:, i])
```

**å…¨å±€è¯„åˆ†ï¼ˆåˆ†æ•°èƒŒåŒ…ï¼‰**ï¼š
```
Score(group) = Importance(group) / Cost(group)
å‰ªæç­–ç•¥: é€‰æ‹© Score æœ€ä½çš„ groups è¿›è¡Œå‰ªæ
```

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <your-repo-url>
cd llama-pruning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**ä¾èµ–**ï¼štorch, transformers, datasets, peft, pandas

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šå…¨å±€å‰ªæï¼ˆæ¨èï¼‰

åŸºäºæ€§ä»·æ¯”å¾—åˆ†ï¼ˆImportance/Costï¼‰å…¨å±€é€‰æ‹©æœ€ä¼˜å‰ªæç­–ç•¥ã€‚

```bash
python run_global_pruning.py \
    --base_model /path/to/llama-3-8b \
    --save_ckpt_log_name my_experiment \
    --pruning_ratio 0.25 \
    --importance_method taylor \
    --num_samples 128 \
    --test_after_prune \
    --output_model pruned_model.bin
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `--pruning_ratio`: å‰ªæç‡ï¼ˆ0.25 = 25%ï¼‰
- `--importance_method`: taylorï¼ˆä¸€é˜¶ï¼‰/ taylor_2ndï¼ˆäºŒé˜¶ï¼‰/ wanda
- `--num_samples`: é‡è¦æ€§è¯„ä¼°æ ·æœ¬æ•°
- `--remove_empty_layers`: è‡ªåŠ¨ç§»é™¤å‰ªç©ºçš„å±‚ï¼ˆæ·±åº¦å‰ªæï¼‰

### æ–¹æ³•2ï¼šå±‚çº§å‰ªæï¼ˆä¼ ç»Ÿï¼‰

å…ˆè¯„ä¼°å±‚é‡è¦æ€§ï¼Œå†ä¸ºæ¯å±‚åˆ†é…å‰ªæç‡ã€‚

```bash
python layer_pruning.py \
    --base_model /path/to/llama-3-8b \
    --save_ckpt_log_name my_experiment \
    --pruning_ratio 0.25 \
    --pruning_distribution 2:8 \
    --pruning_strategy inverse \
    --test_after_prune \
    --save_model
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `--pruning_distribution`: Attention:MLPå‰ªææ¯”ä¾‹ï¼ˆå¦‚2:8ï¼‰
- `--pruning_strategy`: inverseï¼ˆé‡è¦å±‚å°‘å‰ªï¼‰/ uniformï¼ˆå‡åŒ€ï¼‰
- `--freeze_top_n_layers`: å†»ç»“æœ€é‡è¦çš„Nå±‚

## ğŸ“Š ä¸¤ç§æ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | å…¨å±€å‰ªæ | å±‚çº§å‰ªæ |
|------|---------|---------|
| **ä¼˜åŒ–ç›®æ ‡** | å…¨å±€æœ€ä¼˜ | å±‚çº§æœ€ä¼˜ |
| **Attn:MLP** | è‡ªåŠ¨å¹³è¡¡ | éœ€æ‰‹åŠ¨æŒ‡å®š |
| **æ·±åº¦å‰ªæ** | âœ… è‡ªåŠ¨ | âŒ |
| **è®¡ç®—æ—¶é—´** | è¾ƒæ…¢ | è¾ƒå¿« |
| **PPL** | æœ€ä¼˜ | è‰¯å¥½ |
| **æ¨èåœºæ™¯** | è¿½æ±‚æè‡´æ€§èƒ½ | å¿«é€ŸåŸå‹ |

**å…¸å‹ç»“æœ**ï¼ˆLLaMA-3-8Bï¼Œå‰ªæ25%ï¼‰ï¼š
- åŸå§‹æ¨¡å‹ï¼šPPL 12.3
- å…¨å±€å‰ªæï¼ˆtaylor_2ndï¼‰ï¼šPPL 58.9
- å±‚çº§å‰ªæï¼ˆ2:8, inverseï¼‰ï¼šPPL 83.8
- + LoRAå¾®è°ƒï¼šPPL 18.5

## ğŸ”§ å¾®è°ƒæ¢å¤

å‰ªæåä½¿ç”¨LoRAå¾®è°ƒæ¢å¤æ€§èƒ½ï¼š

```bash
# å…¨å±€å‰ªæ + LoRAå¾®è°ƒ
python run_global_pruning.py \
    --base_model /path/to/llama-3-8b \
    --pruning_ratio 0.25 \
    --finetune \
    --finetune_method lora \
    --lora_r 16 \
    --lora_alpha 32 \
    --finetune_samples 1000 \
    --finetune_lr 1e-4 \
    --test_after_prune \
    --output_model finetuned_model.bin
```

**å¾®è°ƒå‚æ•°**ï¼š
- `--finetune_method`: fullï¼ˆå…¨å‚æ•°ï¼‰/ loraï¼ˆæ¨èï¼‰
- `--lora_r`: LoRAç§©ï¼ˆ4-16ï¼‰
- `--lora_alpha`: ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸=2Ã—rï¼‰
- `--finetune_lr`: å­¦ä¹ ç‡ï¼ˆLoRAå»ºè®®1e-4ï¼Œå…¨å‚æ•°å»ºè®®1e-5ï¼‰

## ğŸ“ˆ è¯„ä¼°

```python
from evaluation.metrics.ppl import PPLMetric
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/path/to/pruned_model')
tokenizer = AutoTokenizer.from_pretrained('/path/to/llama-3-8b')

# è¯„ä¼°PPL
ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], seq_len=128, device='cuda')
print(ppl)  # {'wikitext2 (wikitext-2-raw-v1)': 58.9}
```

## ğŸ“‚ è¾“å‡ºæ–‡ä»¶

è¿è¡Œåç”Ÿæˆï¼š

```
prune_log/my_experiment/
â”œâ”€â”€ description.txt              # å®éªŒé…ç½®
â”œâ”€â”€ global_group_table.csv       # å…¨å±€åˆ†æè¡¨ï¼ˆä»…å…¨å±€å‰ªæï¼‰
â”œâ”€â”€ layer_importance_config.json # å±‚é‡è¦æ€§ï¼ˆä»…å±‚çº§å‰ªæï¼‰
â”œâ”€â”€ pruning_strategy.png         # å‰ªæç­–ç•¥å¯è§†åŒ–
â”œâ”€â”€ pytorch_model.bin            # å‰ªæåæ¨¡å‹
â””â”€â”€ YYYYMMDD_HHMMSS/
    â””â”€â”€ training.log             # è¯¦ç»†æ—¥å¿—
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### å‰ªæç‡é€‰æ‹©

| å‰ªæç‡ | æ¨èæ–¹æ³• | æ˜¯å¦å¾®è°ƒ | PPLé€€åŒ– |
|--------|---------|---------|---------|
| 15-20% | å…¨å±€/å±‚çº§å‡å¯ | å¯é€‰ | < 10% |
| 20-30% | å…¨å±€å‰ªæ | **æ¨è** | 10-30% |
| 30-40% | å…¨å±€å‰ªæ | **å¿…é¡»** | > 30% |

### é‡è¦æ€§æ–¹æ³•é€‰æ‹©

- **taylor**ï¼šå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ï¼Œå¤§å¤šæ•°åœºæ™¯æ¨è
- **taylor_2nd**ï¼šæœ€é«˜ç²¾åº¦ï¼Œæ„¿æ„ç‰ºç‰²è®¡ç®—æ—¶é—´æ—¶ä½¿ç”¨
- **wanda**ï¼šå¿«é€ŸåŸå‹éªŒè¯ï¼Œæ— éœ€æ¢¯åº¦è®¡ç®—

### å±‚çº§å‰ªæåˆ†å¸ƒæ¨è

å¯¹äºLLaMA-3-8Bï¼ˆAttentionå 19.2%ï¼ŒMLPå 80.8%ï¼‰ï¼š
- **2:8**ï¼šå‡è¡¡å‰ªæç‡ï¼ˆæ¨èï¼‰
- **0:10**ï¼šåªå‰ªMLPï¼Œä¿æŠ¤Attention
- **5:5**ï¼šç­‰é‡å‰ªæå‚æ•°

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### ä»…å‰ªæAttentionæˆ–MLP

```bash
# åªå‰ªMLP
python layer_pruning.py \
    --pruning_distribution 0:10 \
    --pruning_ratio 0.25

# åªå‰ªAttention
python layer_pruning.py \
    --pruning_distribution 10:0 \
    --pruning_ratio 0.25
```

### ä¿æŠ¤å…³é”®å±‚

```bash
# å†»ç»“æœ€é‡è¦çš„3å±‚
python layer_pruning.py \
    --freeze_top_n_layers 3 \
    --pruning_ratio 0.25
```

### æ·±åº¦å‰ªæï¼ˆè‡ªåŠ¨ç§»é™¤ç©ºå±‚ï¼‰

```bash
python run_global_pruning.py \
    --pruning_ratio 0.30 \
    --remove_empty_layers
```

## ğŸ› æ•…éšœæ’é™¤

**CUDA OOM**ï¼š
```bash
--num_samples 50             # å‡å°‘æ ·æœ¬æ•°
--gradient_batch_size 2      # å‡å°æ‰¹æ¬¡å¤§å°
--seq_len 64                 # å‡å°åºåˆ—é•¿åº¦
```

**PPLè¿‡é«˜**ï¼š
- é™ä½å‰ªæç‡ï¼ˆ0.15-0.20ï¼‰
- ä½¿ç”¨å…¨å±€å‰ªæè€Œéå±‚çº§å‰ªæ
- å¯ç”¨å¾®è°ƒæ¢å¤
- å°è¯•äºŒé˜¶Tayloré‡è¦æ€§

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{llama_pruning_toolkit,
  title={LLaMA Pruning Toolkit: GQA-Aware Structured Pruning},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/llama-pruning}}
}
```

## ğŸ“„ License

MIT License

---

## ğŸ”¬ æŠ€æœ¯å®ç°ç»†èŠ‚

### åˆ†ç»„å‰ªæä»£ç å®ç°

#### Attention åˆ†ç»„å‰ªæ (`core/methods/gqa_aware.py`)

```python
def prune_attention_by_gqa_groups(layer, keep_kv_indices, head_dim=128, gqa_ratio=4):
    """
    æ ¹æ®ä¿ç•™çš„ KV head ç´¢å¼•å‰ªææ•´ä¸ª GQA ç»„

    Args:
        keep_kv_indices: è¦ä¿ç•™çš„ KV head ç´¢å¼•åˆ—è¡¨ [0, 2, 5, ...]
    """
    # 1. è®¡ç®—å¯¹åº”çš„ Q head ç´¢å¼•
    keep_q_indices = []
    for kv_idx in keep_kv_indices:
        q_start = kv_idx * gqa_ratio  # ä¾‹å¦‚ KV[1] å¯¹åº” Q[4:8]
        keep_q_indices.extend(range(q_start, q_start + gqa_ratio))

    # 2. è½¬æ¢ä¸ºé€šé“ç´¢å¼•ï¼ˆhead â†’ channelï¼‰
    keep_q_channels = [range(q*head_dim, (q+1)*head_dim) for q in keep_q_indices]
    keep_kv_channels = [range(kv*head_dim, (kv+1)*head_dim) for kv in keep_kv_indices]

    # 3. å‰ªææƒé‡çŸ©é˜µ
    layer.self_attn.q_proj.weight = layer.self_attn.q_proj.weight[keep_q_channels, :]
    layer.self_attn.k_proj.weight = layer.self_attn.k_proj.weight[keep_kv_channels, :]
    layer.self_attn.v_proj.weight = layer.self_attn.v_proj.weight[keep_kv_channels, :]
    layer.self_attn.o_proj.weight = layer.self_attn.o_proj.weight[:, keep_q_channels]

    # 4. æ›´æ–°é…ç½®
    layer.self_attn.num_heads = len(keep_q_indices)
    layer.self_attn.num_key_value_heads = len(keep_kv_indices)
```

#### MLP åˆ†ç»„å‰ªæ (`run_global_pruning.py`)

```python
def prune_mlp_by_channels(layer, keep_channel_indices):
    """
    æ ¹æ®ä¿ç•™çš„é€šé“ç´¢å¼•å‰ªæ MLP

    Args:
        keep_channel_indices: è¦ä¿ç•™çš„ä¸­é—´å±‚é€šé“ç´¢å¼• [0, 5, 10, ...]
    """
    # 1. å‰ªæ gate_proj å’Œ up_proj çš„è¾“å‡ºé€šé“ï¼ˆè¡Œï¼‰
    layer.mlp.gate_proj.weight = layer.mlp.gate_proj.weight[keep_channel_indices, :]
    layer.mlp.up_proj.weight = layer.mlp.up_proj.weight[keep_channel_indices, :]

    # 2. å‰ªæ down_proj çš„è¾“å…¥é€šé“ï¼ˆåˆ—ï¼‰
    layer.mlp.down_proj.weight = layer.mlp.down_proj.weight[:, keep_channel_indices]

    # 3. æ›´æ–°é…ç½®
    new_intermediate_size = len(keep_channel_indices)
    layer.mlp.gate_proj.out_features = new_intermediate_size
    layer.mlp.up_proj.out_features = new_intermediate_size
    layer.mlp.down_proj.in_features = new_intermediate_size
```

### é‡è¦æ€§è®¡ç®—æ–¹æ³•

#### Taylor Expansion (ä¸€é˜¶)

```python
# å¯¹äºæ¯ä¸ªæƒé‡å‚æ•°
importance = |weight Ã— gradient|

# Attention ç»„: ç´¯åŠ æ‰€æœ‰ç›¸å…³çš„ projection å±‚
I_group = |W_q Ã— âˆ‡W_q| + |W_k Ã— âˆ‡W_k| + |W_v Ã— âˆ‡W_v| + |W_o Ã— âˆ‡W_o|

# MLP ç»„: ç´¯åŠ ä¸‰ä¸ª projection å±‚
I_channel = |W_gate[i] Ã— âˆ‡W_gate[i]| + |W_up[i] Ã— âˆ‡W_up[i]| + |W_down[:,i] Ã— âˆ‡W_down[:,i]|
```

#### Taylor Expansion (äºŒé˜¶)

```python
# å¢åŠ  Hessian å¯¹è§’çº¿é¡¹
importance = |weight Ã— gradient| + 0.5 Ã— |weightÂ² Ã— hessian_diag|

# Hessian å¯¹è§’çº¿è¿‘ä¼¼: âˆ‡Â²L â‰ˆ (âˆ‡L)Â²
```

#### Wanda (Weight Ã— Activation)

```python
# ä½¿ç”¨æ¿€æ´»å€¼ä»£æ›¿æ¢¯åº¦
importance = |weight Ã— activation|

# æ— éœ€åå‘ä¼ æ’­ï¼Œè®¡ç®—æ›´å¿«
```

### å…¨å±€å‰ªæç®—æ³•ï¼ˆåˆ†æ•°èƒŒåŒ…ï¼‰

```python
# 1. æ„å»ºå…¨å±€åˆ†æè¡¨
for layer in model.layers:
    for group in [attention_groups, mlp_groups]:
        importance = compute_importance(group)
        cost = count_parameters(group)
        score = importance / cost
        table.append((layer_id, group_id, score, cost))

# 2. æŒ‰ score æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
table.sort(key=lambda x: x['score'])

# 3. ç´¯åŠ æˆæœ¬ï¼Œç›´åˆ°è¾¾åˆ°å‰ªæç›®æ ‡
pruned_params = 0
for group in table:
    if pruned_params + group.cost <= target_pruned_params:
        prune_group(group)
        pruned_params += group.cost
    else:
        break  # è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢å‰ªæ
```

### å…³é”®è¶…å‚æ•°å¯¹ç…§è¡¨

| å‚æ•° | Attention åˆ†ç»„ | MLP åˆ†ç»„ |
|------|---------------|---------|
| **ç»„çš„å¤§å°** | 6ä¸ªçŸ©é˜µå— (Q/K/Vå„1ä¸ª + Oå„1ä¸ª) | 3ä¸ªå‘é‡ (gate/up/downå„1ä¸ª) |
| **head_dim** | 128 (LLaMA-3) | N/A |
| **gqa_ratio** | 4:1 (Q:KV) | N/A |
| **num_groups** | num_kv_heads (é€šå¸¸8) | intermediate_size (é€šå¸¸14336) |
| **cost/group** | ~1.6M å‚æ•° | ~12K å‚æ•° |

### ç»´åº¦å˜åŒ–ç¤ºä¾‹

**å‰ªæå‰** (LLaMA-3-8B):
```
Attention:
- num_q_heads = 32, num_kv_heads = 8, head_dim = 128
- q_proj: [4096, 4096]  (32 * 128 = 4096)
- k_proj: [1024, 4096]  (8 * 128 = 1024)
- v_proj: [1024, 4096]
- o_proj: [4096, 4096]

MLP:
- gate_proj: [14336, 4096]
- up_proj:   [14336, 4096]
- down_proj: [4096, 14336]
```

**å‰ªæå** (å‡è®¾å‰ªæ‰ 50% Attention å’Œ 30% MLP):
```
Attention:
- num_q_heads = 16, num_kv_heads = 4, head_dim = 128
- q_proj: [2048, 4096]  (16 * 128 = 2048)
- k_proj: [512, 4096]   (4 * 128 = 512)
- v_proj: [512, 4096]
- o_proj: [4096, 2048]  â† æ³¨æ„è¿™é‡Œæ˜¯è¾“å…¥é€šé“å˜åŒ–

MLP:
- gate_proj: [10035, 4096]  (14336 * 0.7 â‰ˆ 10035)
- up_proj:   [10035, 4096]
- down_proj: [4096, 10035]  â† æ³¨æ„è¿™é‡Œæ˜¯è¾“å…¥é€šé“å˜åŒ–
```

---

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `run_global_pruning.py` - å…¨å±€å‰ªæä¸»è„šæœ¬ï¼ˆæ¨èä½¿ç”¨ï¼‰
- `layer_pruning.py` - å±‚çº§å‰ªæä¸»è„šæœ¬
- `core/methods/global_pruning.py` - å…¨å±€å‰ªæç®—æ³•å®ç°
- `core/methods/gqa_aware.py` - GQA æ„ŸçŸ¥çš„ Attention åˆ†ç»„å‰ªæ
- `core/importance/layer_analyzer.py` - å±‚é‡è¦æ€§åˆ†æ
- `core/trainer/finetuner.py` - LoRA/å…¨å‚æ•°å¾®è°ƒ
- `evaluation/metrics/ppl.py` - å›°æƒ‘åº¦è¯„ä¼°

**ç›¸å…³è®ºæ–‡**ï¼š
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)
- [Wanda: A Simple and Effective Pruning Approach](https://arxiv.org/abs/2306.11695)
- [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning](https://arxiv.org/abs/2203.07259)