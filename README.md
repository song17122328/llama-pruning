# LLaMA Pruning Toolkit

é«˜æ•ˆçš„LLaMAæ¨¡å‹ç»“æ„åŒ–å‰ªæå·¥å…·ï¼Œæ”¯æŒå…¨å±€å‰ªæå’Œå±‚çº§å‰ªæä¸¤ç§æ–¹æ³•ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **GQAæ¶æ„æ„ŸçŸ¥**ï¼šè‡ªåŠ¨ç»´æŠ¤4:1 Q:KV headæ¯”ä¾‹
- ğŸ”¬ **å¤šç§é‡è¦æ€§åº¦é‡**ï¼šTaylorä¸€é˜¶/äºŒé˜¶ã€Wanda
- ğŸš€ **å…¨å±€ä¼˜åŒ–**ï¼šåŸºäºæ€§ä»·æ¯”çš„åˆ†æ•°èƒŒåŒ…å‰ªæ
- ğŸ”§ **å±‚çº§æ§åˆ¶**ï¼šéå‡è¡¡å‰ªæç­–ç•¥ï¼Œä¿æŠ¤é‡è¦å±‚
- ğŸ’ª **å¾®è°ƒæ¢å¤**ï¼šæ”¯æŒå…¨å‚æ•°å’ŒLoRAå¾®è°ƒ

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <your-repo-url>
cd llama-pruning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**ä¾èµ–**ï¼štorch, transformers, datasets, peft, pandas

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šå…¨å±€å‰ªæï¼ˆæ¨èï¼‰

åŸºäºæ€§ä»·æ¯”å¾—åˆ†ï¼ˆImportance/Costï¼‰å…¨å±€é€‰æ‹©æœ€ä¼˜å‰ªæç­–ç•¥ã€‚

```bash
python global_pruning.py \
    --base_model /path/to/llama-3-8b \
    --save_ckpt_log_name my_experiment \
    --pruning_ratio 0.25 \
    --importance_method taylor \
    --num_samples 128 \
    --test_after_prune \
    --save_model
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `--pruning_ratio`: å‰ªæç‡ï¼ˆ0.25 = 25%ï¼‰
- `--importance_method`: taylorï¼ˆä¸€é˜¶ï¼‰/ taylor_2ndï¼ˆäºŒé˜¶ï¼‰/ wanda
- `--num_samples`: é‡è¦æ€§è¯„ä¼°æ ·æœ¬æ•°
- `--remove_empty_layers`: è‡ªåŠ¨ç§»é™¤å‰ªç©ºçš„å±‚ï¼ˆæ·±åº¦å‰ªæï¼‰

### æ–¹æ³•2ï¼šå±‚çº§å‰ªæï¼ˆä¼ ç»Ÿï¼‰

å…ˆè¯„ä¼°å±‚é‡è¦æ€§ï¼Œå†ä¸ºæ¯å±‚åˆ†é…å‰ªæç‡ã€‚

```bash
python layer_pruning.py \
    --base_model /path/to/llama-3-8b \
    --save_ckpt_log_name my_experiment \
    --pruning_ratio 0.25 \
    --pruning_distribution 2:8 \
    --pruning_strategy inverse \
    --test_after_prune \
    --save_model
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `--pruning_distribution`: Attention:MLPå‰ªææ¯”ä¾‹ï¼ˆå¦‚2:8ï¼‰
- `--pruning_strategy`: inverseï¼ˆé‡è¦å±‚å°‘å‰ªï¼‰/ uniformï¼ˆå‡åŒ€ï¼‰
- `--freeze_top_n_layers`: å†»ç»“æœ€é‡è¦çš„Nå±‚

## ğŸ“Š ä¸¤ç§æ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | å…¨å±€å‰ªæ | å±‚çº§å‰ªæ |
|------|---------|---------|
| **ä¼˜åŒ–ç›®æ ‡** | å…¨å±€æœ€ä¼˜ | å±‚çº§æœ€ä¼˜ |
| **Attn:MLP** | è‡ªåŠ¨å¹³è¡¡ | éœ€æ‰‹åŠ¨æŒ‡å®š |
| **æ·±åº¦å‰ªæ** | âœ… è‡ªåŠ¨ | âŒ |
| **è®¡ç®—æ—¶é—´** | è¾ƒæ…¢ | è¾ƒå¿« |
| **PPL** | æœ€ä¼˜ | è‰¯å¥½ |
| **æ¨èåœºæ™¯** | è¿½æ±‚æè‡´æ€§èƒ½ | å¿«é€ŸåŸå‹ |

**å…¸å‹ç»“æœ**ï¼ˆLLaMA-3-8Bï¼Œå‰ªæ25%ï¼‰ï¼š
- åŸå§‹æ¨¡å‹ï¼šPPL 12.3
- å…¨å±€å‰ªæï¼ˆtaylor_2ndï¼‰ï¼šPPL 58.9
- å±‚çº§å‰ªæï¼ˆ2:8, inverseï¼‰ï¼šPPL 83.8
- + LoRAå¾®è°ƒï¼šPPL 18.5

## ğŸ”§ å¾®è°ƒæ¢å¤

å‰ªæåä½¿ç”¨LoRAå¾®è°ƒæ¢å¤æ€§èƒ½ï¼š

```bash
# å…¨å±€å‰ªæ + LoRAå¾®è°ƒ
python global_pruning.py \
    --base_model /path/to/llama-3-8b \
    --pruning_ratio 0.25 \
    --finetune \
    --finetune_method lora \
    --lora_r 16 \
    --lora_alpha 32 \
    --finetune_samples 1000 \
    --finetune_lr 1e-4 \
    --test_after_prune \
    --save_model
```

**å¾®è°ƒå‚æ•°**ï¼š
- `--finetune_method`: fullï¼ˆå…¨å‚æ•°ï¼‰/ loraï¼ˆæ¨èï¼‰
- `--lora_r`: LoRAç§©ï¼ˆ4-16ï¼‰
- `--lora_alpha`: ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸=2Ã—rï¼‰
- `--finetune_lr`: å­¦ä¹ ç‡ï¼ˆLoRAå»ºè®®1e-4ï¼Œå…¨å‚æ•°å»ºè®®1e-5ï¼‰

## ğŸ“ˆ è¯„ä¼°

```python
from evaluation.metrics.ppl import PPLMetric
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/path/to/pruned_model')
tokenizer = AutoTokenizer.from_pretrained('/path/to/llama-3-8b')

# è¯„ä¼°PPL
ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], seq_len=128, device='cuda')
print(ppl)  # {'wikitext2 (wikitext-2-raw-v1)': 58.9}
```

## ğŸ“‚ è¾“å‡ºæ–‡ä»¶

è¿è¡Œåç”Ÿæˆï¼š

```
prune_log/my_experiment/
â”œâ”€â”€ description.txt              # å®éªŒé…ç½®
â”œâ”€â”€ global_group_table.csv       # å…¨å±€åˆ†æè¡¨ï¼ˆä»…å…¨å±€å‰ªæï¼‰
â”œâ”€â”€ layer_importance_config.json # å±‚é‡è¦æ€§ï¼ˆä»…å±‚çº§å‰ªæï¼‰
â”œâ”€â”€ pruning_strategy.png         # å‰ªæç­–ç•¥å¯è§†åŒ–
â”œâ”€â”€ pytorch_model.bin            # å‰ªæåæ¨¡å‹
â””â”€â”€ YYYYMMDD_HHMMSS/
    â””â”€â”€ training.log             # è¯¦ç»†æ—¥å¿—
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### å‰ªæç‡é€‰æ‹©

| å‰ªæç‡ | æ¨èæ–¹æ³• | æ˜¯å¦å¾®è°ƒ | PPLé€€åŒ– |
|--------|---------|---------|---------|
| 15-20% | å…¨å±€/å±‚çº§å‡å¯ | å¯é€‰ | < 10% |
| 20-30% | å…¨å±€å‰ªæ | **æ¨è** | 10-30% |
| 30-40% | å…¨å±€å‰ªæ | **å¿…é¡»** | > 30% |

### é‡è¦æ€§æ–¹æ³•é€‰æ‹©

- **taylor**ï¼šå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ï¼Œå¤§å¤šæ•°åœºæ™¯æ¨è
- **taylor_2nd**ï¼šæœ€é«˜ç²¾åº¦ï¼Œæ„¿æ„ç‰ºç‰²è®¡ç®—æ—¶é—´æ—¶ä½¿ç”¨
- **wanda**ï¼šå¿«é€ŸåŸå‹éªŒè¯ï¼Œæ— éœ€æ¢¯åº¦è®¡ç®—

### å±‚çº§å‰ªæåˆ†å¸ƒæ¨è

å¯¹äºLLaMA-3-8Bï¼ˆAttentionå 19.2%ï¼ŒMLPå 80.8%ï¼‰ï¼š
- **2:8**ï¼šå‡è¡¡å‰ªæç‡ï¼ˆæ¨èï¼‰
- **0:10**ï¼šåªå‰ªMLPï¼Œä¿æŠ¤Attention
- **5:5**ï¼šç­‰é‡å‰ªæå‚æ•°

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### ä»…å‰ªæAttentionæˆ–MLP

```bash
# åªå‰ªMLP
python layer_pruning.py \
    --pruning_distribution 0:10 \
    --pruning_ratio 0.25

# åªå‰ªAttention
python layer_pruning.py \
    --pruning_distribution 10:0 \
    --pruning_ratio 0.25
```

### ä¿æŠ¤å…³é”®å±‚

```bash
# å†»ç»“æœ€é‡è¦çš„3å±‚
python layer_pruning.py \
    --freeze_top_n_layers 3 \
    --pruning_ratio 0.25
```

### æ·±åº¦å‰ªæï¼ˆè‡ªåŠ¨ç§»é™¤ç©ºå±‚ï¼‰

```bash
python global_pruning.py \
    --pruning_ratio 0.30 \
    --remove_empty_layers
```

## ğŸ› æ•…éšœæ’é™¤

**CUDA OOM**ï¼š
```bash
--num_samples 50             # å‡å°‘æ ·æœ¬æ•°
--gradient_batch_size 2      # å‡å°æ‰¹æ¬¡å¤§å°
--seq_len 64                 # å‡å°åºåˆ—é•¿åº¦
```

**PPLè¿‡é«˜**ï¼š
- é™ä½å‰ªæç‡ï¼ˆ0.15-0.20ï¼‰
- ä½¿ç”¨å…¨å±€å‰ªæè€Œéå±‚çº§å‰ªæ
- å¯ç”¨å¾®è°ƒæ¢å¤
- å°è¯•äºŒé˜¶Tayloré‡è¦æ€§

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{llama_pruning_toolkit,
  title={LLaMA Pruning Toolkit: GQA-Aware Structured Pruning},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/llama-pruning}}
}
```

## ğŸ“„ License

MIT License

---

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `global_pruning.py` - å…¨å±€å‰ªæä¸»è„šæœ¬
- `layer_pruning.py` - å±‚çº§å‰ªæä¸»è„šæœ¬
- `core/methods/global_pruning.py` - å…¨å±€å‰ªæç®—æ³•
- `core/methods/gqa_aware.py` - GQAæ„ŸçŸ¥å‰ªæ
- `core/importance/layer_analyzer.py` - å±‚é‡è¦æ€§åˆ†æ
- `core/trainer/finetuner.py` - LoRAå¾®è°ƒ
- `evaluation/metrics/ppl.py` - PPLè¯„ä¼°