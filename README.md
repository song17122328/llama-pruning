# LLaMA Pruning Toolkit

é«˜æ•ˆçš„ LLaMA / Qwen / Mistral æ¨¡å‹ç»“æ„åŒ–å‰ªæå·¥å…·ï¼ŒåŸºäºå…¨å±€æ€§ä»·æ¯”ä¼˜åŒ–çš„å‰ªæç­–ç•¥ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç»“æ„åŒ–åˆ†ç»„å‰ªæ**ï¼šGQA-aware ç«¯åˆ°ç«¯å‰ªæç­–ç•¥
- ğŸŒ **å¤šæ¨¡å‹æ”¯æŒ**ï¼šLLaMA-3-8Bã€Qwen2.5-7Bã€Mistral-7B-v0.3
- ğŸ”¬ **å¤šç§é‡è¦æ€§åº¦é‡**ï¼šTaylor ä¸€é˜¶/äºŒé˜¶ã€Magnitude
- ğŸš€ **å…¨å±€ä¼˜åŒ–**ï¼šåŸºäºæ€§ä»·æ¯”çš„åˆ†æ•°èƒŒåŒ…å‰ªæç®—æ³•
- ğŸ”§ **è‡ªåŠ¨é…ç½®æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«ä¸åŒæ¨¡å‹çš„ GQA æ¶æ„ï¼ˆ4:1 / 7:1ï¼‰
- ğŸ’ª **å¾®è°ƒæ¢å¤**ï¼šæ”¯æŒ LoRA å¾®è°ƒæ¢å¤æ€§èƒ½

## ğŸ§  æ ¸å¿ƒè®¾è®¡ï¼šåˆ†ç»„å‰ªæé€»è¾‘

æœ¬å·¥å…·é‡‡ç”¨**ç»“æ„åŒ–åˆ†ç»„å‰ªæ**ç­–ç•¥ï¼Œç¡®ä¿å‰ªæåæ¨¡å‹çš„ç»´åº¦ä¸€è‡´æ€§å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚

### 1ï¸âƒ£ Attention åˆ†ç»„ï¼ˆGQA-Awareï¼‰

åœ¨ Grouped Query Attention (GQA) æ¶æ„ä¸­ï¼Œå°†ç›¸å…³çš„ Q/K/V/O heads ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå‰ªæï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç¬¬ i ä¸ª GQA å‰ªæç»„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ 1 ä¸ª KV head (åŒ…å« K head + V head)          â”‚
â”‚  â€¢ å¯¹åº”çš„ 4 ä¸ª Q heads                          â”‚
â”‚  â€¢ å¯¹åº”çš„ 4 ä¸ª O heads                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¿æŒ 4:1 çš„ Q:KV æ¯”ä¾‹ä¸å˜
```

**å®ç°ç»†èŠ‚**ï¼š
- `q_proj`: å‰ªæè¾“å‡ºé€šé“ `[4Ã—head_dim]`
- `k_proj`: å‰ªæè¾“å‡ºé€šé“ `[head_dim]`
- `v_proj`: å‰ªæè¾“å‡ºé€šé“ `[head_dim]`
- `o_proj`: å‰ªæè¾“å…¥é€šé“ `[4Ã—head_dim]`ï¼ˆå¯¹åº” Q heads concat çš„ç»“æœï¼‰

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼Ÿ
- ä¿æŒ GQA çš„ 4:1 ç»“æ„çº¦æŸ
- ç¡®ä¿ Q heads å’Œ KV heads çš„è¯­ä¹‰å¯¹åº”å…³ç³»
- é¿å…ç»´åº¦ä¸åŒ¹é…å¯¼è‡´çš„æ¨ç†é”™è¯¯

### 2ï¸âƒ£ MLP åˆ†ç»„ï¼ˆé€šé“çº§ï¼‰

åœ¨ SwiGLU MLP ç»“æ„ä¸­ï¼Œå°† gate/up/down çš„å¯¹åº”é€šé“ä½œä¸ºä¸€ç»„å‰ªæï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ç¬¬ i ä¸ª MLP å‰ªæç»„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  gate_proj[i, :]  hidden_dim â†’ ç¬¬iä¸ªè¾“å‡º        â”‚
â”‚  up_proj[i, :]    hidden_dim â†’ ç¬¬iä¸ªè¾“å‡º        â”‚
â”‚  down_proj[:, i]  ç¬¬iä¸ªè¾“å…¥ â†’ hidden_dim        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‰å‘ä¼ æ’­: x â†’ SwiGLU(gate[i], up[i]) â†’ down[:, i] â†’ out
```

**å®ç°ç»†èŠ‚**ï¼š
- `gate_proj.weight[i, :]`: ä¿ç•™/åˆ é™¤ç¬¬ i è¡Œï¼ˆè¾“å‡ºé€šé“ï¼‰
- `up_proj.weight[i, :]`: ä¿ç•™/åˆ é™¤ç¬¬ i è¡Œï¼ˆè¾“å‡ºé€šé“ï¼‰
- `down_proj.weight[:, i]`: ä¿ç•™/åˆ é™¤ç¬¬ i åˆ—ï¼ˆè¾“å…¥é€šé“ï¼‰

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**ï¼Ÿ
- ç¡®ä¿ `gate` å’Œ `up` çš„å¯¹åº”é€šé“ä¸€èµ·å‚ä¸ SwiGLU æ¿€æ´»
- ä¿è¯ `down` çš„è¾“å…¥ç»´åº¦ä¸å‰é¢çš„è¾“å‡ºå¯¹é½
- ç»´æŒå®Œæ•´çš„ç«¯åˆ°ç«¯è®¡ç®—è·¯å¾„

### ğŸ“ æ•°å­¦å½¢å¼

**Attention ç»„é‡è¦æ€§**ï¼š
```
I_attention(group_i) = I(Q_heads[4i:4i+4]) + I(K_head[i]) + I(V_head[i]) + I(O_heads[4i:4i+4])
```

**MLP ç»„é‡è¦æ€§**ï¼š
```
I_mlp(channel_i) = I(gate[i, :]) + I(up[i, :]) + I(down[:, i])
```

**å…¨å±€è¯„åˆ†ï¼ˆåˆ†æ•°èƒŒåŒ…ï¼‰**ï¼š
```
Score(group) = Importance(group) / Cost(group)
å‰ªæç­–ç•¥: é€‰æ‹© Score æœ€ä½çš„ groups è¿›è¡Œå‰ªæ
```

## ğŸ“¦ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <your-repo-url>
cd llama-pruning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**ä¾èµ–**ï¼štorch, transformers, datasets, peft, pandas

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å…¨å±€å‰ªæï¼ˆGlobal Structural Pruningï¼‰

åŸºäºæ€§ä»·æ¯”å¾—åˆ†ï¼ˆImportance/Costï¼‰å…¨å±€é€‰æ‹©æœ€ä¼˜å‰ªæç­–ç•¥ã€‚

```bash
    
# Llama-3-8B-Instruct
python run_global_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --output_name Llama-3-8B-Instruct/Taylor_only_20 \
    --pruning_ratio 0.2 \
    --temperature 0.0 


# LLaMA-3-8B
python run_global_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B \
    --output_name LLaMA-3-8B/Taylor_only_20 \
    --pruning_ratio 0.2 \
    --temperature 0.0 


# Qwen2.5-7Bï¼ˆè‡ªåŠ¨æ£€æµ‹ GQA 7:1ï¼‰
python run_global_pruning.py \
    --base_model /newdata/LLMs/Qwen2.5-7B \
    --output_name Qwen2.5-7B/Taylor_only_20 \
    --pruning_ratio 0.2 \
    --temperature 1.0 \


# Mistral-7B-v0.3ï¼ˆè‡ªåŠ¨æ£€æµ‹ GQA 4:1ï¼‰
python run_global_pruning.py \
    --base_model /newdata/LLMs/Mistral-7B-v0.3 \
    --output_name Mistral-7B-v0.3/blockwise_20_c4 \
    --pruning_ratio 0.2 \
    --importance_method taylor \
    --temperature 1.0 --tau 0 --dataset c4 

python run_global_pruning.py \
    --base_model /newdata/LLMs/Mistral-7B-v0.3 \
    --output_name Mistral-7B-v0.3/layerwise_20_c4_loss \
    --pruning_ratio 0.2 \
    --importance_method taylor \
    --temperature 1.0 --tau inf --dataset c4 

python run_global_pruning.py \
    --base_model /newdata/LLMs/Mistral-7B-v0.3 \
    --output_name Mistral-7B-v0.3/Taylor_2nd_20 \
    --pruning_ratio 0.2 \
    --importance_method taylor_2nd \
    --temperature 0 --dataset c4 


python run_global_pruning.py \
    --base_model /newdata/LLMs/Mistral-7B-v0.3 \
    --output_name Mistral-7B-v0.3/Taylor_only_20_c4 \
    --pruning_ratio 0.2 \
    --importance_method taylor \
    --dataset c4 \
    --temperature 0.0 
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `--pruning_ratio`: ç›®æ ‡å‰ªæç‡ï¼ˆ0.2 = 20%ï¼‰
- `--importance_method`: taylorï¼ˆä¸€é˜¶ï¼Œé»˜è®¤ï¼‰/ taylor_2ndï¼ˆäºŒé˜¶ï¼‰/ wanda / magnitude
- `--dataset`: æ ¡å‡†æ•°æ®é›†ï¼ˆwikitext2 / ptb / c4ï¼Œé»˜è®¤ wikitext2ï¼‰
- `--temperature`: H-GSP æ¸©åº¦å‚æ•° Tï¼ˆé»˜è®¤ 1.0ï¼‰
  - `T=0`: çº¯ Taylor æ¨¡å¼ï¼ˆè·³è¿‡å±‚/å—é‡è¦æ€§åˆ†æï¼Œæœ€å¿«ï¼‰
  - `T=1`: æ¨èæ¨¡å¼ï¼ˆå¹³è¡¡åŸºç¡€æ–¹æ³•ä¸å±‚çº§å…ˆéªŒï¼‰
  - `T>1`: æ¿€è¿›æ¨¡å¼ï¼ˆå¼ºåŒ–é¦–å°¾ä¿æŠ¤ï¼‰
- `--tau`: H-GSP é—¨æ§é˜ˆå€¼ Ï„ï¼ˆé»˜è®¤ None è‡ªåŠ¨è®¡ç®—ï¼‰
  - `tau=0`: çº¯ Block-wise æ¨¡å¼ï¼ˆåªä½¿ç”¨å—çº§é‡è¦æ€§ï¼‰
  - `tau=None`: è‡ªåŠ¨æ¨¡å¼ï¼ˆè®¡ç®—25åˆ†ä½æ•°ï¼Œæ¨èï¼‰
  - `tau=inf`: çº¯ Layer-wise æ¨¡å¼ï¼ˆåªä½¿ç”¨å±‚çº§é‡è¦æ€§ï¼‰
- `--epsilon`: H-GSP åç¼©é˜ˆå€¼ Îµï¼ˆé»˜è®¤ 0ï¼‰
- `--freeze_first_n_layers`: å†»ç»“å‰Nå±‚ä¸å‰ªæï¼ˆé»˜è®¤ 0ï¼‰
- `--freeze_last_n_layers`: å†»ç»“åNå±‚ä¸å‰ªæï¼ˆé»˜è®¤ 0ï¼‰

**å…¸å‹ç»“æœ**ï¼ˆLLaMA-3-8Bï¼‰ï¼š
- åŸå§‹æ¨¡å‹ï¼šWikiText-2 PPL ~12.3
- 20% å‰ªæï¼šPPL ~58.9
- 30% å‰ªæï¼šPPL ~83.8
- + LoRA å¾®è°ƒï¼šPPL ~18.5

## ğŸ”§ å¾®è°ƒæ¢å¤

å‰ªæåä½¿ç”¨ LoRA å¾®è°ƒæ¢å¤æ€§èƒ½ï¼š

```bash
# å‰ªæ + å¾®è°ƒï¼ˆé›†æˆï¼‰
python run_global_pruning.py \
    --base_model Qwen/Qwen2.5-7B \
    --output_name Qwen2.5-7B/prune_20_finetune \
    --pruning_ratio 0.2 \
    --finetune \
    --finetune_data_path yahma/alpaca-cleaned \
    --finetune_epochs 3 \
    --finetune_lr 3e-4 \
    --lora_r 8 \
    --lora_alpha 16 \
    --device cuda:0

# æˆ–ä½¿ç”¨ç‹¬ç«‹å¾®è°ƒè„šæœ¬
python finetune_lora.py \
    --pruned_model results/Qwen2.5-7B/prune_20/pruned_model.bin \
    --data_path yahma/alpaca-cleaned \
    --output_dir results/Qwen2.5-7B/prune_20_finetuned \
    --num_epochs 3 \
    --learning_rate 3e-4 \
    --lora_r 8 \
    --lora_alpha 16 \
    --device cuda:0
```

**å¾®è°ƒå‚æ•°**ï¼š
- `--lora_r`: LoRA ç§©ï¼ˆæ¨è 8-16ï¼‰
- `--lora_alpha`: ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸ = 2Ã—rï¼‰
- `--finetune_lr`: å­¦ä¹ ç‡ï¼ˆæ¨è 3e-4ï¼‰
- `--finetune_epochs`: å¾®è°ƒè½®æ•°ï¼ˆæ¨è 3-5ï¼‰

## ğŸ¯ H-GSP æ–¹æ³•è¯¦è§£

### æ ¸å¿ƒæ€æƒ³

H-GSP (Hierarchical Global Structural Pruning) æ˜¯ä¸€ç§åˆ†å±‚æ¬¡çš„å…¨å±€ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œç»“åˆäº†**å…¨å±€ Taylor é‡è¦æ€§**å’Œ**å±‚çº§/å—çº§å…ˆéªŒçŸ¥è¯†**ã€‚

### è¯„åˆ†å…¬å¼

```
åŸºç¡€è¯„åˆ†: S_base = Importance / Cost

æ··åˆåŠ æƒ: S_final = S_base Ã— M

å…¶ä¸­: M = B^T
      B = ln(1 + importance_prior)
      T = temperature (æ¸©åº¦å‚æ•°)
```

### å‚æ•°è¯¦è§£

#### 1. Temperature (æ¸©åº¦ T)

æ§åˆ¶å±‚çº§å…ˆéªŒçš„å½±å“å¼ºåº¦ï¼š

- **T=0** (çº¯ Taylor æ¨¡å¼)
  ```bash
  python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --temperature 0.0  # æœ€å¿«ï¼Œè·³è¿‡å±‚/å—é‡è¦æ€§åˆ†æ
  ```
  - âœ… åªä½¿ç”¨å…¨å±€ Taylor é‡è¦æ€§
  - âœ… æœ€å¿«ï¼ˆè·³è¿‡ Step 3.5-3.6ï¼‰
  - âœ… é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼ˆæ— å…¼å®¹æ€§é—®é¢˜ï¼‰
  - âš ï¸ ä¸è€ƒè™‘å±‚çº§ç»“æ„

- **T=1** (æ¨èæ¨¡å¼)
  ```bash
  python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --temperature 1.0  # æ¨èï¼Œå¹³è¡¡æ€§èƒ½
  ```
  - âœ… å¹³è¡¡åŸºç¡€æ–¹æ³•ä¸å±‚çº§å…ˆéªŒ
  - âœ… è‡ªåŠ¨ä¿æŠ¤é‡è¦å±‚çš„é¦–å°¾
  - âœ… ä½¿ç”¨ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆShortGPTï¼‰

- **T>1** (æ¿€è¿›æ¨¡å¼)
  - å¼ºåŒ–é¦–å°¾ä¿æŠ¤ï¼Œæ›´æ¿€è¿›åœ°å‰ªæä¸­é—´å±‚

#### 2. Tau (é—¨æ§é˜ˆå€¼ Ï„)

æ§åˆ¶ Layer-wise å’Œ Block-wise æ¨¡å¼çš„åˆ‡æ¢ï¼š

- **tau=0** (çº¯ Block-wise)
  ```bash
  python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --temperature 1.0 \
    --tau 0  # å¼ºåˆ¶ä½¿ç”¨å—çº§é‡è¦æ€§
  ```
  - æ‰€æœ‰å±‚éƒ½ä½¿ç”¨ Attention/MLP å—çº§é‡è¦æ€§
  - ç²¾ç»†åŒ–å‰ªæç­–ç•¥

- **tau=None** (è‡ªåŠ¨æ¨¡å¼ï¼Œæ¨è)
  ```bash
  python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --temperature 1.0
    # tau é»˜è®¤ Noneï¼Œè‡ªåŠ¨è®¡ç®—
  ```
  - è‡ªåŠ¨è®¡ç®— Ï„ = 25åˆ†ä½æ•°(å±‚é‡è¦æ€§)
  - ä½äº Ï„ çš„å±‚ â†’ Layer-Dominant æ¨¡å¼
  - é«˜äº Ï„ çš„å±‚ â†’ Block-Dominant æ¨¡å¼

- **tau=inf** (çº¯ Layer-wise)
  ```bash
  python run_global_pruning.py \
    --base_model /path/to/model \
    --pruning_ratio 0.2 \
    --temperature 1.0 \
    --tau inf  # å¼ºåˆ¶ä½¿ç”¨å±‚çº§é‡è¦æ€§
  ```
  - æ‰€æœ‰å±‚éƒ½ä½¿ç”¨å±‚çº§é‡è¦æ€§
  - é¼“åŠ±æ•´å±‚ç§»é™¤

#### 3. å±‚å†»ç»“å‚æ•°

ä¿æŠ¤æ¨¡å‹çš„é¦–å°¾å±‚ä¸è¢«å‰ªæï¼š

```bash
python run_global_pruning.py \
  --base_model /path/to/model \
  --pruning_ratio 0.2 \
  --freeze_first_n_layers 2  # å†»ç»“å‰2å±‚
  --freeze_last_n_layers 2   # å†»ç»“å2å±‚
```

### é‡è¦æ€§è®¡ç®—æ–¹æ³•

**ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆShortGPTï¼Œé»˜è®¤ï¼‰**ï¼š
- å±‚é‡è¦æ€§ = 1 - cosine_similarity(å±‚è¾“å…¥, å±‚è¾“å‡º)
- å—é‡è¦æ€§ = 1 - cosine_similarity(å—è¾“å…¥, å—è¾“å‡º)
- âœ… å¯¹æ‰€æœ‰æ¨¡å‹é€šç”¨ï¼ˆQwenã€Mistralç­‰ï¼‰
- âœ… æ— éœ€ç§»é™¤å±‚ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
- âœ… è®¡ç®—é«˜æ•ˆ

### ä½¿ç”¨å»ºè®®

1. **å¿«é€Ÿå®éªŒ**ï¼šä½¿ç”¨ `--temperature 0.0`ï¼ˆçº¯ Taylorï¼‰
2. **æœ€ä½³æ€§èƒ½**ï¼šä½¿ç”¨ `--temperature 1.0`ï¼ˆH-GSPï¼Œæ¨èï¼‰
3. **ä¿æŠ¤é¦–å°¾**ï¼šä½¿ç”¨ `--freeze_first_n_layers` å’Œ `--freeze_last_n_layers`

## ğŸ“ˆ è¯„ä¼°

```python
from evaluation.metrics.ppl import PPLMetric
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/path/to/pruned_model')
tokenizer = AutoTokenizer.from_pretrained('/path/to/llama-3-8b')

# è¯„ä¼°PPL
ppl = PPLMetric(model, tokenizer, datasets=['wikitext2'], seq_len=128, device='cuda')
print(ppl)  # {'wikitext2 (wikitext-2-raw-v1)': 58.9}
```

## ğŸ“‚ è¾“å‡ºæ–‡ä»¶

è¿è¡Œåç”Ÿæˆï¼š

```
results/{output_name}/
â”œâ”€â”€ pruned_model.bin             # å‰ªæåæ¨¡å‹æƒé‡
â”œâ”€â”€ config.json                  # æ¨¡å‹é…ç½®
â”œâ”€â”€ pruning_analysis.json        # å‰ªæåˆ†ææŠ¥å‘Š
â”œâ”€â”€ global_group_table.csv       # å…¨å±€åˆ†ç»„è¡¨
â””â”€â”€ logs/
    â””â”€â”€ training.log             # è¯¦ç»†æ—¥å¿—
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### ç¨€ç–åº¦é€‰æ‹©

| ç¨€ç–åº¦ | æ˜¯å¦å¾®è°ƒ | PPL é€€åŒ– | é€‚ç”¨åœºæ™¯ |
|--------|---------|----------|----------|
| 10-20% | å¯é€‰ | < 10% | å¿«é€Ÿå‹ç¼© |
| 20-30% | **æ¨è** | 10-30% | å¹³è¡¡æ€§èƒ½ |
| 30-50% | **å¿…é¡»** | > 30% | æé™å‹ç¼© |

### é‡è¦æ€§åº¦é‡é€‰æ‹©

- **taylor_fo**ï¼šTaylor ä¸€é˜¶ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ï¼ˆæ¨èï¼‰
- **taylor_so**ï¼šTaylor äºŒé˜¶ï¼Œæœ€é«˜ç²¾åº¦ï¼Œè®¡ç®—è¾ƒæ…¢
- **magnitude**ï¼šæƒé‡å¤§å°ï¼Œå¿«é€ŸåŸå‹éªŒè¯

### å¤šæ¨¡å‹æµ‹è¯•å»ºè®®

- **LLaMA-3-8B**ï¼šåŸºå‡†æ¨¡å‹ï¼ŒGQA 4:1
- **Mistral-7B-v0.3**ï¼šéªŒè¯ç›¸åŒ GQA æ¯”ä¾‹ï¼ˆ4:1ï¼‰çš„æ³›åŒ–æ€§
- **Qwen2.5-7B**ï¼šéªŒè¯ä¸åŒ GQA æ¯”ä¾‹ï¼ˆ7:1ï¼‰çš„é€‚åº”æ€§

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

```bash
python run_global_pruning.py \
    --base_model Qwen/Qwen2.5-7B \
    --output_name Qwen2.5-7B/prune_20 \
    --pruning_ratio 0.2 \
    --use_gradient_checkpointing \
    --device cuda:0
```

### ä½¿ç”¨ Taylor äºŒé˜¶ï¼ˆæ›´ç²¾ç¡®ï¼‰

```bash
python run_global_pruning.py \
    --base_model Qwen/Qwen2.5-7B \
    --output_name Qwen2.5-7B/prune_20_taylor2nd \
    --pruning_ratio 0.2 \
    --importance_method taylor_2nd \
    --device cuda:0
```


```bash
python run_global_pruning.py \
    --base_model Qwen/Qwen2.5-7B \
    --output_name Qwen2.5-7B/prune_30 \
    --pruning_ratio 0.3 \
    --temperature 1.5 \
    --epsilon 0.2 \
    --device cuda:0
```

## ğŸ› æ•…éšœæ’é™¤

**CUDA OOM**ï¼š
```bash
--use_gradient_checkpointing     # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
--gradient_batch_size 2          # å‡å°æ‰¹æ¬¡å¤§å°
```

**PPL è¿‡é«˜**ï¼š
- é™ä½å‰ªæç‡ï¼ˆ10-20%ï¼‰
- ä½¿ç”¨ Taylor äºŒé˜¶ï¼ˆ`--importance_method taylor_2nd`ï¼‰
- å¯ç”¨å¾®è°ƒæ¢å¤ï¼ˆ`--finetune`ï¼‰
- è°ƒæ•´æ¸©åº¦å‚æ•°ï¼ˆ`--temperature 1.5`ï¼‰

**è‡ªåŠ¨é…ç½®æ£€æµ‹å¤±è´¥**ï¼š
- æ£€æŸ¥æ¨¡å‹ config ä¸­æ˜¯å¦æœ‰ `num_key_value_heads` å­—æ®µ
- ä»£ç ä¼šè‡ªåŠ¨å›é€€åˆ° MHA æ¨¡å¼ï¼ˆQ heads = KV headsï¼‰

## ğŸ“š å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{llama_pruning_toolkit,
  title={LLaMA Pruning Toolkit: GQA-Aware Structured Pruning},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/llama-pruning}}
}
```

## ğŸ“„ License

MIT License

---

## ğŸ”¬ æŠ€æœ¯å®ç°ç»†èŠ‚

### åˆ†ç»„å‰ªæä»£ç å®ç°

#### Attention åˆ†ç»„å‰ªæ (`core/methods/gqa_aware.py`)

```python
def prune_attention_by_gqa_groups(layer, keep_kv_indices, head_dim=128, gqa_ratio=4):
    """
    æ ¹æ®ä¿ç•™çš„ KV head ç´¢å¼•å‰ªææ•´ä¸ª GQA ç»„

    Args:
        keep_kv_indices: è¦ä¿ç•™çš„ KV head ç´¢å¼•åˆ—è¡¨ [0, 2, 5, ...]
    """
    # 1. è®¡ç®—å¯¹åº”çš„ Q head ç´¢å¼•
    keep_q_indices = []
    for kv_idx in keep_kv_indices:
        q_start = kv_idx * gqa_ratio  # ä¾‹å¦‚ KV[1] å¯¹åº” Q[4:8]
        keep_q_indices.extend(range(q_start, q_start + gqa_ratio))

    # 2. è½¬æ¢ä¸ºé€šé“ç´¢å¼•ï¼ˆhead â†’ channelï¼‰
    keep_q_channels = [range(q*head_dim, (q+1)*head_dim) for q in keep_q_indices]
    keep_kv_channels = [range(kv*head_dim, (kv+1)*head_dim) for kv in keep_kv_indices]

    # 3. å‰ªææƒé‡çŸ©é˜µ
    layer.self_attn.q_proj.weight = layer.self_attn.q_proj.weight[keep_q_channels, :]
    layer.self_attn.k_proj.weight = layer.self_attn.k_proj.weight[keep_kv_channels, :]
    layer.self_attn.v_proj.weight = layer.self_attn.v_proj.weight[keep_kv_channels, :]
    layer.self_attn.o_proj.weight = layer.self_attn.o_proj.weight[:, keep_q_channels]

    # 4. æ›´æ–°é…ç½®
    layer.self_attn.num_heads = len(keep_q_indices)
    layer.self_attn.num_key_value_heads = len(keep_kv_indices)
```

#### MLP åˆ†ç»„å‰ªæ (`run_global_pruning.py`)

```python
def prune_mlp_by_channels(layer, keep_channel_indices):
    """
    æ ¹æ®ä¿ç•™çš„é€šé“ç´¢å¼•å‰ªæ MLP

    Args:
        keep_channel_indices: è¦ä¿ç•™çš„ä¸­é—´å±‚é€šé“ç´¢å¼• [0, 5, 10, ...]
    """
    # 1. å‰ªæ gate_proj å’Œ up_proj çš„è¾“å‡ºé€šé“ï¼ˆè¡Œï¼‰
    layer.mlp.gate_proj.weight = layer.mlp.gate_proj.weight[keep_channel_indices, :]
    layer.mlp.up_proj.weight = layer.mlp.up_proj.weight[keep_channel_indices, :]

    # 2. å‰ªæ down_proj çš„è¾“å…¥é€šé“ï¼ˆåˆ—ï¼‰
    layer.mlp.down_proj.weight = layer.mlp.down_proj.weight[:, keep_channel_indices]

    # 3. æ›´æ–°é…ç½®
    new_intermediate_size = len(keep_channel_indices)
    layer.mlp.gate_proj.out_features = new_intermediate_size
    layer.mlp.up_proj.out_features = new_intermediate_size
    layer.mlp.down_proj.in_features = new_intermediate_size
```

### é‡è¦æ€§è®¡ç®—æ–¹æ³•

#### Taylor Expansion (ä¸€é˜¶)

```python
# å¯¹äºæ¯ä¸ªæƒé‡å‚æ•°
importance = |weight Ã— gradient|

# Attention ç»„: ç´¯åŠ æ‰€æœ‰ç›¸å…³çš„ projection å±‚
I_group = |W_q Ã— âˆ‡W_q| + |W_k Ã— âˆ‡W_k| + |W_v Ã— âˆ‡W_v| + |W_o Ã— âˆ‡W_o|

# MLP ç»„: ç´¯åŠ ä¸‰ä¸ª projection å±‚
I_channel = |W_gate[i] Ã— âˆ‡W_gate[i]| + |W_up[i] Ã— âˆ‡W_up[i]| + |W_down[:,i] Ã— âˆ‡W_down[:,i]|
```

#### Taylor Expansion (äºŒé˜¶)

```python
# å¢åŠ  Hessian å¯¹è§’çº¿é¡¹
importance = |weight Ã— gradient| + 0.5 Ã— |weightÂ² Ã— hessian_diag|

# Hessian å¯¹è§’çº¿è¿‘ä¼¼: âˆ‡Â²L â‰ˆ (âˆ‡L)Â²
```

#### Wanda (Weight Ã— Activation)

```python
# ä½¿ç”¨æ¿€æ´»å€¼ä»£æ›¿æ¢¯åº¦
importance = |weight Ã— activation|

# æ— éœ€åå‘ä¼ æ’­ï¼Œè®¡ç®—æ›´å¿«
```

### å…¨å±€å‰ªæç®—æ³•ï¼ˆåˆ†æ•°èƒŒåŒ…ï¼‰

```python
# 1. æ„å»ºå…¨å±€åˆ†æè¡¨
for layer in model.layers:
    for group in [attention_groups, mlp_groups]:
        importance = compute_importance(group)
        cost = count_parameters(group)
        score = importance / cost
        table.append((layer_id, group_id, score, cost))

# 2. æŒ‰ score æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
table.sort(key=lambda x: x['score'])

# 3. ç´¯åŠ æˆæœ¬ï¼Œç›´åˆ°è¾¾åˆ°å‰ªæç›®æ ‡
pruned_params = 0
for group in table:
    if pruned_params + group.cost <= target_pruned_params:
        prune_group(group)
        pruned_params += group.cost
    else:
        break  # è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢å‰ªæ
```

### å…³é”®è¶…å‚æ•°å¯¹ç…§è¡¨

| å‚æ•° | Attention åˆ†ç»„ | MLP åˆ†ç»„ |
|------|---------------|---------|
| **ç»„çš„å¤§å°** | 6ä¸ªçŸ©é˜µå— (Q/K/Vå„1ä¸ª + Oå„1ä¸ª) | 3ä¸ªå‘é‡ (gate/up/downå„1ä¸ª) |
| **head_dim** | 128 (LLaMA-3) | N/A |
| **gqa_ratio** | 4:1 (Q:KV) | N/A |
| **num_groups** | num_kv_heads (é€šå¸¸8) | intermediate_size (é€šå¸¸14336) |
| **cost/group** | ~1.6M å‚æ•° | ~12K å‚æ•° |

### ç»´åº¦å˜åŒ–ç¤ºä¾‹

**å‰ªæå‰** (LLaMA-3-8B):
```
Attention:
- num_q_heads = 32, num_kv_heads = 8, head_dim = 128
- q_proj: [4096, 4096]  (32 * 128 = 4096)
- k_proj: [1024, 4096]  (8 * 128 = 1024)
- v_proj: [1024, 4096]
- o_proj: [4096, 4096]

MLP:
- gate_proj: [14336, 4096]
- up_proj:   [14336, 4096]
- down_proj: [4096, 14336]
```

**å‰ªæå** (å‡è®¾å‰ªæ‰ 50% Attention å’Œ 30% MLP):
```
Attention:
- num_q_heads = 16, num_kv_heads = 4, head_dim = 128
- q_proj: [2048, 4096]  (16 * 128 = 2048)
- k_proj: [512, 4096]   (4 * 128 = 512)
- v_proj: [512, 4096]
- o_proj: [4096, 2048]  â† æ³¨æ„è¿™é‡Œæ˜¯è¾“å…¥é€šé“å˜åŒ–

MLP:
- gate_proj: [10035, 4096]  (14336 * 0.7 â‰ˆ 10035)
- up_proj:   [10035, 4096]
- down_proj: [4096, 10035]  â† æ³¨æ„è¿™é‡Œæ˜¯è¾“å…¥é€šé“å˜åŒ–
```

---

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `run_global_pruning.py` - å…¨å±€å‰ªæä¸»è„šæœ¬ï¼ˆæ¨èä½¿ç”¨ï¼‰
- `layer_pruning.py` - å±‚çº§å‰ªæä¸»è„šæœ¬
- `core/methods/global_pruning.py` - å…¨å±€å‰ªæç®—æ³•å®ç°
- `core/methods/gqa_aware.py` - GQA æ„ŸçŸ¥çš„ Attention åˆ†ç»„å‰ªæ
- `core/importance/layer_analyzer.py` - å±‚é‡è¦æ€§åˆ†æ
- `core/trainer/finetuner.py` - LoRA/å…¨å‚æ•°å¾®è°ƒ
- `evaluation/metrics/ppl.py` - å›°æƒ‘åº¦è¯„ä¼°

**ç›¸å…³è®ºæ–‡**ï¼š
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)
- [Wanda: A Simple and Effective Pruning Approach](https://arxiv.org/abs/2306.11695)
- [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning](https://arxiv.org/abs/2203.07259)