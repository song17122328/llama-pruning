# æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å®Œæ•´è§£å†³æ–¹æ¡ˆ

## ğŸ“Š é—®é¢˜è¯†åˆ«

### ä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±ï¼ˆåœ¨å‰ªæä¸­ï¼‰

```
æ­£å¸¸æ¢¯åº¦åˆ†å¸ƒï¼š
Layer 0:  grad ~ 1e-5
Layer 10: grad ~ 1e-5
Layer 20: grad ~ 1e-5
Layer 31: grad ~ 1e-5
â†’ æ‰€æœ‰å±‚æ¢¯åº¦åœ¨åŒä¸€æ•°é‡çº§

æ¢¯åº¦æ¶ˆå¤±ï¼š
Layer 0:  grad ~ 1e-9  â† æå°ï¼
Layer 2:  grad ~ 1e-8  â† å¾ˆå°
Layer 10: grad ~ 1e-7
Layer 20: grad ~ 1e-6
Layer 31: grad ~ 1e-5
â†’ æ¢¯åº¦ç›¸å·®10000å€ï¼
```

**å¯¹å‰ªæçš„å½±å“ï¼š**
- å‰å‡ å±‚çš„Tayloré‡è¦æ€§ = |w Ã— grad| â‰ˆ 0
- è¢«è¯¯åˆ¤ä¸º"ä¸é‡è¦"
- è¢«è¿‡åº¦å‰ªæï¼ˆå¦‚99%ï¼‰

---

## ğŸ” å¦‚ä½•æ£€æµ‹

### æ–¹æ³•1ï¼šæŸ¥çœ‹è¿è¡Œæ—¥å¿—

è¿è¡Œå‰ªææ—¶ï¼Œç°åœ¨ä¼šè‡ªåŠ¨æ‰“å°ï¼š

```
æ¢¯åº¦åˆ†å¸ƒè¯Šæ–­ï¼ˆåºåˆ—é•¿åº¦ 256ï¼‰ï¼š
  Layer  0: grad_mean=1.234e-05, grad_std=2.345e-05
  Layer  2: grad_mean=5.678e-06, grad_std=8.901e-06  â† æ³¨æ„è¿™é‡Œ
  Layer 10: grad_mean=3.456e-06, grad_std=4.567e-06
  Layer 20: grad_mean=2.345e-06, grad_std=3.456e-06
  Layer 31: grad_mean=1.234e-06, grad_std=2.345e-06
```

**åˆ¤æ–­æ ‡å‡†ï¼š**
- âœ… æ­£å¸¸ï¼šæœ€å¤§æ¢¯åº¦ / æœ€å°æ¢¯åº¦ < 100
- âš ï¸ è½»å¾®ï¼š100 < æ¯”å€¼ < 1000
- âŒ ä¸¥é‡ï¼šæ¯”å€¼ > 1000

### æ–¹æ³•2ï¼šä½¿ç”¨å¯è§†åŒ–å·¥å…·

```python
from visualize_gradients import GradientVisualizer

visualizer = GradientVisualizer()
visualizer.collect_gradients(model, step_name='batch_0')
visualizer.plot_gradient_distribution()  # ç”Ÿæˆå›¾è¡¨
```

ç”Ÿæˆçš„å›¾ä¼šæ¸…æ¥šæ˜¾ç¤ºæ¢¯åº¦æ¶ˆå¤±åŒºåŸŸã€‚

### æ–¹æ³•3ï¼šæ£€æŸ¥å‰ªæåˆ†å¸ƒ

å¦‚æœçœ‹åˆ°ï¼š
```
Layer 2:  MLP 99% å‰ªæ  â† å¼‚å¸¸ï¼
Layer 3:  MLP 98% å‰ªæ  â† å¼‚å¸¸ï¼
Layer 10: MLP 95% å‰ªæ  â† å¼‚å¸¸ï¼
...
Layer 20: MLP 5% å‰ªæ   â† æ­£å¸¸
```

è¯´æ˜å‰å‡ å±‚è¢«è¿‡åº¦å‰ªæ â†’ æ¢¯åº¦æ¶ˆå¤±ï¼

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### â­ æ–¹æ¡ˆ1ï¼šè‡ªé€‚åº”æ¢¯åº¦ç¼©æ”¾ï¼ˆæ¨èï¼‰

**åŸç†ï¼š** å°†æ‰€æœ‰å±‚çš„æ¢¯åº¦ç¼©æ”¾åˆ°ç›¸ä¼¼èŒƒå›´

**å®ç°ï¼š**

åœ¨ `run_global_pruning.py` ä¸­æ·»åŠ ï¼ˆæˆ‘å·²ç»å‡†å¤‡å¥½ä»£ç ï¼‰ï¼š

```python
# åœ¨ loss.backward() ä¹‹å
loss.backward()

# è‡ªé€‚åº”æ¢¯åº¦ç¼©æ”¾
from fix_gradient_vanishing import adaptive_gradient_scaling
adaptive_gradient_scaling(model)

# ç„¶åç»§ç»­ç´¯åŠ  Hessian
if args.importance_method == 'taylor_2nd':
    ...
```

**ä¼˜ç‚¹ï¼š**
- âœ… æ•ˆæœæœ€å¥½
- âœ… è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤
- âœ… ä»£ç ç®€å•

**ç¼ºç‚¹ï¼š**
- âš ï¸ æ”¹å˜äº†æ¢¯åº¦çš„ç»å¯¹scale
- âš ï¸ ä½†ä¿ç•™ç›¸å¯¹é‡è¦æ€§

---

### â­ æ–¹æ¡ˆ2ï¼šå±‚çº§å½’ä¸€åŒ–

**åŸç†ï¼š** æ¯å±‚ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œæ¶ˆé™¤å±‚é—´å·®å¼‚

**å®ç°ï¼š**

```python
loss.backward()

from fix_gradient_vanishing import normalize_gradients_per_layer
normalize_gradients_per_layer(model)
```

**ä¼˜ç‚¹ï¼š**
- âœ… æœ€å®‰å…¨
- âœ… ä¿è¯æ¯å±‚å¹³ç­‰å¯¹å¾…

**ç¼ºç‚¹ï¼š**
- âš ï¸ å®Œå…¨å¿½ç•¥äº†å±‚é—´å·®å¼‚ï¼ˆå¯èƒ½ä¸¢å¤±ä¿¡æ¯ï¼‰

---

### â­ æ–¹æ¡ˆ3ï¼šæ·±åº¦åŠ æƒ

**åŸç†ï¼š** ç»™å‰é¢å±‚æ›´å¤§çš„æƒé‡ï¼Œè¡¥å¿æ¢¯åº¦æ¶ˆå¤±

**å®ç°ï¼š**

```python
loss.backward()

from fix_gradient_vanishing import weight_gradients_by_depth
weight_gradients_by_depth(model, strategy='sqrt')
```

**ç­–ç•¥ï¼š**
- `'linear'`: çº¿æ€§åŠ æƒï¼ˆå‰é¢å±‚æƒé‡æ›´å¤§ï¼‰
- `'sqrt'`: å¹³æ–¹æ ¹åŠ æƒï¼ˆä¸­ç­‰è¡¥å¿ï¼‰
- `'log'`: å¯¹æ•°åŠ æƒï¼ˆè½»å¾®è¡¥å¿ï¼‰

**ä¼˜ç‚¹ï¼š**
- âœ… æœ‰ç†è®ºä¾æ®ï¼ˆè¡¥å¿åå‘ä¼ æ’­è¡°å‡ï¼‰
- âœ… å¯è°ƒèŠ‚å¼ºåº¦

**ç¼ºç‚¹ï¼š**
- âš ï¸ éœ€è¦é€‰æ‹©åˆé€‚çš„ç­–ç•¥

---

### â­ æ–¹æ¡ˆ4ï¼šç»„åˆä½¿ç”¨ï¼ˆæœ€å¼ºï¼‰

```python
loss.backward()

# å…ˆè‡ªé€‚åº”ç¼©æ”¾
from fix_gradient_vanishing import adaptive_gradient_scaling
adaptive_gradient_scaling(model)

# å†æ·±åº¦åŠ æƒ
from fix_gradient_vanishing import weight_gradients_by_depth
weight_gradients_by_depth(model, strategy='sqrt')
```

**æ•ˆæœï¼š** è‡ªé€‚åº”ç¼©æ”¾ + æ·±åº¦åŠ æƒ = æœ€å¼ºä¿®å¤

---

### æ–¹æ¡ˆ5ï¼šä½¿ç”¨H-GSPä¿®æ­£ï¼ˆå·²æœ‰ï¼‰

**è¿™å°±æ˜¯ä½ çš„ T=1, tau=0 é…ç½®ï¼**

```bash
python run_global_pruning.py \
  --temperature 1 \
  --tau 0 \
  ...
```

**åŸç†ï¼š**
- ä¸ç›´æ¥ä¿®å¤æ¢¯åº¦
- è€Œæ˜¯ç”¨blockwiseé‡è¦æ€§ä¿®æ­£Tayloråˆ†æ•°
- å³ä½¿æŸå±‚æ¢¯åº¦å°ï¼Œä¹Ÿèƒ½é€šè¿‡å—çº§åˆ†æä¿ç•™é‡è¦éƒ¨åˆ†

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸éœ€è¦ä¿®æ”¹æ¢¯åº¦
- âœ… ä»ç®—æ³•å±‚é¢è§£å†³

**ç¼ºç‚¹ï¼š**
- âš ï¸ éœ€è¦é¢å¤–è®¡ç®—ï¼ˆå±‚çº§/å—çº§é‡è¦æ€§ï¼‰

---

## ğŸš€ å®é™…ä½¿ç”¨æŒ‡å—

### æ­¥éª¤1ï¼šå…ˆæ£€æµ‹æ˜¯å¦æœ‰é—®é¢˜

è¿è¡Œä¸€æ¬¡å‰ªæï¼ŒæŸ¥çœ‹æ—¥å¿—ï¼š

```bash
python run_global_pruning.py \
  --importance_method taylor_2nd \
  --temperature 1 --tau 0 \
  --output_name test
```

çœ‹æ¢¯åº¦è¯Šæ–­è¾“å‡ºï¼Œå¦‚æœæ¢¯åº¦æ¯”å€¼ > 100ï¼Œè¯´æ˜æœ‰é—®é¢˜ã€‚

### æ­¥éª¤2ï¼šé€‰æ‹©ä¿®å¤æ–¹æ¡ˆ

| æƒ…å†µ | æ¨èæ–¹æ¡ˆ |
|------|---------|
| æ¢¯åº¦æ¯”å€¼ < 100 | ä¸éœ€è¦ä¿®å¤ |
| 100 < æ¯”å€¼ < 1000 | è‡ªé€‚åº”ç¼©æ”¾ |
| æ¯”å€¼ > 1000 | è‡ªé€‚åº”ç¼©æ”¾ + æ·±åº¦åŠ æƒ |
| å‰ªæåˆ†å¸ƒå¼‚å¸¸ | æ£€æŸ¥æ˜¯å¦ç”¨äº† T=1, tau=0 |

### æ­¥éª¤3ï¼šé›†æˆåˆ°ä»£ç 

ä¿®æ”¹ `run_global_pruning.py`ï¼š

```python
# æ‰¾åˆ°è¿™ä¸€æ®µï¼ˆçº¦925è¡Œï¼‰
loss.backward()

# æ·»åŠ æ¢¯åº¦ä¿®å¤ï¼ˆå¦‚æœéœ€è¦ï¼‰
if args.fix_gradient_vanishing:  # æ–°å¢ä¸€ä¸ªå‚æ•°
    from fix_gradient_vanishing import adaptive_gradient_scaling
    adaptive_gradient_scaling(model)

# ç»§ç»­åŸæ¥çš„ä»£ç 
if args.importance_method == 'taylor_2nd':
    ...
```

æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼š

```python
parser.add_argument('--fix_gradient_vanishing', action='store_true',
                   help='æ˜¯å¦ä¿®å¤æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆè‡ªé€‚åº”ç¼©æ”¾ï¼‰')
```

ä½¿ç”¨ï¼š

```bash
python run_global_pruning.py \
  --fix_gradient_vanishing \  # å¯ç”¨ä¿®å¤
  --importance_method taylor_2nd \
  --temperature 1 --tau 0 \
  --output_name test_fixed
```

---

## ğŸ“Š å¯è§†åŒ–åˆ†æ

### ç”Ÿæˆæ¢¯åº¦å¯è§†åŒ–

åœ¨ `run_global_pruning.py` ä¸­æ·»åŠ ï¼š

```python
from visualize_gradients import GradientVisualizer

# åˆ›å»ºå¯è§†åŒ–å™¨
visualizer = GradientVisualizer(output_dir='gradient_analysis')

# åœ¨æ¢¯åº¦è®¡ç®—å¾ªç¯ä¸­
for batch_idx in pbar:
    loss.backward()

    # æ”¶é›†å‰5ä¸ªbatchçš„æ¢¯åº¦
    if batch_idx < 5:
        visualizer.collect_gradients(model, step_name=f'batch_{batch_idx}')

# å¾ªç¯ç»“æŸåï¼Œç”Ÿæˆæ‰€æœ‰å›¾è¡¨
logger.log("\nç”Ÿæˆæ¢¯åº¦å¯è§†åŒ–...")
visualizer.plot_gradient_distribution(step_idx=0)
visualizer.plot_gradient_heatmap(step_idx=0)
visualizer.plot_gradient_comparison()
visualizer.plot_layer_variance()
visualizer.generate_report()
logger.log("âœ“ æ¢¯åº¦åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ gradient_analysis/")
```

### æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶

```
gradient_analysis/
â”œâ”€â”€ grad_dist_batch0.png      # æ¢¯åº¦åˆ†å¸ƒå›¾
â”œâ”€â”€ grad_heatmap_batch0.png   # æ¢¯åº¦çƒ­åŠ›å›¾
â”œâ”€â”€ grad_comparison.png       # å¤šæ‰¹æ¬¡å¯¹æ¯”
â”œâ”€â”€ layer_variance.png        # å±‚å†…æ–¹å·®
â””â”€â”€ gradient_report.txt       # æ–‡æœ¬æŠ¥å‘Š
```

---

## ğŸ¯ æ–¹æ¡ˆé€‰æ‹©å»ºè®®

### æ¨èé…ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

#### 1ï¸âƒ£ é¦–é€‰ï¼šH-GSPä¿®æ­£ï¼ˆå·²æœ‰ï¼‰

```bash
python run_global_pruning.py \
  --importance_method taylor_2nd \
  --temperature 1 \
  --tau 0
```

**ä¼˜ç‚¹ï¼š** ä¸ä¿®æ”¹æ¢¯åº¦ï¼Œä»ç®—æ³•å±‚é¢è§£å†³

#### 2ï¸âƒ£ å¦‚æœè¿˜æœ‰é—®é¢˜ï¼šH-GSP + è‡ªé€‚åº”ç¼©æ”¾

```bash
python run_global_pruning.py \
  --importance_method taylor_2nd \
  --temperature 1 \
  --tau 0 \
  --fix_gradient_vanishing  # æ–°å¢å‚æ•°
```

**ä¼˜ç‚¹ï¼š** åŒé‡ä¿é™©

#### 3ï¸âƒ£ æç«¯æƒ…å†µï¼šæ‰€æœ‰æ–¹æ³•ç»„åˆ

ä¿®æ”¹ä»£ç ï¼ŒåŒæ—¶ä½¿ç”¨ï¼š
- H-GSPä¿®æ­£ (T=1, tau=0)
- è‡ªé€‚åº”ç¼©æ”¾
- æ·±åº¦åŠ æƒ
- å¯è§†åŒ–ç›‘æ§

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ä¸è¦è¿‡åº¦ä¿®å¤

- æ¢¯åº¦å·®å¼‚ < 100å€æ˜¯æ­£å¸¸çš„
- ä¸è¦å¼ºåˆ¶æ‰€æœ‰å±‚æ¢¯åº¦å®Œå…¨ç›¸åŒ
- ä¿ç•™ä¸€å®šçš„è‡ªç„¶å·®å¼‚

### 2. éªŒè¯ä¿®å¤æ•ˆæœ

ä¿®å¤åï¼Œé‡æ–°æŸ¥çœ‹ï¼š
- æ¢¯åº¦è¯Šæ–­è¾“å‡º
- å‰ªæåˆ†å¸ƒï¼ˆæ˜¯å¦è¿˜æœ‰99%çš„æç«¯æƒ…å†µï¼‰
- æœ€ç»ˆæ€§èƒ½ï¼ˆACCæ˜¯å¦æå‡ï¼‰

### 3. åºåˆ—é•¿åº¦çš„å½±å“

- 128åºåˆ—ï¼šæ¢¯åº¦æœ€ç¨³å®š
- 256åºåˆ—ï¼šæŠ˜ä¸­é€‰æ‹©
- 512åºåˆ—ï¼šå¯èƒ½éœ€è¦æ¢¯åº¦ä¿®å¤

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**
   - Bengio et al. "Learning Long-Term Dependencies with Gradient Descent is Difficult" (1994)

2. **å‰ªæä¸­çš„æ¢¯åº¦é—®é¢˜**
   - He et al. "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks" (2019)

3. **å±‚çº§å½’ä¸€åŒ–**
   - Ba et al. "Layer Normalization" (2016)

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `fix_gradient_vanishing.py` - æ¢¯åº¦ä¿®å¤æ–¹æ¡ˆå®ç°
- `visualize_gradients.py` - å¯è§†åŒ–å·¥å…·
- `run_global_pruning.py:927-938` - æ¢¯åº¦è¯Šæ–­ä»£ç 
- `SEQUENCE_LENGTH_UPDATE.md` - åºåˆ—é•¿åº¦å½±å“åˆ†æ
