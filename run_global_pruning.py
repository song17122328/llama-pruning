#!/usr/bin/env python3
"""
åŸºäºå…¨å±€æ€§ä»·æ¯”æ’åºçš„æ··åˆç»“æ„åŒ–å‰ªæ

æ ¸å¿ƒæ€æƒ³ï¼š
- å°†å‰ªæé—®é¢˜å»ºæ¨¡ä¸ºåˆ†æ•°èƒŒåŒ…é—®é¢˜
- Score = Importance / Cost
- å…¨å±€æ’åºï¼Œä¼˜å…ˆå‰ªé™¤"æ€§ä»·æ¯”"æœ€ä½çš„ groups
- è‡ªåŠ¨å®ç°æ·±åº¦å‰ªæï¼ˆå±‚ç§»é™¤ï¼‰+ å®½åº¦å‰ªæï¼ˆç¥ç»å…ƒå‰ªé™¤ï¼‰çš„æ··åˆç­–ç•¥
"""

import os
import torch
import argparse
import time
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

from core.methods.global_pruning import (
    build_global_group_table,
    select_groups_to_prune
)
from core.methods.gqa_aware import prune_attention_by_gqa_groups
from core.datasets import DatasetManager
from core.models import IdentityDecoderLayer
from evaluation.metrics.ppl import PPLMetric
from core.trainer.finetuner import FineTuner
from core.utils.logger import LoggerWithDepth


def collect_layer_activations(model, input_ids, device='cuda'):
    """
    æ”¶é›†æ¯å±‚çš„æ¿€æ´»å€¼ç”¨äº Wanda æ–¹æ³•

    Returns:
        activations: Dict[layer_idx -> Dict[name -> Tensor]]
    """
    activations = {}
    hooks = []

    def get_activation_hook(layer_idx, name):
        def hook(module, input, output):
            if layer_idx not in activations:
                activations[layer_idx] = {}
            # å­˜å‚¨è¾“å…¥æ¿€æ´»å€¼çš„å¹³å‡å€¼ï¼ˆç”¨äº Wandaï¼‰
            if isinstance(input, tuple):
                act = input[0].detach()
            else:
                act = input.detach()
            # è®¡ç®—æ‰€æœ‰ç»´åº¦çš„å¹³å‡ï¼ˆé™¤äº†æœ€åçš„ç‰¹å¾ç»´åº¦ï¼‰
            if act.dim() > 1:
                act = act.abs().mean(dim=tuple(range(act.dim() - 1)))
            activations[layer_idx][name] = act.cpu()
        return hook

    # ä¸ºæ¯å±‚çš„å…³é”®æ¨¡å—æ³¨å†Œ hooks
    for layer_idx, layer in enumerate(model.model.layers):
        # Attention çš„è¾“å…¥æ¿€æ´»
        hooks.append(layer.self_attn.q_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'q_proj')))
        hooks.append(layer.self_attn.k_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'k_proj')))
        hooks.append(layer.self_attn.v_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'v_proj')))
        hooks.append(layer.self_attn.o_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'o_proj')))

        # MLP çš„è¾“å…¥æ¿€æ´»
        hooks.append(layer.mlp.gate_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'mlp_input')))

        # MLP ä¸­é—´æ¿€æ´»ï¼ˆç”¨äº down_projï¼‰
        def get_mlp_intermediate_hook(layer_idx):
            def hook(module, input, output):
                if layer_idx not in activations:
                    activations[layer_idx] = {}
                act = output.detach().abs().mean(dim=tuple(range(output.dim() - 1)))
                activations[layer_idx]['intermediate'] = act.cpu()
            return hook

        hooks.append(layer.mlp.up_proj.register_forward_hook(
            get_mlp_intermediate_hook(layer_idx)))

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    with torch.no_grad():
        model(input_ids)

    # ç§»é™¤æ‰€æœ‰ hooks
    for hook in hooks:
        hook.remove()

    return activations


def apply_global_pruning(model, groups_to_prune_df, head_dim=128, gqa_ratio=4, logger=None):
    """
    æ ¹æ®å…¨å±€åˆ†æè¡¨æ‰§è¡Œå®é™…å‰ªæ

    Args:
        model: æ¨¡å‹
        groups_to_prune_df: è¦å‰ªæçš„ groups DataFrame
        head_dim: attention head ç»´åº¦
        gqa_ratio: Q:KV æ¯”ä¾‹
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        pruned_layers: è¢«å®Œå…¨å‰ªç©ºçš„å±‚åˆ—è¡¨
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log("\n" + "="*60)
    log("æ‰§è¡Œå…¨å±€å‰ªæ")
    log("="*60)

    num_layers = len(model.model.layers)
    pruning_stats = {
        'attention': {},  # {layer_idx: (old_kv, new_kv)}
        'mlp': {},        # {layer_idx: (old_channels, new_channels)}
        'empty_layers': []
    }

    # æŒ‰å±‚ç»„ç»‡è¦å‰ªæçš„ groups
    layer_prune_info = {}
    for layer_idx in range(num_layers):
        layer_data = groups_to_prune_df[groups_to_prune_df['layer_idx'] == layer_idx]

        attn_groups = layer_data[layer_data['group_type'] == 'attention']['group_idx'].tolist()
        mlp_groups = layer_data[layer_data['group_type'] == 'mlp']['group_idx'].tolist()

        layer_prune_info[layer_idx] = {
            'attention': attn_groups,
            'mlp': mlp_groups
        }

    # æ‰§è¡Œå‰ªæ
    for layer_idx in range(num_layers):
        layer = model.model.layers[layer_idx]
        prune_info = layer_prune_info[layer_idx]

        log(f"\nå¤„ç† Layer {layer_idx}:")

        # ========== Attention å‰ªæ ==========
        attn_prune_indices = prune_info['attention']

        if len(attn_prune_indices) > 0:
            # è·å–å½“å‰ KV heads æ•°é‡ï¼ˆä»æƒé‡å½¢çŠ¶æ¨æ–­ï¼‰
            k_proj_out_features = layer.self_attn.k_proj.out_features
            num_kv_heads = k_proj_out_features // head_dim

            # è®¡ç®—ä¿ç•™çš„ indices
            all_kv_indices = set(range(num_kv_heads))
            keep_kv_indices = sorted(list(all_kv_indices - set(attn_prune_indices)))

            # ä»æƒé‡å½¢çŠ¶è·å– Q heads æ•°é‡
            q_proj_out_features = layer.self_attn.q_proj.out_features
            old_q = q_proj_out_features // head_dim
            old_kv = num_kv_heads

            if len(keep_kv_indices) > 0:
                # æ‰§è¡Œå‰ªæ
                new_q, new_kv = prune_attention_by_gqa_groups(
                    layer,
                    keep_kv_indices,
                    head_dim=head_dim,
                    gqa_ratio=gqa_ratio
                )
                log(f"  Attention: {old_q}Q:{old_kv}KV â†’ {new_q}Q:{new_kv}KV")
                pruning_stats['attention'][layer_idx] = (old_kv, new_kv)
            else:
                # è¯¥å±‚ Attention è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ Attention è¢«å®Œå…¨å‰ªç©ºï¼ˆ{old_kv} â†’ 0 KV headsï¼‰")
                pruning_stats['attention'][layer_idx] = (old_kv, 0)

        # ========== MLP å‰ªæ ==========
        mlp_prune_indices = prune_info['mlp']

        if len(mlp_prune_indices) > 0:
            intermediate_size = layer.mlp.gate_proj.out_features

            # è®¡ç®—ä¿ç•™çš„ indices
            all_mlp_indices = set(range(intermediate_size))
            keep_mlp_indices = sorted(list(all_mlp_indices - set(mlp_prune_indices)))

            if len(keep_mlp_indices) > 0:
                # æ‰§è¡Œ MLP å‰ªæ
                keep_mlp_indices_tensor = torch.tensor(keep_mlp_indices, device=layer.mlp.gate_proj.weight.device)

                # å‰ªæ gate_proj å’Œ up_projï¼ˆä¿ç•™å¯¹åº”çš„è¡Œï¼‰
                layer.mlp.gate_proj.weight = torch.nn.Parameter(
                    layer.mlp.gate_proj.weight[keep_mlp_indices_tensor, :]
                )
                layer.mlp.up_proj.weight = torch.nn.Parameter(
                    layer.mlp.up_proj.weight[keep_mlp_indices_tensor, :]
                )

                # å‰ªæ down_projï¼ˆä¿ç•™å¯¹åº”çš„åˆ—ï¼‰
                layer.mlp.down_proj.weight = torch.nn.Parameter(
                    layer.mlp.down_proj.weight[:, keep_mlp_indices_tensor]
                )

                # æ›´æ–° intermediate_size
                new_intermediate_size = len(keep_mlp_indices)
                layer.mlp.gate_proj.out_features = new_intermediate_size
                layer.mlp.up_proj.out_features = new_intermediate_size
                layer.mlp.down_proj.in_features = new_intermediate_size

                log(f"  MLP: {intermediate_size} â†’ {new_intermediate_size} channels")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, new_intermediate_size)
            else:
                # è¯¥å±‚ MLP è¢«å®Œå…¨å‰ªç©º
                log(f"  âš ï¸ MLP è¢«å®Œå…¨å‰ªç©ºï¼ˆ{intermediate_size} â†’ 0 channelsï¼‰")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, 0)

        # æ£€æŸ¥æ˜¯å¦æ•´å±‚è¢«å‰ªç©º
        attn_empty = (layer_idx in pruning_stats['attention'] and
                     pruning_stats['attention'][layer_idx][1] == 0)
        mlp_empty = (layer_idx in pruning_stats['mlp'] and
                    pruning_stats['mlp'][layer_idx][1] == 0)

        if attn_empty and mlp_empty:
            log(f"  ğŸ”´ Layer {layer_idx} è¢«å®Œå…¨å‰ªç©ºï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰")
            pruning_stats['empty_layers'].append(layer_idx)

    return pruning_stats


def remove_empty_layers(model, empty_layers, logger=None):
    """
    "ç§»é™¤"è¢«å®Œå…¨å‰ªç©ºçš„å±‚ - é€šè¿‡æ›¿æ¢ä¸º identity å±‚

    æ³¨æ„ï¼šç”±äº HuggingFace Transformers çš„å†…éƒ¨å®ç°å¯èƒ½åœ¨å¤šå¤„å‡è®¾å±‚æ•°å›ºå®šï¼Œ
    å®Œå…¨åˆ é™¤å±‚å¯èƒ½å¯¼è‡´ "list index out of range" é”™è¯¯ã€‚
    å› æ­¤æˆ‘ä»¬é‡‡ç”¨æ›´å®‰å…¨çš„ç­–ç•¥ï¼šå°†ç©ºå±‚æ›¿æ¢ä¸ºç®€å•çš„ pass-through å±‚ã€‚

    Args:
        model: æ¨¡å‹
        empty_layers: è¦ç§»é™¤çš„å±‚ç´¢å¼•åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    if len(empty_layers) == 0:
        log("\nâœ“ æ²¡æœ‰å±‚è¢«å®Œå…¨å‰ªç©ºï¼Œè·³è¿‡å±‚ç§»é™¤")
        return

    log(f"\n{'='*60}")
    log(f"ç§»é™¤å®Œå…¨å‰ªç©ºçš„å±‚")
    log(f"{'='*60}")
    log(f"è¦ç§»é™¤çš„å±‚: {empty_layers}")
    log(f"ç­–ç•¥: æ›¿æ¢ä¸º Identity å±‚ï¼ˆä¿æŒæ¨¡å‹ç»“æ„å®Œæ•´ï¼‰")

    # ä¸ºäº†é¿å… HuggingFace Transformers å†…éƒ¨çš„å„ç§å‡è®¾è¢«æ‰“ç ´
    # æˆ‘ä»¬ä¸åˆ é™¤å±‚ï¼Œè€Œæ˜¯å°†å®ƒä»¬æ›¿æ¢ä¸ºå…¨å±€å®šä¹‰çš„ IdentityDecoderLayer
    num_layers = len(model.model.layers)

    # æ›¿æ¢ç©ºå±‚ä¸º identity å±‚
    for layer_idx in empty_layers:
        if layer_idx < num_layers:
            log(f"  æ›¿æ¢ Layer {layer_idx} ä¸º Identity å±‚")
            model.model.layers[layer_idx] = IdentityDecoderLayer()

    log(f"âœ“ å·²æ›¿æ¢ {len(empty_layers)} å±‚ä¸º Identity å±‚")
    log(f"  ç‰©ç†å±‚æ•°: {num_layers} (ä¿æŒä¸å˜)")
    log(f"  æœ‰æ•ˆå±‚æ•°: {num_layers - len(empty_layers)}")

    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶å¤„äºevalæ¨¡å¼
    device = next(model.parameters()).device
    model.eval()

    log(f"âœ“ æ¨¡å‹çŠ¶æ€å·²åˆ·æ–°")

    # éªŒè¯æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸forwardï¼ˆä½¿ç”¨ä¸€ä¸ªå°çš„dummyè¾“å…¥ï¼‰
    try:
        with torch.no_grad():
            dummy_input = torch.randint(0, 1000, (1, 10)).to(device)
            _ = model(dummy_input)
        log(f"âœ“ æ¨¡å‹forwardéªŒè¯é€šè¿‡")
    except Exception as e:
        log(f"âš ï¸  æ¨¡å‹forwardéªŒè¯å¤±è´¥: {e}")
        log(f"   è¿™å¯èƒ½ä¼šå¯¼è‡´åç»­PPLè®¡ç®—å‡ºé”™")
        import traceback
        log(f"   é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")


def auto_collapse(model, pruning_stats, collapse_threshold=0.15, logger=None):
    """
    è‡ªåŠ¨åç¼©ï¼šæ£€æµ‹ç¨€ç–å±‚å¹¶å¼ºåˆ¶ç§»é™¤æ•´å±‚

    H-GSP æ ¸å¿ƒæ€æƒ³ï¼šé¿å…"ç•™ 10% ä¸å¦‚ä¸ç•™"çš„æƒ…å†µ
    å½“æŸå±‚çš„å‚æ•°ä¿ç•™ç‡ä½äºé˜ˆå€¼æ—¶ï¼Œç›´æ¥ç§»é™¤æ•´å±‚

    Args:
        model: æ¨¡å‹
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
            {'attention': {layer_idx: (old, new)}, 'mlp': {layer_idx: (old, new)}, 'empty_layers': []}
        collapse_threshold: åç¼©é˜ˆå€¼ï¼ˆé»˜è®¤ 0.15 = 15%ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        additional_empty_layers: éœ€è¦é¢å¤–ç§»é™¤çš„å±‚åˆ—è¡¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log(f"\n{'='*60}")
    log(f"è‡ªåŠ¨åç¼©æ£€æµ‹ (Auto-Collapse)")
    log(f"{'='*60}")
    log(f"åç¼©é˜ˆå€¼: {collapse_threshold:.1%}")
    log(f"æ£€æµ‹é€»è¾‘: å½“å±‚å‚æ•°ä¿ç•™ç‡ < {collapse_threshold:.1%} æ—¶ï¼Œå¼ºåˆ¶ç§»é™¤æ•´å±‚")

    num_layers = len(model.model.layers)
    additional_empty_layers = []

    for layer_idx in range(num_layers):
        # è·³è¿‡å·²ç»è¢«å‰ªç©ºçš„å±‚
        if layer_idx in pruning_stats.get('empty_layers', []):
            continue

        # è®¡ç®—è¯¥å±‚çš„å‚æ•°ä¿ç•™ç‡
        attn_info = pruning_stats['attention'].get(layer_idx)
        mlp_info = pruning_stats['mlp'].get(layer_idx)

        # è®¡ç®— Attention ä¿ç•™ç‡
        if attn_info:
            old_kv, new_kv = attn_info
            attn_retain_rate = new_kv / old_kv if old_kv > 0 else 1.0
        else:
            attn_retain_rate = 1.0

        # è®¡ç®— MLP ä¿ç•™ç‡
        if mlp_info:
            old_channels, new_channels = mlp_info
            mlp_retain_rate = new_channels / old_channels if old_channels > 0 else 1.0
        else:
            mlp_retain_rate = 1.0

        # è®¡ç®—ç»¼åˆä¿ç•™ç‡ï¼ˆå–ä¸¤è€…çš„å¹³å‡ï¼‰
        avg_retain_rate = (attn_retain_rate + mlp_retain_rate) / 2.0

        # åˆ¤æ–­æ˜¯å¦è§¦å‘åç¼©
        if avg_retain_rate < collapse_threshold:
            log(f"  ğŸ”» Layer {layer_idx} è§¦å‘åç¼©:")
            log(f"     Attn ä¿ç•™ç‡: {attn_retain_rate:.1%}, MLP ä¿ç•™ç‡: {mlp_retain_rate:.1%}")
            log(f"     å¹³å‡ä¿ç•™ç‡: {avg_retain_rate:.1%} < {collapse_threshold:.1%}")
            log(f"     å†³ç­–: å¼ºåˆ¶ç§»é™¤æ•´å±‚")
            additional_empty_layers.append(layer_idx)

    if len(additional_empty_layers) == 0:
        log(f"\nâœ“ æ²¡æœ‰å±‚è§¦å‘åç¼©é˜ˆå€¼")
    else:
        log(f"\nâœ“ æ£€æµ‹åˆ° {len(additional_empty_layers)} å±‚éœ€è¦åç¼©: {additional_empty_layers}")
        log(f"  è¿™äº›å±‚å°†è¢«å¼ºåˆ¶ç§»é™¤ï¼ˆåˆ©ç”¨æ®‹å·®æ‚–è®ºï¼‰")

    return additional_empty_layers


def main():
    parser = argparse.ArgumentParser(description='åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_ckpt_log_name', type=str, default='llama_global_prune',
                       help='å®éªŒåç§°')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆç›¸å¯¹äºæ¨¡å‹æ€»å‚æ•°ï¼‰')
    parser.add_argument('--importance_method', type=str, default='taylor',
                       choices=['taylor', 'wanda', 'taylor_2nd'],
                       help='é‡è¦æ€§è®¡ç®—æ–¹æ³•: taylor(ä¸€é˜¶), wanda(æƒé‡Ã—æ¿€æ´»), taylor_2nd(äºŒé˜¶)')
    parser.add_argument('--dataset', type=str, default='wikitext2',
                       choices=['wikitext2', 'ptb', 'c4'],
                       help='æ•°æ®é›†é€‰æ‹©ï¼ˆç”¨äºæ‰€æœ‰é‡è¦æ€§è®¡ç®—å’Œè¯„ä¼°ï¼‰')
    parser.add_argument('--gradient_batch_size', type=int, default=4,
                       help='æ¢¯åº¦è®¡ç®—æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼ˆç”¨äºèŠ‚çœå†…å­˜ï¼‰')
    parser.add_argument('--use_gradient_checkpointing', action='store_true',
                       help='ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ä½†ä¼šæ…¢ä¸€äº›ï¼‰')
    parser.add_argument('--remove_empty_layers', action='store_true',
                       help='æ˜¯å¦ç§»é™¤è¢«å®Œå…¨å‰ªç©ºçš„å±‚ï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰')

    # H-GSP æ ¸å¿ƒå‚æ•°
    parser.add_argument('--temperature', type=float, default=1.0,
                       help='H-GSP æ¸©åº¦å‚æ•° Tï¼šæ§åˆ¶æ•æ„Ÿåº¦åŠ æƒå¼ºåº¦ (T=0: çº¯Taylor, T=1: æ¨èå¹³è¡¡, T>1: æ¿€è¿›å¼ºåŒ–é¦–å°¾)')
    parser.add_argument('--tau', type=float, default=None,
                       help='H-GSP é—¨æ§é˜ˆå€¼ Ï„ï¼šLayer/Block æ¨¡å¼åˆ‡æ¢ç‚¹ (None: è‡ªåŠ¨è®¡ç®—25åˆ†ä½æ•°, 0: çº¯Block, inf: çº¯Layer)')
    parser.add_argument('--epsilon', type=float, default=0.15,
                       help='H-GSP åç¼©é˜ˆå€¼ Îµï¼šå±‚å‰©ä½™å‚æ•°ç‡ä½äºæ­¤å€¼æ—¶è‡ªåŠ¨åç¼©æ•´å±‚ï¼ˆé»˜è®¤0.15ï¼‰')

    # GQA é…ç½®
    parser.add_argument('--head_dim', type=int, default=128,
                       help='Attention head ç»´åº¦')
    parser.add_argument('--gqa_ratio', type=int, default=4,
                       help='Q:KV æ¯”ä¾‹')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--test_before_prune', action='store_true',
                       help='å‰ªæå‰è¯„ä¼°åŸºçº¿ PPL')
    parser.add_argument('--test_after_prune', action='store_true',
                       help='å‰ªæåè¯„ä¼° PPL')

    # å¾®è°ƒå‚æ•°
    parser.add_argument('--finetune', action='store_true',
                       help='å‰ªæåè¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--finetune_method', type=str, default='lora',
                       choices=['full', 'lora'],
                       help='å¾®è°ƒæ–¹æ³•')
    parser.add_argument('--finetune_samples', type=int, default=500,
                       help='å¾®è°ƒæ ·æœ¬æ•°')
    parser.add_argument('--finetune_lr', type=float, default=1e-4,
                       help='å¾®è°ƒå­¦ä¹ ç‡')
    parser.add_argument('--finetune_epochs', type=int, default=1,
                       help='å¾®è°ƒè½®æ•°')
    parser.add_argument('--lora_r', type=int, default=8,
                       help='LoRA rank')
    parser.add_argument('--lora_alpha', type=int, default=16,
                       help='LoRA alpha')

    # ä¿å­˜å‚æ•°
    parser.add_argument('--save_model', action='store_true',
                       help='ä¿å­˜å‰ªæåçš„æ¨¡å‹')

    # å…¶ä»–
    from core.utils.get_best_gpu import get_best_gpu
    bestDevice = "cuda:"+str(get_best_gpu())  # è‡ªåŠ¨é€‰æ‹©æ˜¾å­˜æœ€å¤§çš„GPU
    # bestDevice = "cpu"  # å¦‚æœè¦ç”¨CPUï¼Œå–æ¶ˆæ³¨é‡Šè¿™è¡Œ
    parser.add_argument('--device', type=str, default=bestDevice,
                       help='è®¾å¤‡')
    parser.add_argument('--layer_start', type=int, default=0,
                       help='èµ·å§‹å±‚ï¼ˆdebugç”¨ï¼‰')
    parser.add_argument('--layer_end', type=int, default=None,
                       help='ç»“æŸå±‚ï¼ˆdebugç”¨ï¼‰')

    args = parser.parse_args()

    # è®¾ç½® logger
    logger = LoggerWithDepth(
        env_name=args.save_ckpt_log_name,
        config=args.__dict__,
        root_dir='prune_log'
    )

    logger.log("="*60)
    logger.log("åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ")
    logger.log("="*60)
    logger.log(f"æ¨¡å‹: {args.base_model}")
    logger.log(f"å‰ªæç‡: {args.pruning_ratio:.1%}")
    logger.log(f"é‡è¦æ€§æ–¹æ³•: {args.importance_method}")
    logger.log(f"æ•°æ®é›†: {args.dataset}")

    # ========== Step 1: åŠ è½½æ¨¡å‹ ==========
    logger.log("\n[Step 1] åŠ è½½æ¨¡å‹...")

    # æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
    if 'cpu' in args.device.lower():
        device_map = args.device
    else:
        # å•GPUï¼šç›´æ¥æŒ‡å®šè®¾å¤‡ï¼Œé¿å…å¤šGPUåˆ†å¸ƒ
        device_map = args.device

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map=device_map,
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    if args.use_gradient_checkpointing:
        logger.log("  å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰...")
        model.gradient_checkpointing_enable()

    # è·å–å®é™…ä½¿ç”¨çš„è®¾å¤‡
    if hasattr(model, 'hf_device_map'):
        logger.log(f"  æ¨¡å‹åˆ†å¸ƒ: {model.hf_device_map}")
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—çš„è®¾å¤‡ï¼ˆè¾“å…¥æ•°æ®åº”è¯¥å‘é€åˆ°è¿™é‡Œï¼‰
        first_device = next(iter(model.hf_device_map.values()))
        args.device = f'cuda:{first_device}' if isinstance(first_device, int) else first_device
        logger.log(f"  è¾“å…¥è®¾å¤‡: {args.device}")
    else:
        args.device = next(model.parameters()).device

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    logger.log(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    logger.log(f"  æ€»å‚æ•°é‡: {total_params:,}")

    # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available() and 'cuda' in str(args.device).lower():
        device_str = str(args.device)
        gpu_id = int(device_str.split(':')[-1]) if ':' in device_str else 0
        allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
        reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3
        total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
        logger.log(f"  GPU æ˜¾å­˜: {allocated:.2f}GB / {total_mem:.2f}GB (å·²åˆ†é…)")
        logger.log(f"  GPU æ˜¾å­˜: {reserved:.2f}GB / {total_mem:.2f}GB (å·²é¢„ç•™)")

    # åˆ›å»ºæ•°æ®é›†ç®¡ç†å™¨ï¼ˆç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®é›†åŠ è½½ï¼‰
    logger.log(f"\nâœ“ åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨: {args.dataset}")
    dataset_manager = DatasetManager(dataset_name=args.dataset, tokenizer=tokenizer)

    # ========== Step 2: è¯„ä¼°åŸºçº¿ ==========
    if args.test_before_prune:
        logger.log("\n[Step 2] è¯„ä¼°åŸºçº¿ PPL...")
        baseline_ppl = PPLMetric(model, tokenizer, datasets=[args.dataset], device=args.device)
        logger.log(f"âœ“ åŸºçº¿ PPL: {baseline_ppl}")

    # ========== Step 3: è®¡ç®—é‡è¦æ€§ï¼ˆæ¢¯åº¦æˆ–æ¿€æ´»ï¼‰ ==========
    activations = None
    hessian_diag = None

    # H-GSP å†…éƒ¨å›ºå®šå‚æ•°ï¼ˆä¸å¯¹å¤–æš´éœ²ï¼‰
    TAYLOR_NUM_SAMPLES = 128
    TAYLOR_SEQ_LEN = 128
    LAYER_IMPORTANCE_NUM_SAMPLES = 50
    LAYER_IMPORTANCE_SEQ_LEN = 128
    BLOCK_IMPORTANCE_NUM_SAMPLES = 50
    BLOCK_IMPORTANCE_SEQ_LEN = 128

    if args.importance_method in ['taylor', 'taylor_2nd']:
        logger.log(f"\n[Step 3] è®¡ç®—æ¢¯åº¦ï¼ˆ{'ä¸€é˜¶' if args.importance_method == 'taylor' else 'äºŒé˜¶'} Taylor importanceï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {TAYLOR_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {TAYLOR_SEQ_LEN} (å†…éƒ¨å›ºå®š)")

        # åˆ†æ‰¹è®¡ç®—æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜
        batch_size = args.gradient_batch_size
        num_batches = (TAYLOR_NUM_SAMPLES + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        model.zero_grad()
        total_loss = 0.0
        start_time = time.time()

        # å¦‚æœåœ¨ CPU ä¸Šè¿è¡Œï¼Œç»™å‡ºæç¤º
        if 'cpu' in str(args.device).lower():
            logger.log(f"  âš ï¸ åœ¨ CPU ä¸Šè¿è¡Œï¼Œé€Ÿåº¦ä¼šéå¸¸æ…¢ï¼")
            logger.log(f"  é¢„è®¡æ¯ä¸ªæ‰¹æ¬¡éœ€è¦ 5-10 åˆ†é’Ÿï¼ˆå–å†³äº CPU æ€§èƒ½ï¼‰")
            logger.log(f"  æ€»é¢„è®¡æ—¶é—´: {num_batches * 7:.0f} åˆ†é’Ÿå·¦å³")
            logger.log("")

        # äºŒé˜¶æ³°å‹’éœ€è¦ç´¯ç§¯ Hessian å¯¹è§’çº¿è¿‘ä¼¼
        # âš ï¸ å­˜å‚¨åœ¨CPUä¸Šä»¥é¿å…GPU OOM
        if args.importance_method == 'taylor_2nd':
            hessian_diag = {}
            logger.log("  åˆå§‹åŒ– Hessian å¯¹è§’çº¿å­˜å‚¨ï¼ˆåœ¨CPUä¸Šä»¥èŠ‚çœGPUæ˜¾å­˜ï¼‰...")
            for name, param in model.named_parameters():
                if param.requires_grad:
                    # å­˜å‚¨åœ¨CPUä¸Šï¼Œé¿å…å ç”¨GPUæ˜¾å­˜
                    hessian_diag[name] = torch.zeros_like(param.data, device='cpu')

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(range(num_batches), desc="è®¡ç®—æ¢¯åº¦", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, TAYLOR_NUM_SAMPLES)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # åŠ è½½å½“å‰æ‰¹æ¬¡
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åŠ è½½æ•°æ®...")
            input_ids = dataset_manager.get_gradient_samples(
                num_samples=current_batch_size,
                seq_len=TAYLOR_SEQ_LEN
            )
            input_ids = input_ids.to(args.device)

            # å‰å‘ä¼ æ’­
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å‰å‘ä¼ æ’­...")
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss / num_batches  # å½’ä¸€åŒ–

            # åå‘ä¼ æ’­
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åå‘ä¼ æ’­...")
            loss.backward()

            # äºŒé˜¶æ³°å‹’ï¼šç´¯ç§¯ Hessian å¯¹è§’çº¿ï¼ˆä½¿ç”¨æ¢¯åº¦å¹³æ–¹è¿‘ä¼¼ï¼‰
            if args.importance_method == 'taylor_2nd':
                for name, param in model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        # å°†æ¢¯åº¦å¹³æ–¹ç§»åŠ¨åˆ°CPUåç´¯åŠ ï¼Œé¿å…GPU OOM
                        hessian_diag[name] += (param.grad ** 2).cpu() / num_batches

            batch_time = time.time() - batch_start_time
            total_loss += loss.item() * num_batches

            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å®Œæˆï¼è€—æ—¶: {batch_time:.2f}s, loss: {loss.item() * num_batches:.4f}")

            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({
                'loss': f'{loss.item() * num_batches:.4f}',
                'batch_time': f'{batch_time:.2f}s'
            })

            # æ¸…ç†å†…å­˜
            del input_ids, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¢¯åº¦è®¡ç®—å®Œæˆ")
        logger.log(f"  å¹³å‡ loss: {total_loss:.4f}")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

        if args.importance_method == 'taylor_2nd':
            logger.log(f"  âœ“ Hessian å¯¹è§’çº¿è¿‘ä¼¼è®¡ç®—å®Œæˆ")

    elif args.importance_method == 'wanda':
        logger.log(f"\n[Step 3] æ”¶é›†æ¿€æ´»å€¼ï¼ˆWanda importanceï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {TAYLOR_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {TAYLOR_SEQ_LEN} (å†…éƒ¨å›ºå®š)")

        # åˆ†æ‰¹æ”¶é›†æ¿€æ´»
        batch_size = args.gradient_batch_size
        num_batches = (TAYLOR_NUM_SAMPLES + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        all_activations = {}
        start_time = time.time()

        pbar = tqdm(range(num_batches), desc="æ”¶é›†æ¿€æ´»", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, TAYLOR_NUM_SAMPLES)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # åŠ è½½å½“å‰æ‰¹æ¬¡
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] åŠ è½½æ•°æ®...")
            input_ids = dataset_manager.get_gradient_samples(
                num_samples=current_batch_size,
                seq_len=TAYLOR_SEQ_LEN
            )
            input_ids = input_ids.to(args.device)

            # æ”¶é›†æ¿€æ´»
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] æ”¶é›†æ¿€æ´»...")
            batch_activations = collect_layer_activations(model, input_ids, args.device)

            # ç´¯åŠ æ¿€æ´»å€¼
            for layer_idx, layer_acts in batch_activations.items():
                if layer_idx not in all_activations:
                    all_activations[layer_idx] = {}
                for name, act in layer_acts.items():
                    if name not in all_activations[layer_idx]:
                        all_activations[layer_idx][name] = act.to(args.device)
                    else:
                        all_activations[layer_idx][name] += act.to(args.device)

            batch_time = time.time() - batch_start_time
            logger.log(f"  [æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}] å®Œæˆï¼è€—æ—¶: {batch_time:.2f}s")

            pbar.set_postfix({'batch_time': f'{batch_time:.2f}s'})

            del input_ids, batch_activations
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        # å¹³å‡æ¿€æ´»å€¼
        for layer_idx in all_activations:
            for name in all_activations[layer_idx]:
                all_activations[layer_idx][name] /= num_batches

        activations = all_activations

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¿€æ´»å€¼æ”¶é›†å®Œæˆ")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

    # ========== Step 3.5: è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP å¿…éœ€ï¼‰==========
    logger.log(f"\n[Step 3.5] è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP Layer-wise é‡è¦æ€§ï¼‰...")
    logger.log(f"  æ ·æœ¬æ•°: {LAYER_IMPORTANCE_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {LAYER_IMPORTANCE_SEQ_LEN} (å†…éƒ¨å›ºå®š)")

    from core.importance.layer_analyzer import LayerImportanceAnalyzer

    # åŠ è½½ç”¨äºå±‚é‡è¦æ€§åˆ†æçš„æ ·æœ¬ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    layer_texts_list = dataset_manager.get_layer_importance_samples(
        num_samples=LAYER_IMPORTANCE_NUM_SAMPLES,
        seq_len=LAYER_IMPORTANCE_SEQ_LEN
    )

    # åˆ›å»ºåˆ†æå™¨
    analyzer = LayerImportanceAnalyzer(model, tokenizer, device=args.device)

    # è®¡ç®—æ¯å±‚çš„ç§»é™¤å›°æƒ‘åº¦
    num_layers = len(model.model.layers)
    layer_removal_ppl = analyzer.measure_layer_importance_by_removal(
        texts=layer_texts_list,
        num_layers=num_layers
    )

    logger.log(f"âœ“ å±‚ç§»é™¤å›°æƒ‘åº¦è®¡ç®—å®Œæˆ")
    logger.log(f"  ç¤ºä¾‹ - Layer 0: Removal PPL = {layer_removal_ppl[0]:.4f}")
    logger.log(f"  ç¤ºä¾‹ - Layer {num_layers//2}: Removal PPL = {layer_removal_ppl[num_layers//2]:.4f}")
    logger.log(f"  ç¤ºä¾‹ - Layer {num_layers-1}: Removal PPL = {layer_removal_ppl[num_layers-1]:.4f}")

    # ä¿å­˜å±‚ç§»é™¤å›°æƒ‘åº¦åˆ°æ–‡ä»¶
    import json
    if not hasattr(logger, 'env_name'):
        logger.env_name = 'global_results'
    if not os.path.exists(logger.env_name):
        os.makedirs(logger.env_name, exist_ok=True)

    layer_ppl_path = os.path.join(logger.env_name, 'layer_removal_ppl.json')
    with open(layer_ppl_path, 'w') as f:
        json.dump(layer_removal_ppl, f, indent=2)
    logger.log(f"âœ“ å±‚ç§»é™¤å›°æƒ‘åº¦å·²ä¿å­˜: {layer_ppl_path}")

    # ========== Step 3.6: è®¡ç®—å—ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP å¿…éœ€ï¼‰==========
    logger.log(f"\n[Step 3.6] è®¡ç®—å—ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP Block-wise é‡è¦æ€§ï¼‰...")
    logger.log(f"  æ ·æœ¬æ•°: {BLOCK_IMPORTANCE_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {BLOCK_IMPORTANCE_SEQ_LEN} (å†…éƒ¨å›ºå®š)")

    # åŠ è½½ç”¨äºå—é‡è¦æ€§åˆ†æçš„æ ·æœ¬ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    block_texts_list = dataset_manager.get_layer_importance_samples(
        num_samples=BLOCK_IMPORTANCE_NUM_SAMPLES,
        seq_len=BLOCK_IMPORTANCE_SEQ_LEN
    )

    # è®¡ç®—æ¯å±‚çš„ Attention å’Œ MLP å—ç§»é™¤å›°æƒ‘åº¦
    block_removal_ppl = analyzer.measure_block_importance_by_removal(
        texts=block_texts_list,
        num_layers=num_layers
    )

    logger.log(f"âœ“ å—ç§»é™¤å›°æƒ‘åº¦è®¡ç®—å®Œæˆ")
    logger.log(f"  ç¤ºä¾‹ - Layer 0 Attention: {block_removal_ppl['attention'][0]:.4f}, MLP: {block_removal_ppl['mlp'][0]:.4f}")
    logger.log(f"  ç¤ºä¾‹ - Layer {num_layers-1} Attention: {block_removal_ppl['attention'][num_layers-1]:.4f}, MLP: {block_removal_ppl['mlp'][num_layers-1]:.4f}")

    # ä¿å­˜å—ç§»é™¤å›°æƒ‘åº¦åˆ°æ–‡ä»¶
    block_ppl_path = os.path.join(logger.env_name, 'block_removal_ppl.json')
    with open(block_ppl_path, 'w') as f:
        json.dump(block_removal_ppl, f, indent=2)
    logger.log(f"âœ“ å—ç§»é™¤å›°æƒ‘åº¦å·²ä¿å­˜: {block_ppl_path}")

    # ========== Step 4: æ„å»ºå…¨å±€åˆ†æè¡¨ ==========
    logger.log("\n[Step 4] æ„å»ºå…¨å±€ Group åˆ†æè¡¨...")

    layer_end = args.layer_end if args.layer_end else len(model.model.layers)

    # ä¼ é€’é‡è¦æ€§ä¿¡æ¯
    importance_info = {}
    if args.importance_method in ['taylor', 'taylor_2nd']:
        importance_info['gradients'] = {name: param.grad for name, param in model.named_parameters() if param.grad is not None}
        if args.importance_method == 'taylor_2nd':
            importance_info['hessian_diag'] = hessian_diag
    elif args.importance_method == 'wanda':
        importance_info['activations'] = activations

    df = build_global_group_table(
        model=model,
        importance_method=args.importance_method,
        importance_info=importance_info,
        layer_start=args.layer_start,
        layer_end=layer_end,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        device=args.device,
        layer_removal_ppl=layer_removal_ppl,    # H-GSP: å±‚çº§é‡è¦æ€§
        block_removal_ppl=block_removal_ppl,    # H-GSP: å—çº§é‡è¦æ€§
        temperature=args.temperature,           # H-GSP: æ¸©åº¦å‚æ•° T
        tau=args.tau                           # H-GSP: é—¨æ§é˜ˆå€¼ Ï„
    )

    logger.log(f"âœ“ åˆ†æè¡¨æ„å»ºå®Œæˆ")

    # ========== Step 5: é€‰æ‹©è¦å‰ªæçš„ groups ==========
    logger.log(f"\n[Step 5] æ ¹æ®å‰ªæç‡é€‰æ‹©è¦å‰ªæçš„ groups...")

    groups_to_prune = select_groups_to_prune(
        df=df,
        pruning_ratio=args.pruning_ratio,
        total_params=total_params
    )

    logger.log(f"âœ“ é€‰ä¸­ {len(groups_to_prune)} ä¸ª groups è¿›è¡Œå‰ªæ")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = 'global_results'
    if not hasattr(logger, 'env_name'):
        logger.env_name = output_dir
    if not os.path.exists(logger.env_name):
        os.makedirs(logger.env_name, exist_ok=True)

    # ä¿å­˜åˆ†æè¡¨ï¼ˆæŒ‰scoreæ’åºï¼‰
    table_path = os.path.join(logger.env_name, 'global_group_table.csv')
    df.to_csv(table_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {table_path}")

    prune_table_path = os.path.join(logger.env_name, 'groups_to_prune.csv')
    groups_to_prune.to_csv(prune_table_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {prune_table_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„åˆ†æè¡¨
    df_by_layer = df.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    table_by_layer_path = os.path.join(logger.env_name, 'global_group_table_by_layer.csv')
    df_by_layer.to_csv(table_by_layer_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {table_by_layer_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„å‰ªæåˆ—è¡¨
    prune_by_layer = groups_to_prune.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    prune_by_layer_path = os.path.join(logger.env_name, 'groups_to_prune_by_layer.csv')
    prune_by_layer.to_csv(prune_by_layer_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {prune_by_layer_path}")

    # ç”Ÿæˆå±‚çº§ç»Ÿè®¡æ‘˜è¦
    summary_lines = []
    summary_lines.append("="*80)
    summary_lines.append("å„å±‚å‰ªæç»Ÿè®¡æ‘˜è¦")
    summary_lines.append("="*80)
    summary_lines.append(f"{'Layer':<8} {'Attentionå‰ªæ':<20} {'MLPå‰ªæ':<20} {'æ€»å‚æ•°å‰ªæ':<20}")
    summary_lines.append("-"*80)

    for layer_idx in sorted(groups_to_prune['layer_idx'].unique()):
        layer_data = groups_to_prune[groups_to_prune['layer_idx'] == layer_idx]
        attn_data = layer_data[layer_data['group_type'] == 'attention']
        mlp_data = layer_data[layer_data['group_type'] == 'mlp']

        attn_count = len(attn_data)
        mlp_count = len(mlp_data)
        attn_params = attn_data['cost'].sum() if len(attn_data) > 0 else 0
        mlp_params = mlp_data['cost'].sum() if len(mlp_data) > 0 else 0
        total_params_prune = attn_params + mlp_params

        summary_lines.append(
            f"{layer_idx:<8} "
            f"{attn_count} groups ({attn_params:,} params)".ljust(20) + " "
            f"{mlp_count} channels ({mlp_params:,} params)".ljust(20) + " "
            f"{total_params_prune:,} params".ljust(20)
        )

    summary_lines.append("-"*80)
    summary_lines.append(f"æ€»è®¡: {len(groups_to_prune)} groups, "
                        f"{groups_to_prune['cost'].sum():,} params")
    summary_lines.append("="*80)

    # ä¿å­˜æ‘˜è¦æ–‡ä»¶
    summary_path = os.path.join(logger.env_name, 'pruning_summary_by_layer.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary_lines))
    logger.log(f"âœ“ å±‚çº§ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    # ä¹Ÿåœ¨æ—¥å¿—ä¸­æ˜¾ç¤º
    logger.log("\n" + '\n'.join(summary_lines))

    # ========== Step 6: æ‰§è¡Œå…¨å±€å‰ªæ ==========
    logger.log(f"\n[Step 6] æ‰§è¡Œå…¨å±€å‰ªæ...")

    pruning_stats = apply_global_pruning(
        model=model,
        groups_to_prune_df=groups_to_prune,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        logger=logger
    )

    logger.log("\nâœ“ å…¨å±€å‰ªæå®Œæˆ")

    # ========== Step 6.5: è‡ªåŠ¨åç¼©ï¼ˆH-GSP å¿…éœ€ï¼‰==========
    logger.log(f"\n[Step 6.5] è‡ªåŠ¨åç¼©æ£€æµ‹ï¼ˆH-GSP Auto-Collapse, Îµ={args.epsilon}ï¼‰...")
    additional_empty_layers = auto_collapse(
        model=model,
        pruning_stats=pruning_stats,
        collapse_threshold=args.epsilon,
        logger=logger
    )
    # å°†é¢å¤–çš„ç©ºå±‚åŠ å…¥åˆ° empty_layers åˆ—è¡¨
    pruning_stats['empty_layers'].extend(additional_empty_layers)

    # ========== Step 7: ç§»é™¤ç©ºå±‚ï¼ˆå¯é€‰ï¼‰==========
    all_empty_layers = pruning_stats['empty_layers']
    if args.remove_empty_layers and len(all_empty_layers) > 0:
        logger.log(f"\n[Step 7] ç§»é™¤ç©ºå±‚...")
        logger.log(f"  åŸå§‹ç©ºå±‚: {len(all_empty_layers) - len(additional_empty_layers)}")
        if len(additional_empty_layers) > 0:
            logger.log(f"  åç¼©è§¦å‘: {len(additional_empty_layers)}")
        logger.log(f"  æ€»è®¡ç§»é™¤: {len(all_empty_layers)} å±‚")
        remove_empty_layers(model, all_empty_layers, logger)

    # ========== Step 8: ç»Ÿè®¡å‰ªæç»“æœ ==========
    logger.log(f"\n{'='*60}")
    logger.log(f"å‰ªæç»Ÿè®¡")
    logger.log(f"{'='*60}")

    after_params = sum(p.numel() for p in model.parameters())
    actual_ratio = (total_params - after_params) / total_params

    logger.log(f"å‚æ•°ç»Ÿè®¡:")
    logger.log(f"  å‰ªæå‰: {total_params:,}")
    logger.log(f"  å‰ªæå: {after_params:,}")
    logger.log(f"  å®é™…å‰ªæç‡: {actual_ratio:.2%}")

    if len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\nè‡ªåŠ¨æ·±åº¦å‰ªæ:")
        logger.log(f"  æ›¿æ¢ä¸ºIdentityçš„å±‚: {pruning_stats['empty_layers']}")
        logger.log(f"  ç‰©ç†å±‚æ•°: {len(model.model.layers)} (ä¿æŒä¸å˜)")
        logger.log(f"  æœ‰æ•ˆå±‚æ•°: {len(model.model.layers) - len(pruning_stats['empty_layers'])}")

    # ========== Step 9: è¯„ä¼°å‰ªæå PPL ==========
    if args.test_after_prune:
        logger.log(f"\n[Step 9] è¯„ä¼°å‰ªæå PPL...")
        pruned_ppl = PPLMetric(model, tokenizer, datasets=[args.dataset], device=args.device)
        logger.log(f"âœ“ å‰ªæå PPL: {pruned_ppl}")

        if args.test_before_prune and len(pruned_ppl.results) > 0 and len(baseline_ppl.results) > 0:
            # è·å–å¯¹åº”æ•°æ®é›†çš„ PPL keyï¼ˆä¸¤è€…åº”è¯¥ä¸€è‡´ï¼‰
            pruned_key = list(pruned_ppl.results.keys())[0]
            baseline_key = list(baseline_ppl.results.keys())[0]

            # ç¡®ä¿éƒ½ä¸æ˜¯inf
            if pruned_ppl[pruned_key] != float('inf') and baseline_ppl[baseline_key] != float('inf'):
                degradation = (pruned_ppl[pruned_key] / baseline_ppl[baseline_key] - 1) * 100
                logger.log(f"  PPL é€€åŒ–: {degradation:.2f}%")
            else:
                logger.log(f"  âš ï¸  æ— æ³•è®¡ç®—PPLé€€åŒ–ï¼ˆå­˜åœ¨infå€¼ï¼‰")

    # ========== Step 10: å¾®è°ƒæ¢å¤ï¼ˆå¯é€‰ï¼‰==========
    if args.finetune:
        logger.log(f"\n[Step 10] å¾®è°ƒæ¢å¤...")

        finetuner = FineTuner(model, tokenizer, device=args.device, logger=logger)

        finetuner.finetune(
            dataset_name=args.dataset,
            num_samples=args.finetune_samples,
            lr=args.finetune_lr,
            epochs=args.finetune_epochs,
            method=args.finetune_method,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha
        )

        logger.log(f"âœ“ å¾®è°ƒå®Œæˆ")

        # è¯„ä¼°å¾®è°ƒå PPL
        if args.test_after_prune:
            logger.log(f"\nè¯„ä¼°å¾®è°ƒå PPL...")
            finetuned_ppl = PPLMetric(model, tokenizer, datasets=[args.dataset], device=args.device)
            logger.log(f"âœ“ å¾®è°ƒå PPL: {finetuned_ppl}")

            if args.test_before_prune and len(finetuned_ppl.results) > 0 and len(baseline_ppl.results) > 0:
                finetuned_key = list(finetuned_ppl.results.keys())[0]
                baseline_key = list(baseline_ppl.results.keys())[0]

                if finetuned_ppl[finetuned_key] != float('inf') and baseline_ppl[baseline_key] != float('inf'):
                    final_degradation = (finetuned_ppl[finetuned_key] / baseline_ppl[baseline_key] - 1) * 100
                    logger.log(f"  æœ€ç»ˆ PPL é€€åŒ–: {final_degradation:.2f}%")
                else:
                    logger.log(f"  âš ï¸  æ— æ³•è®¡ç®—æœ€ç»ˆPPLé€€åŒ–ï¼ˆå­˜åœ¨infå€¼ï¼‰")

    # ========== Step 11: ä¿å­˜æ¨¡å‹ ==========
    if args.save_model:
        logger.log(f"\n[Step 11] ä¿å­˜æ¨¡å‹...")

        save_path = os.path.join(logger.env_name, 'pytorch_model.bin')

        save_dict = {
            'model': model,
            'tokenizer': tokenizer,
            'pruning_stats': pruning_stats,
            'pruning_ratio': args.pruning_ratio,
            'actual_ratio': actual_ratio,
            'method': 'global_pruning',
            'config': args.__dict__
        }

        torch.save(save_dict, save_path)
        logger.log(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {save_path}")

    logger.log(f"\n{'='*60}")
    logger.log(f"âœ“ å…¨éƒ¨å®Œæˆï¼")
    logger.log(f"{'='*60}")


if __name__ == '__main__':
    main()
