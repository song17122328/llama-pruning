#!/usr/bin/env python3
"""
åŸºäºå…¨å±€æ€§ä»·æ¯”æ’åºçš„æ··åˆç»“æ„åŒ–å‰ªæ

æ ¸å¿ƒæ€æƒ³ï¼š
- å°†å‰ªæé—®é¢˜å»ºæ¨¡ä¸ºåˆ†æ•°èƒŒåŒ…é—®é¢˜
- Score = Importance / Cost
- å…¨å±€æ’åºï¼Œä¼˜å…ˆå‰ªé™¤"æ€§ä»·æ¯”"æœ€ä½çš„ groups
- è‡ªåŠ¨å®ç°æ·±åº¦å‰ªæï¼ˆå±‚ç§»é™¤ï¼‰+ å®½åº¦å‰ªæï¼ˆç¥ç»å…ƒå‰ªé™¤ï¼‰çš„æ··åˆç­–ç•¥
"""

import os
import gc
import torch
import argparse
import time
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éGUIåç«¯
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

from core.methods.global_pruning import (
    build_global_group_table,
    select_groups_to_prune
)
from core.methods.gqa_aware import prune_attention_by_gqa_groups
from core.datasets import DatasetManager
from core.models import IdentityDecoderLayer, ZeroAttention, ZeroMLP
from evaluation.metrics.ppl import PPLMetric
from core.utils.logger import LoggerWithDepth
from core.analysis import ModelAnalyzer, ModelComparator
from core.analysis.gradient_analysis import GradientAnalyzer

import sys
# å¯¼å…¥ evaluation æ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'evaluation'))
from evaluation.run_evaluation import evaluate_single_model

def setup_chinese_font():
    """é…ç½® matplotlib ä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º"""
    import matplotlib.font_manager as fm
    
    chinese_fonts = [
        'SimHei', 'Microsoft YaHei', 'SimSun',  # Windows
        'STSong', 'STHeiti',  # Mac
        'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei',  # Linux
        'Noto Sans CJK SC', 'Noto Sans CJK',  # é€šç”¨
    ]
    
    # è·å–ç³»ç»Ÿæ‰€æœ‰å¯ç”¨å­—ä½“
    available_fonts = set([f.name for f in fm.fontManager.ttflist])
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    selected_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            selected_font = font
            plt.rcParams['font.sans-serif'] = [font, 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            return font
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å« CJK çš„å­—ä½“
    for font_obj in fm.fontManager.ttflist:
        if 'CJK' in font_obj.name or 'Chinese' in font_obj.name:
            selected_font = font_obj.name
            plt.rcParams['font.sans-serif'] = [selected_font, 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            return selected_font
    
    # å®åœ¨æ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œè¿”å› None è¡¨ç¤ºä¸æ”¯æŒä¸­æ–‡
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    return None


def generate_pruning_charts(pruning_data, model_name, output_dir, use_english=False):
    """
    ç”Ÿæˆå‰ªæç›´æ–¹å›¾ï¼ˆå‰ªæç‡å’Œä¿ç•™ç‡ï¼‰

    Args:
        pruning_data: pruning_comparison æ•°æ®
        model_name: æ¨¡å‹åç§°
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        use_english: æ˜¯å¦ä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼ˆå½“ä¸­æ–‡å­—ä½“ä¸å¯ç”¨æ—¶ï¼‰
    """
    if not pruning_data or 'layers' not in pruning_data:
        return

    layers = pruning_data['layers']
    if not layers:
        return

    # æå–æ¯å±‚çš„å‰ªæç‡å’Œä¿ç•™ç‡
    pruning_ratios = []
    retention_ratios = []
    layer_indices = []

    # æå–æ¯å±‚çš„ MLP å’Œ Attention å‰ªææ¯”é‡ï¼ˆå æ€»å‚æ•°çš„æ¯”ä¾‹ï¼‰
    mlp_pruning_ratios = []
    attention_pruning_ratios = []

    # æå–æ¯å±‚çš„ Attention å’Œ MLP å„è‡ªçš„å‰ªæç‡
    attention_reduction_ratios = []  # Attentionå‰ªæå‚æ•° / åŸå§‹Attentionå‚æ•°
    mlp_reduction_ratios = []        # MLPå‰ªæå‚æ•° / åŸå§‹MLPå‚æ•°

    for layer in layers:
        if 'total' in layer and 'reduction_ratio' in layer['total']:
            layer_indices.append(layer['layer_idx'])
            pruning_ratio = layer['total']['reduction_ratio']
            pruning_ratios.append(pruning_ratio * 100)
            retention_ratios.append((1.0 - pruning_ratio) * 100)

            # è®¡ç®— MLP å’Œ Attention çš„å‰ªææ¯”é‡ï¼ˆå‰ªæå‚æ•°æ•° / åŸå§‹å±‚æ€»å‚æ•°ï¼‰
            total_original = layer['total']['original']

            # MLP å‰ªææ¯”é‡ = MLPå‰ªæå‚æ•° / åŸå§‹å±‚æ€»å‚æ•°
            if total_original > 0 and 'mlp' in layer:
                mlp_reduced = layer['mlp'].get('reduced', 0)
                mlp_pruning_ratios.append(mlp_reduced / total_original * 100)
            else:
                mlp_pruning_ratios.append(0)

            # Attention å‰ªææ¯”é‡ = Attentionå‰ªæå‚æ•° / åŸå§‹å±‚æ€»å‚æ•°
            if total_original > 0 and 'attention' in layer:
                attention_reduced = layer['attention'].get('reduced', 0)
                attention_pruning_ratios.append(attention_reduced / total_original * 100)
            else:
                attention_pruning_ratios.append(0)

            # Attention è‡ªèº«çš„å‰ªæç‡ = Attentionå‰ªæå‚æ•° / åŸå§‹Attentionæ€»å‚æ•°
            if 'attention' in layer and layer['attention'].get('original', 0) > 0:
                attention_original = layer['attention']['original']
                attention_reduced = layer['attention'].get('reduced', 0)
                attention_reduction_ratios.append(attention_reduced / attention_original * 100)
            else:
                attention_reduction_ratios.append(0)

            # MLP è‡ªèº«çš„å‰ªæç‡ = MLPå‰ªæå‚æ•° / åŸå§‹MLPæ€»å‚æ•°
            if 'mlp' in layer and layer['mlp'].get('original', 0) > 0:
                mlp_original = layer['mlp']['original']
                mlp_reduced = layer['mlp'].get('reduced', 0)
                mlp_reduction_ratios.append(mlp_reduced / mlp_original * 100)
            else:
                mlp_reduction_ratios.append(0)

    if not pruning_ratios:
        return

    # è·å–æ•´ä½“å’Œå±‚ç›®æ ‡æ¯”ä¾‹
    total_ratio = None
    layer_target_ratio = None

    if 'total_params' in pruning_data and 'reduction_ratio' in pruning_data['total_params']:
        total_reduction = pruning_data['total_params']['reduction_ratio'] * 100
        total_ratio = total_reduction

    if 'layer_params' in pruning_data and 'reduction_ratio' in pruning_data['layer_params']:
        layer_reduction = pruning_data['layer_params']['reduction_ratio'] * 100
        layer_target_ratio = layer_reduction

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # å®šä¹‰æ ‡ç­¾ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰
    if use_english:
        labels = {
            'pruning_ylabel': 'Pruning Ratio (%)',
            'retention_ylabel': 'Retention Ratio (%)',
            'xlabel': 'Layer Index',
            'pruning_title': f'{model_name} - Pruning Ratio per Layer',
            'retention_title': f'{model_name} - Retention Ratio per Layer',
            'pruning_title_full': f'{model_name} - Pruning Ratio (Overall: {{0:.1f}}%, Layer Target: {{1:.1f}}%)',
            'retention_title_full': f'{model_name} - Retention Ratio (Overall: {{0:.1f}}%, Layer Target: {{1:.1f}}%)',
            'pruning_legend': 'Layer Target Pruning: {0:.1f}%',
            'retention_legend': 'Layer Target Retention: {0:.1f}%',
        }
    else:
        labels = {
            'pruning_ylabel': 'å‰ªææ¯”ä¾‹ (%)',
            'retention_ylabel': 'ä¿ç•™æ¯”ä¾‹ (%)',
            'xlabel': 'å±‚ç´¢å¼•',
            'pruning_title': f'{model_name} - å„å±‚å‰ªææ¯”ä¾‹',
            'retention_title': f'{model_name} - å„å±‚ä¿ç•™æ¯”ä¾‹',
            'pruning_title_full': f'{model_name} - å„å±‚å‰ªææ¯”ä¾‹ (æ¨¡å‹æ•´ä½“: {{0:.1f}}%, å±‚ç›®æ ‡: {{1:.1f}}%)',
            'retention_title_full': f'{model_name} - å„å±‚ä¿ç•™æ¯”ä¾‹ (æ¨¡å‹æ•´ä½“: {{0:.1f}}%, å±‚ç›®æ ‡: {{1:.1f}}%)',
            'pruning_legend': 'å±‚ç›®æ ‡å‰ªæ: {0:.1f}%',
            'retention_legend': 'å±‚ç›®æ ‡ä¿ç•™: {0:.1f}%',
        }

    # ç”Ÿæˆä¸¤ä¸ªå›¾è¡¨ï¼šå‰ªæç‡å’Œä¿ç•™ç‡
    for chart_type, ratios in [('pruning', pruning_ratios), ('retention', retention_ratios)]:
        fig, ax = plt.subplots(figsize=(14, 6))

        # è®¡ç®—å½“å‰å›¾è¡¨å¯¹åº”çš„ç›®æ ‡æ¯”ä¾‹
        if chart_type == 'pruning':
            target = layer_target_ratio
            colors = ['#e74c3c' if r >= target else '#3498db' for r in ratios] if target else ['#3498db'] * len(ratios)
            ylabel = labels['pruning_ylabel']
            if total_ratio and target:
                title = labels['pruning_title_full'].format(total_ratio, target)
            else:
                title = labels['pruning_title']
            line_color = '#ff8c00'
            line_label = labels['pruning_legend'].format(target) if target else None
        else:  # retention
            target = (100 - layer_target_ratio) if layer_target_ratio else None
            colors = ['#27ae60' if r >= target else '#e67e22' for r in ratios] if target else ['#27ae60'] * len(ratios)
            ylabel = labels['retention_ylabel']
            total_ret = (100 - total_ratio) if total_ratio else None
            if total_ret and target:
                title = labels['retention_title_full'].format(total_ret, target)
            else:
                title = labels['retention_title']
            line_color = '#27ae60'
            line_label = labels['retention_legend'].format(target) if target else None

        # ç»˜åˆ¶ç›´æ–¹å›¾
        bars = ax.bar(layer_indices, ratios, color=colors, edgecolor='black', linewidth=0.5)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, ratio in zip(bars, ratios):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{ratio:.1f}%',
                   ha='center', va='bottom', fontsize=8)

        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel(labels['xlabel'], fontsize=12, fontweight='bold')
        ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')
        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(layer_indices)
        ax.set_xticklabels([str(i) for i in layer_indices], fontsize=9)
        ax.set_ylim(0, 105)

        # æ·»åŠ ç½‘æ ¼çº¿
        for y in [20, 40, 60, 80, 100]:
            ax.axhline(y=y, color='lightgray', linestyle=':', linewidth=0.8, alpha=0.6, zorder=1)

        # æ·»åŠ ç›®æ ‡çº¿
        if target is not None and line_label:
            ax.axhline(y=target, color=line_color, linestyle='--', linewidth=2.5, alpha=0.9,
                      label=line_label, zorder=3)
            ax.legend(loc='upper right', fontsize=10, framealpha=0.9)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        chart_path = output_path / f"{chart_type}_ratio.png"
        plt.savefig(str(chart_path), dpi=300, bbox_inches='tight')
        plt.close()

        print(f"  âœ“ å·²ç”Ÿæˆ: {chart_path}")

    # ========== ç”Ÿæˆå‰ªææ¯”é‡ç»†åˆ†å›¾è¡¨ï¼ˆMLP vs Attentionï¼‰==========
    fig, ax = plt.subplots(figsize=(14, 6))

    # åˆ›å»ºå †å æŸ±çŠ¶å›¾
    width = 0.8
    x_pos = range(len(layer_indices))

    # ç»˜åˆ¶ MLP å‰ªææ¯”é‡ï¼ˆåº•éƒ¨ï¼Œè“è‰²ï¼‰
    bars_mlp = ax.bar(x_pos, mlp_pruning_ratios, width,
                      label='MLP' if use_english else 'MLP å‰ªææ¯”é‡',
                      color='#3498db', edgecolor='black', linewidth=0.5)

    # ç»˜åˆ¶ Attention å‰ªææ¯”é‡ï¼ˆå †å åœ¨ MLP ä¸Šæ–¹ï¼Œçº¢è‰²ï¼‰
    bars_attn = ax.bar(x_pos, attention_pruning_ratios, width,
                       bottom=mlp_pruning_ratios,
                       label='Attention' if use_english else 'Attention å‰ªææ¯”é‡',
                       color='#e74c3c', edgecolor='black', linewidth=0.5)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼ˆæ˜¾ç¤º MLP å’Œ Attention çš„æ¯”é‡ï¼‰
    for i, (mlp_ratio, attn_ratio) in enumerate(zip(mlp_pruning_ratios, attention_pruning_ratios)):
        # MLP æ ‡ç­¾ï¼ˆåœ¨ MLP æŸ±å­ä¸­é—´ï¼‰
        if mlp_ratio > 2:  # åªæœ‰å½“æ¯”é‡è¶³å¤Ÿå¤§æ—¶æ‰æ˜¾ç¤º
            ax.text(i, mlp_ratio / 2, f'{mlp_ratio:.1f}%',
                   ha='center', va='center', fontsize=7, color='white', fontweight='bold')

        # Attention æ ‡ç­¾ï¼ˆåœ¨ Attention æŸ±å­ä¸­é—´ï¼‰
        if attn_ratio > 2:  # åªæœ‰å½“æ¯”é‡è¶³å¤Ÿå¤§æ—¶æ‰æ˜¾ç¤º
            ax.text(i, mlp_ratio + attn_ratio / 2, f'{attn_ratio:.1f}%',
                   ha='center', va='center', fontsize=7, color='white', fontweight='bold')

        # æ€»å‰ªæç‡æ ‡ç­¾ï¼ˆåœ¨æŸ±å­é¡¶éƒ¨ï¼‰
        total_ratio = mlp_ratio + attn_ratio
        ax.text(i, total_ratio + 1, f'{total_ratio:.1f}%',
               ha='center', va='bottom', fontsize=8, fontweight='bold')

    # è®¾ç½®åæ ‡è½´
    if use_english:
        ax.set_xlabel('Layer Index', fontsize=12, fontweight='bold')
        ax.set_ylabel('Pruning Ratio (% of Total Layer Params)', fontsize=12, fontweight='bold')
        title = f'{model_name} - Pruning Breakdown: MLP vs Attention'
        if layer_target_ratio:
            title += f' (Target: {layer_target_ratio:.1f}%)'
    else:
        ax.set_xlabel('å±‚ç´¢å¼•', fontsize=12, fontweight='bold')
        ax.set_ylabel('å‰ªææ¯”é‡ (å æ€»å‚æ•°çš„ç™¾åˆ†æ¯”)', fontsize=12, fontweight='bold')
        title = f'{model_name} - å‰ªææ¯”é‡ç»†åˆ†ï¼šMLP vs Attention'
        if layer_target_ratio:
            title += f' (ç›®æ ‡: {layer_target_ratio:.1f}%)'

    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(x_pos)
    ax.set_xticklabels([str(i) for i in layer_indices], fontsize=9)
    ax.set_ylim(0, 105)

    # æ·»åŠ ç½‘æ ¼çº¿
    for y in [20, 40, 60, 80, 100]:
        ax.axhline(y=y, color='lightgray', linestyle=':', linewidth=0.8, alpha=0.6, zorder=1)

    # æ·»åŠ ç›®æ ‡çº¿ï¼ˆæ€»å‰ªæç›®æ ‡ï¼‰
    if layer_target_ratio is not None:
        ax.axhline(y=layer_target_ratio, color='#ff8c00', linestyle='--', linewidth=2.5, alpha=0.9,
                  label=f'{"Target" if use_english else "ç›®æ ‡å‰ªæç‡"}: {layer_target_ratio:.1f}%', zorder=3)

    # æ·»åŠ å›¾ä¾‹
    ax.legend(loc='upper right', fontsize=10, framealpha=0.9)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    breakdown_chart_path = output_path / "pruning_ratio_breakdown.png"
    plt.savefig(str(breakdown_chart_path), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"  âœ“ å·²ç”Ÿæˆ: {breakdown_chart_path}")

    # ========== ç”Ÿæˆ Attention è‡ªèº«å‰ªæç‡å›¾è¡¨ ==========
    fig, ax = plt.subplots(figsize=(14, 6))

    # è®¾ç½®é¢œè‰²ï¼ˆçº¢è‰²ç³»ï¼‰
    colors = ['#e74c3c' if r >= 50 else '#3498db' for r in attention_reduction_ratios]

    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    bars = ax.bar(layer_indices, attention_reduction_ratios, color=colors,
                  edgecolor='black', linewidth=0.5)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ratio in zip(bars, attention_reduction_ratios):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{ratio:.1f}%',
               ha='center', va='bottom', fontsize=8)

    # è®¾ç½®åæ ‡è½´
    if use_english:
        ax.set_xlabel('Layer Index', fontsize=12, fontweight='bold')
        ax.set_ylabel('Attention Pruning Ratio (%)', fontsize=12, fontweight='bold')
        title = f'{model_name} - Attention Module Pruning Ratio per Layer'
    else:
        ax.set_xlabel('å±‚ç´¢å¼•', fontsize=12, fontweight='bold')
        ax.set_ylabel('Attention å‰ªæç‡ (%)', fontsize=12, fontweight='bold')
        title = f'{model_name} - å„å±‚ Attention æ¨¡å—å‰ªæç‡'

    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(layer_indices)
    ax.set_xticklabels([str(i) for i in layer_indices], fontsize=9)
    ax.set_ylim(0, 105)

    # æ·»åŠ ç½‘æ ¼çº¿
    for y in [20, 40, 60, 80, 100]:
        ax.axhline(y=y, color='lightgray', linestyle=':', linewidth=0.8, alpha=0.6, zorder=1)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    attention_chart_path = output_path / "attention_pruning_ratio.png"
    plt.savefig(str(attention_chart_path), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"  âœ“ å·²ç”Ÿæˆ: {attention_chart_path}")

    # ========== ç”Ÿæˆ MLP è‡ªèº«å‰ªæç‡å›¾è¡¨ ==========
    fig, ax = plt.subplots(figsize=(14, 6))

    # è®¾ç½®é¢œè‰²ï¼ˆè“è‰²ç³»ï¼‰
    colors = ['#e74c3c' if r >= 50 else '#3498db' for r in mlp_reduction_ratios]

    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    bars = ax.bar(layer_indices, mlp_reduction_ratios, color=colors,
                  edgecolor='black', linewidth=0.5)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ratio in zip(bars, mlp_reduction_ratios):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{ratio:.1f}%',
               ha='center', va='bottom', fontsize=8)

    # è®¾ç½®åæ ‡è½´
    if use_english:
        ax.set_xlabel('Layer Index', fontsize=12, fontweight='bold')
        ax.set_ylabel('MLP Pruning Ratio (%)', fontsize=12, fontweight='bold')
        title = f'{model_name} - MLP Module Pruning Ratio per Layer'
    else:
        ax.set_xlabel('å±‚ç´¢å¼•', fontsize=12, fontweight='bold')
        ax.set_ylabel('MLP å‰ªæç‡ (%)', fontsize=12, fontweight='bold')
        title = f'{model_name} - å„å±‚ MLP æ¨¡å—å‰ªæç‡'

    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
    ax.set_xticks(layer_indices)
    ax.set_xticklabels([str(i) for i in layer_indices], fontsize=9)
    ax.set_ylim(0, 105)

    # æ·»åŠ ç½‘æ ¼çº¿
    for y in [20, 40, 60, 80, 100]:
        ax.axhline(y=y, color='lightgray', linestyle=':', linewidth=0.8, alpha=0.6, zorder=1)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    mlp_chart_path = output_path / "mlp_pruning_ratio.png"
    plt.savefig(str(mlp_chart_path), dpi=300, bbox_inches='tight')
    plt.close()

    print(f"  âœ“ å·²ç”Ÿæˆ: {mlp_chart_path}")


def get_model_gqa_config(model):
    """
    è‡ªåŠ¨æ£€æµ‹æ¨¡å‹çš„ GQA é…ç½®

    æ”¯æŒ LLaMAã€Mistralã€Qwen ç­‰æ¨¡å‹çš„è‡ªåŠ¨é…ç½®æ£€æµ‹

    Args:
        model: HuggingFace model

    Returns:
        tuple: (num_attention_heads, num_key_value_heads, gqa_ratio, head_dim)
    """
    config = model.config

    # è·å– attention heads æ•°é‡
    num_attention_heads = config.num_attention_heads
    # æœ‰äº›æ¨¡å‹å¯èƒ½æ²¡æœ‰ num_key_value_heads å­—æ®µï¼ˆå¦‚æ—§çš„ MHA æ¨¡å‹ï¼‰
    num_key_value_heads = getattr(config, 'num_key_value_heads', num_attention_heads)

    # è®¡ç®— GQA ratio
    gqa_ratio = num_attention_heads // num_key_value_heads

    # è®¡ç®— head_dim
    hidden_size = config.hidden_size
    head_dim = hidden_size // num_attention_heads

    return num_attention_heads, num_key_value_heads, gqa_ratio, head_dim


def setup_output_directories(base_dir):
    """
    åˆ›å»ºç»Ÿä¸€çš„è¾“å‡ºç›®å½•ç»“æ„

    Args:
        base_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼ˆä¾‹å¦‚: prune_log/my_experimentï¼‰

    Returns:
        dict: åŒ…å«å„ä¸ªå­ç›®å½•è·¯å¾„çš„å­—å…¸
    """
    dirs = {
        'base': base_dir,
        # 'models': os.path.join(base_dir, 'models'),           # ä¿å­˜æ¨¡å‹
        'models': base_dir,   
        'analysis': os.path.join(base_dir, 'analysis'),       # ä¿å­˜ä¸­é—´åˆ†æç»“æœ
        'evaluation': os.path.join(base_dir, 'evaluation'),   # ä¿å­˜è¯„ä¼°ç»“æœ
        'logs': os.path.join(base_dir, 'logs'),               # ä¿å­˜æ—¥å¿—
        'visualization': os.path.join(base_dir, 'visualization')  # ä¿å­˜å‰ªæå¯è§†åŒ–ç»“æœ
    }

    # åˆ›å»ºæ‰€æœ‰ç›®å½•
    for dir_path in dirs.values():
        os.makedirs(dir_path, exist_ok=True)

    return dirs


def collect_layer_activations(model, input_ids, device='cuda'):
    """
    æ”¶é›†æ¯å±‚çš„æ¿€æ´»å€¼ç”¨äº Wanda æ–¹æ³• (ä¿®æ­£ç‰ˆï¼šL2 Norm + æ­£ç¡®çš„ Hook ä½ç½®)

    å…³é”®ä¿®æ­£ï¼š
    1. ä½¿ç”¨ L2 Norm è€Œé Mean (ç¬¦åˆ Wanda è®ºæ–‡)
    2. ç›´æ¥ Hook down_proj è·å–åŒ…å« SwiGLU ä½œç”¨åçš„çœŸå®è¾“å…¥

    Returns:
        activations: Dict[layer_idx -> Dict[name -> Tensor]]
                    æ¯ä¸ª Tensor æ˜¯ [hidden_dim] çš„ L2 Norm
    """
    activations = {}
    hooks = []

    def get_activation_hook(layer_idx, name):
        def hook(module, input, output):
            if layer_idx not in activations:
                activations[layer_idx] = {}

            # æå–è¾“å…¥æ¿€æ´»å€¼
            if isinstance(input, tuple):
                x = input[0].detach()
            else:
                x = input.detach()

            # å±•å¹³ Batch å’Œ Seq ç»´åº¦ -> [Total_Tokens, Hidden]
            x = x.reshape(-1, x.shape[-1])

            # Wanda æ ‡å‡†ï¼šè®¡ç®—æ¯ä¸ª Input Channel çš„ L2 Norm
            # L2 Norm = sqrt(sum(x^2)) over all tokens
            norm = x.pow(2).sum(dim=0).sqrt().cpu()

            activations[layer_idx][name] = norm
        return hook

    # ä¸ºæ¯å±‚çš„å…³é”®æ¨¡å—æ³¨å†Œ hooks
    for layer_idx, layer in enumerate(model.model.layers):
        # Attention çš„è¾“å…¥æ¿€æ´»
        hooks.append(layer.self_attn.q_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'q_proj')))
        hooks.append(layer.self_attn.k_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'k_proj')))
        hooks.append(layer.self_attn.v_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'v_proj')))
        hooks.append(layer.self_attn.o_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'o_proj')))

        # MLP è¾“å…¥æ¿€æ´»
        hooks.append(layer.mlp.gate_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'gate_proj')))
        hooks.append(layer.mlp.up_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'up_proj')))

        # ã€å…³é”®ä¿®æ­£ã€‘ç›´æ¥ Hook down_projï¼Œè·å–åŒ…å« Gate ä½œç”¨åçš„çœŸå®è¾“å…¥
        # down_proj çš„è¾“å…¥æ˜¯ SiLU(gate_proj(x)) * up_proj(x)
        hooks.append(layer.mlp.down_proj.register_forward_hook(
            get_activation_hook(layer_idx, 'down_proj')))

    # æ‰§è¡Œå‰å‘ä¼ æ’­
    with torch.no_grad():
        model(input_ids)

    # ç§»é™¤æ‰€æœ‰ hooks
    for hook in hooks:
        hook.remove()

    return activations


def apply_global_pruning(model, groups_to_prune_df, head_dim=128, gqa_ratio=4, logger=None):
    """
    æ ¹æ®å…¨å±€åˆ†æè¡¨æ‰§è¡Œå®é™…å‰ªæ

    Args:
        model: æ¨¡å‹
        groups_to_prune_df: è¦å‰ªæçš„ groups DataFrame
        head_dim: attention head ç»´åº¦
        gqa_ratio: Q:KV æ¯”ä¾‹
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        pruned_layers: è¢«å®Œå…¨å‰ªç©ºçš„å±‚åˆ—è¡¨
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log("\n" + "="*60)
    log("æ‰§è¡Œå…¨å±€å‰ªæ")
    log("="*60)

    num_layers = len(model.model.layers)
    pruning_stats = {
        'attention': {},  # {layer_idx: (old_kv, new_kv)}
        'mlp': {},        # {layer_idx: (old_channels, new_channels)}
        'empty_layers': []
    }

    # æŒ‰å±‚ç»„ç»‡è¦å‰ªæçš„ groups
    layer_prune_info = {}
    for layer_idx in range(num_layers):
        layer_data = groups_to_prune_df[groups_to_prune_df['layer_idx'] == layer_idx]

        attn_groups = layer_data[layer_data['group_type'] == 'attention']['group_idx'].tolist()
        mlp_groups = layer_data[layer_data['group_type'] == 'mlp']['group_idx'].tolist()

        layer_prune_info[layer_idx] = {
            'attention': attn_groups,
            'mlp': mlp_groups
        }

    # æ‰§è¡Œå‰ªæ
    for layer_idx in range(num_layers):
        layer = model.model.layers[layer_idx]
        prune_info = layer_prune_info[layer_idx]

        log(f"\nå¤„ç† Layer {layer_idx}:")

        # ========== Attention å‰ªæ ==========
        attn_prune_indices = prune_info['attention']

        if len(attn_prune_indices) > 0:
            # è·å–å½“å‰ KV heads æ•°é‡ï¼ˆä»æƒé‡å½¢çŠ¶æ¨æ–­ï¼‰
            k_proj_out_features = layer.self_attn.k_proj.out_features
            num_kv_heads = k_proj_out_features // head_dim

            # è®¡ç®—ä¿ç•™çš„ indices
            all_kv_indices = set(range(num_kv_heads))
            keep_kv_indices = sorted(list(all_kv_indices - set(attn_prune_indices)))

            # ä»æƒé‡å½¢çŠ¶è·å– Q heads æ•°é‡
            q_proj_out_features = layer.self_attn.q_proj.out_features
            old_q = q_proj_out_features // head_dim
            old_kv = num_kv_heads

            if len(keep_kv_indices) == 0:
                # æ‰€æœ‰headséƒ½è¢«å‰ªæï¼Œæ›¿æ¢ä¸º ZeroAttention
                # åˆ©ç”¨æ®‹å·®è¿æ¥ï¼šhidden = hidden + 0 = hiddenï¼ˆè·³è¿‡Attentionï¼‰
                log(f"  âš ï¸ Attention è¢«å®Œå…¨å‰ªç©ºï¼ˆ{old_q}Q:{old_kv}KV â†’ 0ï¼‰ï¼Œæ›¿æ¢ä¸º ZeroAttention")
                # ä¼ å…¥æ¨¡å‹ç±»å‹ä»¥ç¡®ä¿è¿”å›å€¼æ ¼å¼æ­£ç¡®ï¼ˆMistral ç­‰æ¨¡å‹æœ‰ç‰¹æ®Šæ ¼å¼ï¼‰
                layer.self_attn = ZeroAttention(model_type=model.config.model_type)
                pruning_stats['attention'][layer_idx] = (old_kv, 0)
            else:
                # æ‰§è¡Œéƒ¨åˆ†å‰ªæ
                new_q, new_kv = prune_attention_by_gqa_groups(
                    layer,
                    keep_kv_indices,
                    head_dim=head_dim,
                    gqa_ratio=gqa_ratio
                )
                log(f"  Attention: {old_q}Q:{old_kv}KV â†’ {new_q}Q:{new_kv}KV")
                pruning_stats['attention'][layer_idx] = (old_kv, new_kv)

        # ========== MLP å‰ªæ ==========
        mlp_prune_indices = prune_info['mlp']

        if len(mlp_prune_indices) > 0:
            intermediate_size = layer.mlp.gate_proj.out_features

            # è®¡ç®—ä¿ç•™çš„ indices
            all_mlp_indices = set(range(intermediate_size))
            keep_mlp_indices = sorted(list(all_mlp_indices - set(mlp_prune_indices)))

            # æœ€å°ç»´åº¦é˜ˆå€¼ï¼šå°äºç­‰äºæ­¤å€¼æ—¶æ›¿æ¢ä¸º ZeroMLP
            # åŸå› ï¼šintermediate_size=1 æ—¶å­˜åœ¨æ•°å€¼ä¸ç¨³å®šå’Œå†…å­˜å¸ƒå±€é—®é¢˜
            MIN_MLP_DIM = 1

            if len(keep_mlp_indices) <= MIN_MLP_DIM:
                # ç»´åº¦è¿‡å°ï¼Œæ›¿æ¢ä¸º ZeroMLP
                # åˆ©ç”¨æ®‹å·®è¿æ¥ï¼šhidden = hidden + 0 = hiddenï¼ˆè·³è¿‡MLPï¼‰
                log(f"  âš ï¸ MLP ç»´åº¦è¿‡å°ï¼ˆ{intermediate_size} â†’ {len(keep_mlp_indices)} channelsï¼‰ï¼Œæ›¿æ¢ä¸º ZeroMLP")
                layer.mlp = ZeroMLP()
                pruning_stats['mlp'][layer_idx] = (intermediate_size, 0)
            else:
                # æ‰§è¡Œéƒ¨åˆ† MLP å‰ªæ
                keep_mlp_indices_tensor = torch.tensor(keep_mlp_indices, device=layer.mlp.gate_proj.weight.device)

                # å‰ªæ gate_proj å’Œ up_projï¼ˆä¿ç•™å¯¹åº”çš„è¡Œï¼‰
                # é‡è¦ï¼šä½¿ç”¨ .contiguous() ç¡®ä¿å†…å­˜è¿ç»­ï¼Œé¿å… SDPA ç­‰æ“ä½œæŠ¥é”™
                layer.mlp.gate_proj.weight = torch.nn.Parameter(
                    layer.mlp.gate_proj.weight[keep_mlp_indices_tensor, :].contiguous()
                )
                # å‰ªæ gate_proj biasï¼ˆå¦‚æœå­˜åœ¨ï¼Œç”¨äº Qwen2.5 ç­‰æ¨¡å‹ï¼‰
                if layer.mlp.gate_proj.bias is not None:
                    layer.mlp.gate_proj.bias = torch.nn.Parameter(
                        layer.mlp.gate_proj.bias[keep_mlp_indices_tensor].contiguous()
                    )


                layer.mlp.up_proj.weight = torch.nn.Parameter(
                    layer.mlp.up_proj.weight[keep_mlp_indices_tensor, :].contiguous()
                )
                # å‰ªæ up_proj biasï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if layer.mlp.up_proj.bias is not None:
                    layer.mlp.up_proj.bias = torch.nn.Parameter(
                        layer.mlp.up_proj.bias[keep_mlp_indices_tensor].contiguous()
                    )

                # å‰ªæ down_projï¼ˆä¿ç•™å¯¹åº”çš„åˆ—ï¼‰
                layer.mlp.down_proj.weight = torch.nn.Parameter(
                    layer.mlp.down_proj.weight[:, keep_mlp_indices_tensor].contiguous()
                )
                # down_proj bias ä¸éœ€è¦å‰ªæï¼ˆåªå‰ªäº†è¾“å…¥ç»´åº¦ï¼Œè¾“å‡ºç»´åº¦ä¸å˜ï¼‰
                # if layer.mlp.down_proj.bias is not None:
                #     # down_proj.bias ä¸éœ€è¦å‰ªæ

                # æ›´æ–° intermediate_size
                new_intermediate_size = len(keep_mlp_indices)
                layer.mlp.gate_proj.out_features = new_intermediate_size
                layer.mlp.up_proj.out_features = new_intermediate_size
                layer.mlp.down_proj.in_features = new_intermediate_size

                log(f"  MLP: {intermediate_size} â†’ {new_intermediate_size} channels")
                pruning_stats['mlp'][layer_idx] = (intermediate_size, new_intermediate_size)

        # æ£€æŸ¥æ˜¯å¦æ•´å±‚è¢«å‰ªç©º
        attn_empty = (layer_idx in pruning_stats['attention'] and
                     pruning_stats['attention'][layer_idx][1] == 0)
        mlp_empty = (layer_idx in pruning_stats['mlp'] and
                    pruning_stats['mlp'][layer_idx][1] == 0)

        if attn_empty and mlp_empty:
            log(f"  ğŸ”´ Layer {layer_idx} è¢«å®Œå…¨å‰ªç©ºï¼ˆè‡ªåŠ¨æ·±åº¦å‰ªæï¼‰")
            pruning_stats['empty_layers'].append(layer_idx)

    return pruning_stats


def remove_empty_layers(model, empty_layers, logger=None):
    """
    "ç§»é™¤"è¢«å®Œå…¨å‰ªç©ºçš„å±‚ - é€šè¿‡æ›¿æ¢ä¸º identity å±‚

    æ³¨æ„ï¼šç”±äº HuggingFace Transformers çš„å†…éƒ¨å®ç°å¯èƒ½åœ¨å¤šå¤„å‡è®¾å±‚æ•°å›ºå®šï¼Œ
    å®Œå…¨åˆ é™¤å±‚å¯èƒ½å¯¼è‡´ "list index out of range" é”™è¯¯ã€‚
    å› æ­¤æˆ‘ä»¬é‡‡ç”¨æ›´å®‰å…¨çš„ç­–ç•¥ï¼šå°†ç©ºå±‚æ›¿æ¢ä¸ºç®€å•çš„ pass-through å±‚ã€‚

    Args:
        model: æ¨¡å‹
        empty_layers: è¦ç§»é™¤çš„å±‚ç´¢å¼•åˆ—è¡¨
        logger: æ—¥å¿—è®°å½•å™¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    if len(empty_layers) == 0:
        log("\nâœ“ æ²¡æœ‰å±‚è¢«å®Œå…¨å‰ªç©ºï¼Œè·³è¿‡å±‚ç§»é™¤")
        return

    log(f"\n{'='*60}")
    log(f"ç§»é™¤å®Œå…¨å‰ªç©ºçš„å±‚")
    log(f"{'='*60}")
    log(f"è¦ç§»é™¤çš„å±‚: {empty_layers}")
    log(f"ç­–ç•¥: æ›¿æ¢ä¸º Identity å±‚ï¼ˆä¿æŒæ¨¡å‹ç»“æ„å®Œæ•´ï¼‰")

    # ä¸ºäº†é¿å… HuggingFace Transformers å†…éƒ¨çš„å„ç§å‡è®¾è¢«æ‰“ç ´
    # æˆ‘ä»¬ä¸åˆ é™¤å±‚ï¼Œè€Œæ˜¯å°†å®ƒä»¬æ›¿æ¢ä¸ºå…¨å±€å®šä¹‰çš„ IdentityDecoderLayer
    num_layers = len(model.model.layers)

    # æ›¿æ¢ç©ºå±‚ä¸º identity å±‚
    for layer_idx in empty_layers:
        if layer_idx < num_layers:
            log(f"  æ›¿æ¢ Layer {layer_idx} ä¸º Identity å±‚")
            # è·å–åŸå§‹å±‚å’Œé…ç½®ï¼Œä»¥ä¾¿å¤åˆ¶å¿…è¦çš„å±æ€§ï¼ˆå¦‚ Qwen2 çš„ attention_typeï¼‰
            original_layer = model.model.layers[layer_idx]
            model.model.layers[layer_idx] = IdentityDecoderLayer(
                original_layer=original_layer,
                config=model.config,
                layer_idx=layer_idx
            )

    log(f"âœ“ å·²æ›¿æ¢ {len(empty_layers)} å±‚ä¸º Identity å±‚")
    log(f"  ç‰©ç†å±‚æ•°: {num_layers} (ä¿æŒä¸å˜)")
    log(f"  æœ‰æ•ˆå±‚æ•°: {num_layers - len(empty_layers)}")

    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶å¤„äºevalæ¨¡å¼
    device = next(model.parameters()).device
    model.eval()

    log(f"âœ“ æ¨¡å‹çŠ¶æ€å·²åˆ·æ–°")

    # éªŒè¯æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸forwardï¼ˆä½¿ç”¨ä¸€ä¸ªå°çš„dummyè¾“å…¥ï¼‰
    try:
        with torch.no_grad():
            dummy_input = torch.randint(0, 1000, (1, 10)).to(device)
            _ = model(dummy_input)
        log(f"âœ“ æ¨¡å‹forwardéªŒè¯é€šè¿‡")
    except Exception as e:
        log(f"âš ï¸  æ¨¡å‹forwardéªŒè¯å¤±è´¥: {e}")
        log(f"   è¿™å¯èƒ½ä¼šå¯¼è‡´åç»­PPLè®¡ç®—å‡ºé”™")
        import traceback
        log(f"   é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")


def auto_collapse(model, pruning_stats, collapse_threshold=0.15, logger=None):
    """
    è‡ªåŠ¨åç¼©ï¼šæ£€æµ‹ç¨€ç–å±‚å¹¶å¼ºåˆ¶ç§»é™¤æ•´å±‚

    H-GSP æ ¸å¿ƒæ€æƒ³ï¼šé¿å…"ç•™ 10% ä¸å¦‚ä¸ç•™"çš„æƒ…å†µ
    å½“æŸå±‚çš„å‚æ•°ä¿ç•™ç‡ä½äºé˜ˆå€¼æ—¶ï¼Œç›´æ¥ç§»é™¤æ•´å±‚

    Args:
        model: æ¨¡å‹
        pruning_stats: å‰ªæç»Ÿè®¡ä¿¡æ¯
            {'attention': {layer_idx: (old, new)}, 'mlp': {layer_idx: (old, new)}, 'empty_layers': []}
        collapse_threshold: åç¼©é˜ˆå€¼ï¼ˆé»˜è®¤ 0.15 = 15%ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        additional_empty_layers: éœ€è¦é¢å¤–ç§»é™¤çš„å±‚åˆ—è¡¨
    """
    def log(msg):
        if logger:
            logger.log(msg)
        else:
            print(msg)

    log(f"\n{'='*60}")
    log(f"è‡ªåŠ¨åç¼©æ£€æµ‹ (Auto-Collapse)")
    log(f"{'='*60}")
    log(f"åç¼©é˜ˆå€¼: {collapse_threshold:.1%}")
    log(f"æ£€æµ‹é€»è¾‘: å½“å±‚å‚æ•°ä¿ç•™ç‡ < {collapse_threshold:.1%} æ—¶ï¼Œå¼ºåˆ¶ç§»é™¤æ•´å±‚")

    num_layers = len(model.model.layers)
    additional_empty_layers = []

    for layer_idx in range(num_layers):
        # è·³è¿‡å·²ç»è¢«å‰ªç©ºçš„å±‚
        if layer_idx in pruning_stats.get('empty_layers', []):
            continue

        # è®¡ç®—è¯¥å±‚çš„å‚æ•°ä¿ç•™ç‡
        attn_info = pruning_stats['attention'].get(layer_idx)
        mlp_info = pruning_stats['mlp'].get(layer_idx)

        # è®¡ç®— Attention ä¿ç•™ç‡
        if attn_info:
            old_kv, new_kv = attn_info
            attn_retain_rate = new_kv / old_kv if old_kv > 0 else 1.0
        else:
            attn_retain_rate = 1.0

        # è®¡ç®— MLP ä¿ç•™ç‡
        if mlp_info:
            old_channels, new_channels = mlp_info
            mlp_retain_rate = new_channels / old_channels if old_channels > 0 else 1.0
        else:
            mlp_retain_rate = 1.0

        # è®¡ç®—ç»¼åˆä¿ç•™ç‡ï¼ˆå–ä¸¤è€…çš„å¹³å‡ï¼‰
        avg_retain_rate = (attn_retain_rate + mlp_retain_rate) / 2.0

        # åˆ¤æ–­æ˜¯å¦è§¦å‘åç¼©
        if avg_retain_rate < collapse_threshold:
            log(f"  ğŸ”» Layer {layer_idx} è§¦å‘åç¼©:")
            log(f"     Attn ä¿ç•™ç‡: {attn_retain_rate:.1%}, MLP ä¿ç•™ç‡: {mlp_retain_rate:.1%}")
            log(f"     å¹³å‡ä¿ç•™ç‡: {avg_retain_rate:.1%} < {collapse_threshold:.1%}")
            log(f"     å†³ç­–: å¼ºåˆ¶ç§»é™¤æ•´å±‚")
            additional_empty_layers.append(layer_idx)

    if len(additional_empty_layers) == 0:
        log(f"\nâœ“ æ²¡æœ‰å±‚è§¦å‘åç¼©é˜ˆå€¼")
    else:
        log(f"\nâœ“ æ£€æµ‹åˆ° {len(additional_empty_layers)} å±‚éœ€è¦åç¼©: {additional_empty_layers}")
        log(f"  è¿™äº›å±‚å°†è¢«å¼ºåˆ¶ç§»é™¤ï¼ˆåˆ©ç”¨æ®‹å·®æ‚–è®ºï¼‰")

    return additional_empty_layers


def main():
    parser = argparse.ArgumentParser(description='H-GSP: Hybrid Global Structural Pruning for LLaMA')

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_name', type=str, required=True,
                       help='è¾“å‡ºç›®å½•åç§°ï¼Œæ‰€æœ‰ç»“æœä¿å­˜åœ¨ results/{output_name}/ ä¸‹')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.2,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆé»˜è®¤: 0.2ï¼‰')
    parser.add_argument('--importance_method', type=str, default='taylor',
                       choices=['taylor', 'wanda', 'taylor_2nd', 'magnitude'],
                       help='é‡è¦æ€§è®¡ç®—æ–¹æ³•ï¼ˆé»˜è®¤: taylorï¼‰')
    parser.add_argument('--dataset', type=str, default='c4',
                       choices=['wikitext2', 'ptb', 'c4', 'wikitext_zh', 'c4_zh'],
                       help='æ ¡å‡†æ•°æ®é›†é€‰æ‹©ï¼ˆé»˜è®¤: c4\n'
                            '  è‹±æ–‡: wikitext2, ptb, c4\n'
                            '  ä¸­æ–‡: wikitext_zh, c4_zh (æ¨èç”¨äº Qwen/ChatGLM ç­‰ä¸­æ–‡æ¨¡å‹)')
    parser.add_argument('--gradient_batch_size', type=int, default=8,
                       help='æ¢¯åº¦è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 8ï¼‰')
    parser.add_argument('--use_gradient_checkpointing', action='store_true',
                       help='ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜')

    # H-GSP æ ¸å¿ƒå‚æ•°
    parser.add_argument('--temperature', type=float, default=1.0,
                       help='H-GSP æ¸©åº¦å‚æ•° T,å½“temperatureä¸º0æ—¶è¡¨ç¤ºåªç”¨å…¨å±€Taylorï¼ˆé»˜è®¤: 1.0ï¼Œæ¨èèŒƒå›´: 0.5-2.0ï¼‰')
    parser.add_argument('--tau', type=float, default=-100,
                       help='H-GSP é—¨æ§é˜ˆå€¼ Ï„ï¼ˆé»˜è®¤: None è‡ªåŠ¨è®¡ç®—25åˆ†ä½æ•°ï¼‰\n'
                            '  - tau=-100: çº¯ Block-wise æ¨¡å¼ï¼ˆåªç”¨å—çº§é‡è¦æ€§ï¼‰\n'
                            '  - tau=inf: çº¯ Layer-wise æ¨¡å¼ï¼ˆåªç”¨å±‚çº§é‡è¦æ€§ï¼‰\n'
                            '  - tau=None: è‡ªåŠ¨æ¨¡å¼ï¼ˆæ¨èï¼Œæ ¹æ®æ•°æ®è‡ªé€‚åº”ï¼‰')
    parser.add_argument('--epsilon', type=float, default=0,
                       help='H-GSP åç¼©é˜ˆå€¼ Îµï¼ˆé»˜è®¤: 0ï¼‰')

    # å±‚å†»ç»“å‚æ•°
    parser.add_argument('--freeze_first_n_layers', type=int, default=0,
                       help='å†»ç»“å‰Nå±‚ä¸å‰ªæï¼ˆé»˜è®¤: 0ï¼‰')
    parser.add_argument('--freeze_last_n_layers', type=int, default=0,
                       help='å†»ç»“åNå±‚ä¸å‰ªæï¼ˆé»˜è®¤: 0ï¼‰')

    # H-GSP å†…éƒ¨å‚æ•°ï¼ˆç”¨äºè°ƒè¯•å’Œä¼˜åŒ–ï¼‰
    parser.add_argument('--taylor_num_samples', type=int, default=128,
                       help='Taylor é‡è¦æ€§è®¡ç®—çš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--taylor_seq_len', type=int, default=128,
                       help='Taylor é‡è¦æ€§è®¡ç®—çš„åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--layer_importance_num_samples', type=int, default=128,
                       help='å±‚é‡è¦æ€§åˆ†æçš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--layer_importance_seq_len', type=int, default=128,
                       help='å±‚é‡è¦æ€§åˆ†æçš„åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--block_importance_num_samples', type=int, default=128,
                       help='å—é‡è¦æ€§åˆ†æçš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--block_importance_seq_len', type=int, default=128,
                       help='å—é‡è¦æ€§åˆ†æçš„åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 128ï¼‰')

    # GQA é…ç½®
    parser.add_argument('--head_dim', type=int, default=128,
                       help='Attention head ç»´åº¦ï¼ˆé»˜è®¤: 128ï¼‰')
    parser.add_argument('--gqa_ratio', type=int, default=4,
                       help='Q:KV æ¯”ä¾‹ï¼ˆé»˜è®¤: 4ï¼‰')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--run_evaluation', type=str, default="ppl, zeroshot",
                       help='è¯„ä¼°ç±»å‹: ppl, zeroshot, efficiency, allï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--eval_ppl_datasets', type=str, default='wikitext2,ptb',
                       help='PPLè¯„ä¼°æ•°æ®é›†ï¼ˆé»˜è®¤: wikitext2,ptbï¼‰')
    parser.add_argument('--eval_ppl_seq_len', type=int, default=128,
                       help='PPLè¯„ä¼°çª—å£å¤§å°ï¼ˆé»˜è®¤: 128ï¼Œæ ‡å‡†é…ç½®: 2048ï¼‰')
    parser.add_argument('--eval_ppl_stride', type=int, default=None,
                       help='PPLè¯„ä¼°æ­¥é•¿ï¼ˆé»˜è®¤: Noneå³ç­‰äºseq_lenï¼Œæ ‡å‡†é…ç½®: 512ï¼‰')
    parser.add_argument('--eval_zeroshot_tasks', type=str, default='boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa',
                       help='Zero-shotè¯„ä¼°ä»»åŠ¡')
    parser.add_argument('--eval_use_custom_zeroshot', action='store_true',
                       help='ä½¿ç”¨è‡ªå®šä¹‰zero-shotè¯„ä¼°å™¨ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨åœ¨çº¿è¯„ä¼°ï¼‰')
    # å¾®è°ƒå‚æ•°ï¼ˆLoRAï¼‰
    parser.add_argument('--finetune', action='store_true',
                       help='å‰ªæåè¿›è¡Œ LoRA å¾®è°ƒæ¢å¤')
    parser.add_argument('--finetune_data_path', type=str, default='yahma/alpaca-cleaned',
                       help='å¾®è°ƒæ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤: yahma/alpaca-cleanedï¼‰')
    parser.add_argument('--finetune_epochs', type=int, default=2,
                       help='å¾®è°ƒè½®æ•°ï¼ˆé»˜è®¤: 2ï¼‰')
    parser.add_argument('--finetune_lr', type=float, default=1e-4,
                       help='å¾®è°ƒå­¦ä¹ ç‡ï¼ˆé»˜è®¤: 1e-4ï¼‰')
    parser.add_argument('--finetune_batch_size', type=int, default=64,
                       help='å¾®è°ƒ batch sizeï¼ˆé»˜è®¤: 64ï¼‰')
    parser.add_argument('--finetune_micro_batch_size', type=int, default=4,
                       help='å¾®è°ƒ micro batch sizeï¼ˆé»˜è®¤: 4ï¼‰')
    parser.add_argument('--lora_r', type=int, default=8,
                       help='LoRA rankï¼ˆé»˜è®¤: 8ï¼‰')
    parser.add_argument('--lora_alpha', type=int, default=16,
                       help='LoRA alphaï¼ˆé»˜è®¤: 16ï¼‰')
    parser.add_argument('--skip_finetune_evaluation', action='store_true',
                       help='è·³è¿‡å¾®è°ƒåçš„è‡ªåŠ¨è¯„ä¼°')

    # å…¶ä»–
    from core.utils.get_best_gpu import get_best_gpu
    bestDevice = "cuda:"+str(get_best_gpu())
    parser.add_argument('--device', type=str, default=bestDevice,
                       help='è®¾å¤‡')
    parser.add_argument('--layer_start', type=int, default=0,
                       help='èµ·å§‹å±‚ï¼ˆdebugç”¨ï¼‰')
    parser.add_argument('--layer_end', type=int, default=None,
                       help='ç»“æŸå±‚ï¼ˆdebugç”¨ï¼‰')

    args = parser.parse_args()

    # è®¾ç½®è¾“å‡ºç›®å½•ä¸º results/{output_name}
    output_base_dir = os.path.join('results', args.output_name)

    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ï¼ˆå…ˆåˆ›å»ºï¼Œå†åˆå§‹åŒ– loggerï¼‰
    output_dirs = setup_output_directories(output_base_dir)

    # è®¾ç½® loggerï¼Œæ—¥å¿—ä¿å­˜åœ¨ logs å­ç›®å½•ä¸‹
    logger = LoggerWithDepth(
        env_name='logs',  # åœ¨ logs å­ç›®å½•ä¸‹åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        config=args.__dict__,
        root_dir=output_base_dir  # åŸºç¡€ç›®å½•æ˜¯ results/{output_name}
    )
    logger.log(f"\nâœ“ è¾“å‡ºç›®å½•ç»“æ„å·²åˆ›å»º:")
    logger.log(f"  åŸºç¡€ç›®å½•: {output_dirs['base']}")
    logger.log(f"  æ¨¡å‹ä¿å­˜: {output_dirs['models']}")
    logger.log(f"  åˆ†æç»“æœ: {output_dirs['analysis']}")
    logger.log(f"  è¯„ä¼°ç»“æœ: {output_dirs['evaluation']}")
    logger.log(f"  å‰ªæå¯è§†åŒ–ç»“æœ: {output_dirs['visualization']}")
    logger.log(f"  æ—¥å¿—æ–‡ä»¶: {output_dirs['logs']}")

    logger.log("\n" + "="*60)
    logger.log("åŸºäºå…¨å±€æ€§ä»·æ¯”çš„æ··åˆç»“æ„åŒ–å‰ªæ (H-GSP)")
    logger.log("="*60)
    logger.log(f"æ¨¡å‹: {args.base_model}")
    logger.log(f"å‰ªæç‡: {args.pruning_ratio:.1%}")
    logger.log(f"é‡è¦æ€§æ–¹æ³•: {args.importance_method}")
    logger.log(f"æ•°æ®é›†: {args.dataset}")
    logger.log(f"\nH-GSP å‚æ•°:")
    logger.log(f"  æ¸©åº¦ T: {args.temperature}")
    logger.log(f"  é˜ˆå€¼ Ï„: {'è‡ªåŠ¨è®¡ç®—' if args.tau is None else args.tau}")
    logger.log(f"  åç¼©é˜ˆå€¼ Îµ: {args.epsilon}")

    # ========== Step 1: åŠ è½½æ¨¡å‹ ==========
    logger.log("\n[Step 1] åŠ è½½æ¨¡å‹...")

    # æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
    if 'cpu' in args.device.lower():
        device_map = args.device
    else:
        # å•GPUï¼šç›´æ¥æŒ‡å®šè®¾å¤‡ï¼Œé¿å…å¤šGPUåˆ†å¸ƒ
        device_map = args.device

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.float16,
        device_map=device_map,
        low_cpu_mem_usage=True
    )

    # åŠ è½½ tokenizerï¼Œå¤„ç† sentencepiece å…¼å®¹æ€§é—®é¢˜
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)
    except (ValueError, OSError) as e:
        if "sentencepiece" in str(e).lower():
            logger.log("  âš ï¸  Fast tokenizer éœ€è¦ sentencepieceï¼Œå°è¯•ä½¿ç”¨ slow tokenizer...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=False)
            except Exception as e2:
                logger.log(f"  âŒ Slow tokenizer ä¹Ÿå¤±è´¥ï¼Œè¯·å®‰è£…: pip install sentencepiece")
                raise e2
        else:
            raise e

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    if args.use_gradient_checkpointing:
        logger.log("  å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰...")
        model.gradient_checkpointing_enable()

    # è·å–å®é™…ä½¿ç”¨çš„è®¾å¤‡
    if hasattr(model, 'hf_device_map'):
        logger.log(f"  æ¨¡å‹åˆ†å¸ƒ: {model.hf_device_map}")
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å—çš„è®¾å¤‡ï¼ˆè¾“å…¥æ•°æ®åº”è¯¥å‘é€åˆ°è¿™é‡Œï¼‰
        first_device = next(iter(model.hf_device_map.values()))
        args.device = f'cuda:{first_device}' if isinstance(first_device, int) else first_device
        logger.log(f"  è¾“å…¥è®¾å¤‡: {args.device}")
    else:
        args.device = next(model.parameters()).device

    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹çš„ GQA é…ç½®
    num_q_heads, num_kv_heads, detected_gqa_ratio, detected_head_dim = get_model_gqa_config(model)

    logger.log(f"\næ£€æµ‹åˆ°çš„æ¨¡å‹é…ç½®:")
    logger.log(f"  æ¨¡å‹ç±»å‹: {model.config.model_type}")
    logger.log(f"  Q Heads: {num_q_heads}")
    logger.log(f"  KV Heads: {num_kv_heads}")
    logger.log(f"  GQA Ratio: {detected_gqa_ratio}:1")
    logger.log(f"  Head Dim: {detected_head_dim}")

    # è‡ªåŠ¨æ›´æ–°é…ç½®ï¼ˆå¦‚æœä¸å‘½ä»¤è¡Œå‚æ•°ä¸åŒï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„å€¼ï¼‰
    if args.gqa_ratio != detected_gqa_ratio:
        logger.log(f"\nâš ï¸  å‘½ä»¤è¡ŒæŒ‡å®š gqa_ratio={args.gqa_ratio}ï¼Œä½†æ£€æµ‹åˆ° {detected_gqa_ratio}")
        logger.log(f"  å°†ä½¿ç”¨æ£€æµ‹åˆ°çš„å€¼: {detected_gqa_ratio}")
        args.gqa_ratio = detected_gqa_ratio

    if args.head_dim != detected_head_dim:
        logger.log(f"\nâš ï¸  å‘½ä»¤è¡ŒæŒ‡å®š head_dim={args.head_dim}ï¼Œä½†æ£€æµ‹åˆ° {detected_head_dim}")
        logger.log(f"  å°†ä½¿ç”¨æ£€æµ‹åˆ°çš„å€¼: {detected_head_dim}")
        args.head_dim = detected_head_dim

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    logger.log(f"\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    logger.log(f"  æ€»å‚æ•°é‡: {total_params:,}")

    # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available() and 'cuda' in str(args.device).lower():
        device_str = str(args.device)
        gpu_id = int(device_str.split(':')[-1]) if ':' in device_str else 0
        allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
        reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3
        total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
        logger.log(f"  GPU æ˜¾å­˜: {allocated:.2f}GB / {total_mem:.2f}GB (å·²åˆ†é…)")
        logger.log(f"  GPU æ˜¾å­˜: {reserved:.2f}GB / {total_mem:.2f}GB (å·²é¢„ç•™)")

    # åˆ†æåŸå§‹æ¨¡å‹ï¼ˆå‰ªæå‰ï¼‰
    logger.log(f"\nåˆ†æåŸå§‹æ¨¡å‹ç»“æ„...")
    original_analyzer = ModelAnalyzer(model, "åŸå§‹æ¨¡å‹")
    original_analysis = original_analyzer.analyze()
    logger.log(f"  âœ“ åŸå§‹æ¨¡å‹åˆ†æå®Œæˆ")

    # åˆ›å»ºæ•°æ®é›†ç®¡ç†å™¨ï¼ˆç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®é›†åŠ è½½ï¼‰
    logger.log(f"\nâœ“ åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨: {args.dataset}")
    dataset_manager = DatasetManager(dataset_name=args.dataset, tokenizer=tokenizer)

    # ========== Step 3: è®¡ç®—é‡è¦æ€§ï¼ˆæ¢¯åº¦æˆ–æ¿€æ´»ï¼‰ ==========
    activations = None
    hessian_diag = None

    # H-GSP å†…éƒ¨å‚æ•°ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è·å–ï¼‰
    TAYLOR_NUM_SAMPLES = args.taylor_num_samples
    TAYLOR_SEQ_LEN = args.taylor_seq_len
    LAYER_IMPORTANCE_NUM_SAMPLES = args.layer_importance_num_samples
    LAYER_IMPORTANCE_SEQ_LEN = args.layer_importance_seq_len
    BLOCK_IMPORTANCE_NUM_SAMPLES = args.block_importance_num_samples
    BLOCK_IMPORTANCE_SEQ_LEN = args.block_importance_seq_len

    if args.importance_method in ['taylor', 'taylor_2nd']:
        logger.log(f"\n[Step 3] è®¡ç®—æ¢¯åº¦ï¼ˆ{'ä¸€é˜¶' if args.importance_method == 'taylor' else 'äºŒé˜¶'} Taylor importanceï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {TAYLOR_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {TAYLOR_SEQ_LEN}")

        # åˆå§‹åŒ–æ¢¯åº¦åˆ†æå™¨
        gradient_analyzer = GradientAnalyzer(model, logger)
        logger.log(f"  âœ“ æ¢¯åº¦åˆ†æå™¨å·²åˆå§‹åŒ–ï¼ˆå°†æ”¶é›†æ¢¯åº¦ç»Ÿè®¡ç”¨äºè¯Šæ–­ï¼‰")

        # åˆ†æ‰¹è®¡ç®—æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜
        batch_size = args.gradient_batch_size
        num_batches = (TAYLOR_NUM_SAMPLES + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        model.zero_grad()
        total_loss = 0.0
        start_time = time.time()

        # å¦‚æœåœ¨ CPU ä¸Šè¿è¡Œï¼Œç»™å‡ºæç¤º
        if 'cpu' in str(args.device).lower():
            logger.log(f"  âš ï¸ åœ¨ CPU ä¸Šè¿è¡Œï¼Œé€Ÿåº¦ä¼šéå¸¸æ…¢ï¼")
            logger.log(f"  é¢„è®¡æ¯ä¸ªæ‰¹æ¬¡éœ€è¦ 5-10 åˆ†é’Ÿï¼ˆå–å†³äº CPU æ€§èƒ½ï¼‰")
            logger.log(f"  æ€»é¢„è®¡æ—¶é—´: {num_batches * 7:.0f} åˆ†é’Ÿå·¦å³")
            logger.log("")

        # äºŒé˜¶æ³°å‹’éœ€è¦ç´¯ç§¯ Hessian å¯¹è§’çº¿è¿‘ä¼¼
        # âš ï¸ å­˜å‚¨åœ¨CPUä¸Šä»¥é¿å…GPU OOM
        if args.importance_method == 'taylor_2nd':
            hessian_diag = {}
            logger.log("  åˆå§‹åŒ– Hessian å¯¹è§’çº¿å­˜å‚¨ï¼ˆåœ¨CPUä¸Šä»¥èŠ‚çœGPUæ˜¾å­˜ï¼‰...")
            for name, param in model.named_parameters():
                if param.requires_grad:
                    # å­˜å‚¨åœ¨CPUä¸Šï¼Œé¿å…å ç”¨GPUæ˜¾å­˜
                    hessian_diag[name] = torch.zeros_like(param.data, device='cpu')

        # âœ… ä¿®å¤ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ ·æœ¬ï¼Œé¿å…æ¯æ‰¹æ¬¡é‡å¤è·å–ç›¸åŒæ ·æœ¬
        logger.log(f"  åŠ è½½ {TAYLOR_NUM_SAMPLES} ä¸ªæ ·æœ¬ç”¨äºæ¢¯åº¦è®¡ç®—...")
        all_gradient_samples = dataset_manager.get_gradient_samples(
            num_samples=TAYLOR_NUM_SAMPLES,
            seq_len=TAYLOR_SEQ_LEN
        )
        logger.log(f"  âœ“ æ ·æœ¬åŠ è½½å®Œæˆï¼Œshape: {all_gradient_samples.shape}")

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(range(num_batches), desc="è®¡ç®—æ¢¯åº¦", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, TAYLOR_NUM_SAMPLES)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # âš ï¸ å…³é”®ï¼šå¯¹äºäºŒé˜¶æ³°å‹’ï¼Œæ¯ä¸ªæ‰¹æ¬¡æ¸…é›¶æ¢¯åº¦ä»¥è·å¾—ç‹¬ç«‹çš„æ¢¯åº¦
            # å¯¹äºä¸€é˜¶æ³°å‹’ï¼Œä¸æ¸…é›¶æ¢¯åº¦ï¼Œè®©æ¢¯åº¦ç´¯ç§¯
            if args.importance_method == 'taylor_2nd':
                model.zero_grad()

            # âœ… ä¿®å¤ï¼šä»é¢„åŠ è½½çš„æ ·æœ¬ä¸­åˆ‡ç‰‡è·å–å½“å‰æ‰¹æ¬¡
            input_ids = all_gradient_samples[start_idx:end_idx].to(args.device)

            # å‰å‘ä¼ æ’­
            outputs = model(input_ids, labels=input_ids)
            loss = outputs.loss  # âœ… ä¿®å¤ï¼šä¸é™¤ä»¥ num_batches

            # åå‘ä¼ æ’­
            loss.backward()

            # ğŸ” è¯Šæ–­ï¼šæ‰“å°ç¬¬ä¸€ä¸ªbatchçš„æ¢¯åº¦åˆ†å¸ƒï¼ˆå¸®åŠ©è¯Šæ–­åºåˆ—é•¿åº¦é—®é¢˜ï¼‰
            if batch_idx == 0:
                sample_layers = [0, 2, 10, 20, 31]
                logger.log(f"  æ¢¯åº¦åˆ†å¸ƒè¯Šæ–­ï¼ˆåºåˆ—é•¿åº¦ {TAYLOR_SEQ_LEN}ï¼‰ï¼š")
                for layer_idx in sample_layers:
                    layer_name = f'model.layers.{layer_idx}.mlp.gate_proj.weight'
                    for name, param in model.named_parameters():
                        if name == layer_name and param.grad is not None:
                            grad_mean = param.grad.abs().mean().item()
                            grad_std = param.grad.abs().std().item()
                            logger.log(f"    Layer {layer_idx:2d}: grad_mean={grad_mean:.6e}, grad_std={grad_std:.6e}")
                            break

            # ğŸ“Š æ”¶é›†æ¢¯åº¦ç»Ÿè®¡ï¼ˆç”¨äºåç»­è¯Šæ–­å’Œå¯è§†åŒ–ï¼‰
            gradient_analyzer.accumulate_gradient_stats(layer_prefix='model.layers')

            # äºŒé˜¶æ³°å‹’ï¼šç´¯ç§¯ Hessian å¯¹è§’çº¿ï¼ˆä½¿ç”¨æ¢¯åº¦å¹³æ–¹è¿‘ä¼¼ï¼‰
            if args.importance_method == 'taylor_2nd':
                for name, param in model.named_parameters():
                    if param.requires_grad and param.grad is not None:
                        # âœ… ä¿®å¤ï¼šç´¯åŠ æ¯ä¸ªæ‰¹æ¬¡ç‹¬ç«‹çš„æ¢¯åº¦å¹³æ–¹
                        # æ³¨æ„ï¼šè¿™é‡Œç´¯åŠ çš„æ˜¯æ¢¯åº¦å¹³æ–¹ï¼Œä¸æ˜¯å¹³æ–¹å’Œé™¤ä»¥æ‰¹æ¬¡æ•°
                        hessian_diag[name] += (param.grad ** 2).cpu()

            # ç´¯åŠ  lossï¼ˆç”¨äºæŠ¥å‘Šå¹³å‡å€¼ï¼‰
            batch_time = time.time() - batch_start_time
            total_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'time': f'{batch_time:.2f}s'
            })

            # æ¸…ç†å†…å­˜
            del input_ids, outputs, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        # âœ… ä¿®å¤ï¼šåœ¨æ‰€æœ‰æ‰¹æ¬¡å®Œæˆåï¼Œå°† Hessian å¯¹è§’çº¿é™¤ä»¥æ‰¹æ¬¡æ•°å¾—åˆ°å¹³å‡å€¼
        if args.importance_method == 'taylor_2nd':
            for name in hessian_diag:
                hessian_diag[name] /= num_batches

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¢¯åº¦è®¡ç®—å®Œæˆ")
        logger.log(f"  å¹³å‡ loss: {total_loss/num_batches:.4f}")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

        if args.importance_method == 'taylor_2nd':
            logger.log(f"  âœ“ Hessian å¯¹è§’çº¿è¿‘ä¼¼è®¡ç®—å®Œæˆ")
            logger.log(f"  Hessian å­—å…¸åŒ…å« {len(hessian_diag)} ä¸ªå‚æ•°")

            # æ‰“å°ä¸€äº›ç¤ºä¾‹é”®åï¼Œç”¨äºè°ƒè¯•
            sample_keys = list(hessian_diag.keys())[:10]
            logger.log(f"  ç¤ºä¾‹ Hessian é”®åï¼ˆå‰10ä¸ªï¼‰ï¼š")
            for key in sample_keys:
                logger.log(f"    - {key}")

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„é”®å
            layer_0_keys = [k for k in hessian_diag.keys() if 'layers.0.' in k]
            if layer_0_keys:
                logger.log(f"  Layer 0 çš„å‚æ•°ç¤ºä¾‹ï¼š")
                for key in layer_0_keys[:5]:
                    logger.log(f"    - {key}")

    elif args.importance_method == 'magnitude':
        logger.log(f"\n[Step 3] ä½¿ç”¨ Magnitude importance (æƒé‡ç»å¯¹å€¼)...")
        logger.log(f"  âœ“ Magnitude æ–¹æ³•ä¸éœ€è¦è®¡ç®—æ¢¯åº¦æˆ–æ¿€æ´»å€¼")
        logger.log(f"  ç›´æ¥ä½¿ç”¨æ¨¡å‹æƒé‡è¿›è¡Œå‰ªæ")

    elif args.importance_method == 'wanda':
        logger.log(f"\n[Step 3] æ”¶é›†æ¿€æ´»å€¼ï¼ˆWanda importanceï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {TAYLOR_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {TAYLOR_SEQ_LEN}")

        # åˆ†æ‰¹æ”¶é›†æ¿€æ´»
        batch_size = args.gradient_batch_size
        num_batches = (TAYLOR_NUM_SAMPLES + batch_size - 1) // batch_size
        logger.log(f"  æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡æ•°: {num_batches}")

        # âœ… ä¿®å¤ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ ·æœ¬ï¼Œé¿å…æ¯æ‰¹æ¬¡é‡å¤è·å–ç›¸åŒæ ·æœ¬
        logger.log(f"  åŠ è½½ {TAYLOR_NUM_SAMPLES} ä¸ªæ ·æœ¬ç”¨äºæ¿€æ´»æ”¶é›†...")
        all_gradient_samples = dataset_manager.get_gradient_samples(
            num_samples=TAYLOR_NUM_SAMPLES,
            seq_len=TAYLOR_SEQ_LEN
        )
        logger.log(f"  âœ“ æ ·æœ¬åŠ è½½å®Œæˆï¼Œshape: {all_gradient_samples.shape}")

        all_activations = {}
        start_time = time.time()

        pbar = tqdm(range(num_batches), desc="æ”¶é›†æ¿€æ´»", ncols=100)

        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, TAYLOR_NUM_SAMPLES)
            current_batch_size = end_idx - start_idx

            batch_start_time = time.time()

            # âœ… ä¿®å¤ï¼šä»é¢„åŠ è½½çš„æ ·æœ¬ä¸­åˆ‡ç‰‡è·å–å½“å‰æ‰¹æ¬¡
            input_ids = all_gradient_samples[start_idx:end_idx].to(args.device)

            # æ”¶é›†æ¿€æ´»
            batch_activations = collect_layer_activations(model, input_ids, args.device)

            # ç´¯åŠ æ¿€æ´»å€¼
            for layer_idx, layer_acts in batch_activations.items():
                if layer_idx not in all_activations:
                    all_activations[layer_idx] = {}
                for name, act in layer_acts.items():
                    if name not in all_activations[layer_idx]:
                        all_activations[layer_idx][name] = act.to(args.device)
                    else:
                        all_activations[layer_idx][name] += act.to(args.device)

            batch_time = time.time() - batch_start_time

            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar.set_postfix({'time': f'{batch_time:.2f}s'})

            del input_ids, batch_activations
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        pbar.close()

        # å¹³å‡æ¿€æ´»å€¼
        for layer_idx in all_activations:
            for name in all_activations[layer_idx]:
                all_activations[layer_idx][name] /= num_batches

        activations = all_activations

        total_time = time.time() - start_time
        logger.log(f"âœ“ æ¿€æ´»å€¼æ”¶é›†å®Œæˆ")
        logger.log(f"  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.2f}min)")
        logger.log(f"  å¹³å‡æ¯æ‰¹æ¬¡: {total_time/num_batches:.2f}s")

    # ========== Step 3.5: è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP å¿…éœ€ï¼‰==========
    # ========== Step 3.5: è®¡ç®—å±‚ç§»é™¤å›°æƒ‘åº¦ï¼ˆH-GSP Layer-wise é‡è¦æ€§ï¼‰==========
    # å½“æ¸©åº¦ T=0 æ—¶ï¼Œåªä½¿ç”¨å…¨å±€ Taylor é‡è¦æ€§ï¼Œè·³è¿‡å±‚çº§å’Œå—çº§é‡è¦æ€§æµ‹è¯•
    if args.temperature == 0.0:
        logger.log(f"\n[Step 3.5-3.6] è·³è¿‡å±‚çº§å’Œå—çº§é‡è¦æ€§æµ‹è¯•")
        logger.log(f"  åŸå› : temperature=0ï¼Œåªä½¿ç”¨å…¨å±€ Taylor é‡è¦æ€§ï¼ˆæ¨èé…ç½®ï¼‰")
        logger.log(f"  âœ“ é¿å…æ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼Œèšç„¦æ ¸å¿ƒæ–¹æ³•")

        # è®¾ç½®ä¸ºç©ºï¼Œåç»­æ„å»ºå…¨å±€åˆ†æè¡¨æ—¶ä¼šè‡ªåŠ¨å¤„ç†
        layer_removal_ppl = {}
        block_removal_ppl = {'attention': {}, 'mlp': {}}

    else:
        logger.log(f"\n[Step 3.5] è®¡ç®—å±‚é‡è¦æ€§ï¼ˆH-GSP Layer-wise é‡è¦æ€§ï¼‰...")
        logger.log(f"  æ ·æœ¬æ•°: {LAYER_IMPORTANCE_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {LAYER_IMPORTANCE_SEQ_LEN}")

        from core.importance.layer_analyzer import LayerImportanceAnalyzer

        # åŠ è½½ç”¨äºå±‚é‡è¦æ€§åˆ†æçš„æ ·æœ¬ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        layer_texts_list = dataset_manager.get_layer_importance_samples(
            num_samples=LAYER_IMPORTANCE_NUM_SAMPLES,
            seq_len=LAYER_IMPORTANCE_SEQ_LEN
        )

        # åˆ›å»ºåˆ†æå™¨
        analyzer = LayerImportanceAnalyzer(model, tokenizer, device=args.device)

        # è®¡ç®—æ¯å±‚çš„é‡è¦æ€§ï¼ˆä½¿ç”¨losså¢åŠ å€¼æ–¹æ³•ï¼‰
        num_layers = len(model.model.layers)
        layer_removal_ppl = analyzer.measure_layer_importance_by_removal(
            texts=layer_texts_list,
            num_layers=num_layers
        )

        logger.log(f"âœ“ å±‚é‡è¦æ€§è®¡ç®—å®Œæˆï¼ˆlosså¢åŠ å€¼æ–¹æ³•ï¼‰")
        print("\n" + "="*60)
        print("å±‚çº§é‡è¦åº¦ï¼ˆç§»é™¤å±‚åçš„losså¢åŠ å€¼ï¼‰")
        print("="*60)
        for layer_idx in range(num_layers):
            importance = layer_removal_ppl.get(layer_idx, 0.0)
            print(f"Layer {layer_idx:2d}   {importance:10.4f}")

        # ä¿å­˜å±‚é‡è¦æ€§åˆ°åˆ†æç›®å½•
        import json
        layer_importance_path = os.path.join(output_dirs['analysis'], 'layer_importance_loss.json')
        with open(layer_importance_path, 'w') as f:
            json.dump(layer_removal_ppl, f, indent=2)
        logger.log(f"âœ“ å±‚é‡è¦æ€§å·²ä¿å­˜: {layer_importance_path}")

        # ========== Step 3.6: è®¡ç®—å—é‡è¦æ€§ï¼ˆH-GSP Block-wise é‡è¦æ€§ï¼‰==========
        logger.log(f"\n[Step 3.6] è®¡ç®—å—é‡è¦æ€§ï¼ˆH-GSP Block-wise é‡è¦æ€§ï¼‰...")
        logger.log(f"  æ–¹æ³•: åŸºäºlosså¢åŠ å€¼ï¼ˆç§»é™¤å—åçš„losså˜åŒ–ï¼‰")
        logger.log(f"  æ ·æœ¬æ•°: {BLOCK_IMPORTANCE_NUM_SAMPLES}, åºåˆ—é•¿åº¦: {BLOCK_IMPORTANCE_SEQ_LEN}")

        # åŠ è½½ç”¨äºå—é‡è¦æ€§åˆ†æçš„æ ·æœ¬ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        block_texts_list = dataset_manager.get_layer_importance_samples(
            num_samples=BLOCK_IMPORTANCE_NUM_SAMPLES,
            seq_len=BLOCK_IMPORTANCE_SEQ_LEN
        )

        # è®¡ç®—æ¯å±‚çš„ Attention å’Œ MLP å—é‡è¦æ€§ï¼ˆä½¿ç”¨losså¢åŠ å€¼æ–¹æ³•ï¼‰
        block_removal_ppl = analyzer.measure_block_importance_by_removal(
            texts=block_texts_list,
            num_layers=num_layers
        )

        logger.log(f"âœ“ å—é‡è¦æ€§è®¡ç®—å®Œæˆï¼ˆlosså¢åŠ å€¼æ–¹æ³•ï¼‰")
        logger.log(f"  ç¤ºä¾‹ - Layer 0 Attention: {block_removal_ppl['attention'][0]:.4f}, MLP: {block_removal_ppl['mlp'][0]:.4f}")
        logger.log(f"  ç¤ºä¾‹ - Layer {num_layers-1} Attention: {block_removal_ppl['attention'][num_layers-1]:.4f}, MLP: {block_removal_ppl['mlp'][num_layers-1]:.4f}")

        # ä¿å­˜å—é‡è¦æ€§åˆ°åˆ†æç›®å½•
        block_importance_path = os.path.join(output_dirs['analysis'], 'block_importance_loss.json')
        with open(block_importance_path, 'w') as f:
            json.dump(block_removal_ppl, f, indent=2)
        logger.log(f"âœ“ å—é‡è¦æ€§å·²ä¿å­˜: {block_importance_path}")

    # ========== Step 3.7: æ¢¯åº¦è¯Šæ–­å’Œå¯è§†åŒ–ï¼ˆä»…åœ¨ä½¿ç”¨ Taylor æ–¹æ³•æ—¶ï¼‰==========
    if args.importance_method in ['taylor', 'taylor_2nd']:
        logger.log(f"\n[Step 3.7] æ¢¯åº¦è¯Šæ–­å’Œå¯è§†åŒ–...")

        num_layers = len(model.model.layers)

        # ä¿å­˜æ¢¯åº¦ç»Ÿè®¡åˆ°æ–‡ä»¶
        gradient_stats_path = os.path.join(output_dirs['analysis'], 'gradient_statistics.json')
        gradient_analyzer.save_gradient_stats(gradient_stats_path)

        # æ³¨æ„ï¼šæ­¤æ—¶è¿˜æ²¡æœ‰è®¡ç®—é‡è¦æ€§å¾—åˆ†å’Œå‰ªæç‡ï¼Œæ‰€ä»¥å…ˆä¸ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–
        # å®Œæ•´çš„å¯è§†åŒ–å°†åœ¨å‰ªæå®Œæˆåç”Ÿæˆ
        logger.log(f"  âœ“ æ¢¯åº¦ç»Ÿè®¡å·²æ”¶é›†å¹¶ä¿å­˜")
        logger.log(f"  â„¹ï¸  å®Œæ•´çš„æ¢¯åº¦å¯è§†åŒ–å°†åœ¨å‰ªæå®Œæˆåç”Ÿæˆ")

    # ========== Step 4: æ„å»ºå…¨å±€åˆ†æè¡¨ ==========
    logger.log("\n[Step 4] æ„å»ºå…¨å±€ Group åˆ†æè¡¨...")

    layer_end = args.layer_end if args.layer_end else len(model.model.layers)

    # ä¼ é€’é‡è¦æ€§ä¿¡æ¯
    importance_info = {}
    if args.importance_method in ['taylor', 'taylor_2nd']:
        importance_info['gradients'] = {name: param.grad for name, param in model.named_parameters() if param.grad is not None}
        if args.importance_method == 'taylor_2nd':
            importance_info['hessian_diag'] = hessian_diag
    elif args.importance_method == 'wanda':
        importance_info['activations'] = activations

    df = build_global_group_table(
        model=model,
        importance_method=args.importance_method,
        importance_info=importance_info,
        layer_start=args.layer_start,
        layer_end=layer_end,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        device=args.device,
        layer_removal_ppl=layer_removal_ppl,    # H-GSP: å±‚çº§é‡è¦æ€§
        block_removal_ppl=block_removal_ppl,    # H-GSP: å—çº§é‡è¦æ€§
        temperature=args.temperature,           # H-GSP: æ¸©åº¦å‚æ•° T
        tau=args.tau,                          # H-GSP: é—¨æ§é˜ˆå€¼ Ï„
        freeze_first_n_layers=args.freeze_first_n_layers,  # å†»ç»“å‰Nå±‚
        freeze_last_n_layers=args.freeze_last_n_layers     # å†»ç»“åNå±‚
    )

    logger.log(f"âœ“ åˆ†æè¡¨æ„å»ºå®Œæˆ")

    # ========== Step 5: é€‰æ‹©è¦å‰ªæçš„ groups ==========
    logger.log(f"\n[Step 5] æ ¹æ®å‰ªæç‡é€‰æ‹©è¦å‰ªæçš„ groups...")

    groups_to_prune = select_groups_to_prune(
        df=df,
        pruning_ratio=args.pruning_ratio,
        total_params=total_params
    )

    logger.log(f"âœ“ é€‰ä¸­ {len(groups_to_prune)} ä¸ª groups è¿›è¡Œå‰ªæ")

    # ä¿å­˜åˆ†æè¡¨åˆ° analysis ç›®å½•ï¼ˆæŒ‰scoreæ’åºï¼‰
    table_path = os.path.join(output_dirs['analysis'], 'global_group_table.csv')
    df.to_csv(table_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {table_path}")

    prune_table_path = os.path.join(output_dirs['analysis'], 'groups_to_prune.csv')
    groups_to_prune.to_csv(prune_table_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰scoreæ’åºï¼‰: {prune_table_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„åˆ†æè¡¨
    df_by_layer = df.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    table_by_layer_path = os.path.join(output_dirs['analysis'], 'global_group_table_by_layer.csv')
    df_by_layer.to_csv(table_by_layer_path, index=False)
    logger.log(f"âœ“ åˆ†æè¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {table_by_layer_path}")

    # ä¿å­˜æŒ‰å±‚æ’åºçš„å‰ªæåˆ—è¡¨
    prune_by_layer = groups_to_prune.sort_values(['layer_idx', 'group_type', 'group_idx']).reset_index(drop=True)
    prune_by_layer_path = os.path.join(output_dirs['analysis'], 'groups_to_prune_by_layer.csv')
    prune_by_layer.to_csv(prune_by_layer_path, index=False)
    logger.log(f"âœ“ å‰ªæåˆ—è¡¨å·²ä¿å­˜ï¼ˆæŒ‰å±‚æ’åºï¼‰: {prune_by_layer_path}")

    # ç”Ÿæˆå±‚çº§ç»Ÿè®¡æ‘˜è¦
    summary_lines = []
    summary_lines.append("="*80)
    summary_lines.append("å„å±‚å‰ªæç»Ÿè®¡æ‘˜è¦")
    summary_lines.append("="*80)
    summary_lines.append(f"{'Layer':<8} {'Attentionå‰ªæ':<20} {'MLPå‰ªæ':<20} {'æ€»å‚æ•°å‰ªæ':<20}")
    summary_lines.append("-"*80)

    for layer_idx in sorted(groups_to_prune['layer_idx'].unique()):
        layer_data = groups_to_prune[groups_to_prune['layer_idx'] == layer_idx]
        attn_data = layer_data[layer_data['group_type'] == 'attention']
        mlp_data = layer_data[layer_data['group_type'] == 'mlp']

        attn_count = len(attn_data)
        mlp_count = len(mlp_data)
        attn_params = attn_data['cost'].sum() if len(attn_data) > 0 else 0
        mlp_params = mlp_data['cost'].sum() if len(mlp_data) > 0 else 0
        total_params_prune = attn_params + mlp_params

        summary_lines.append(
            f"{layer_idx:<8} "
            f"{attn_count} groups ({attn_params:,} params)".ljust(20) + " "
            f"{mlp_count} channels ({mlp_params:,} params)".ljust(20) + " "
            f"{total_params_prune:,} params".ljust(20)
        )

    summary_lines.append("-"*80)
    summary_lines.append(f"æ€»è®¡: {len(groups_to_prune)} groups, "
                        f"{groups_to_prune['cost'].sum():,} params")
    summary_lines.append("="*80)

    # ä¿å­˜æ‘˜è¦æ–‡ä»¶åˆ° analysis ç›®å½•
    summary_path = os.path.join(output_dirs['analysis'], 'pruning_summary_by_layer.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary_lines))
    logger.log(f"âœ“ å±‚çº§ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    # ä¹Ÿåœ¨æ—¥å¿—ä¸­æ˜¾ç¤º
    logger.log("\n" + '\n'.join(summary_lines))

    # ========== Step 6: æ‰§è¡Œå…¨å±€å‰ªæ ==========
    logger.log(f"\n[Step 6] æ‰§è¡Œå…¨å±€å‰ªæ...")

    pruning_stats = apply_global_pruning(
        model=model,
        groups_to_prune_df=groups_to_prune,
        head_dim=args.head_dim,
        gqa_ratio=args.gqa_ratio,
        logger=logger
    )

    logger.log("\nâœ“ å…¨å±€å‰ªæå®Œæˆ")

    # ========== Step 6.5: è‡ªåŠ¨åç¼©ï¼ˆH-GSP å¿…éœ€ï¼‰==========
    logger.log(f"\n[Step 6.5] è‡ªåŠ¨åç¼©æ£€æµ‹ï¼ˆH-GSP Auto-Collapse, Îµ={args.epsilon}ï¼‰...")
    additional_empty_layers = auto_collapse(
        model=model,
        pruning_stats=pruning_stats,
        collapse_threshold=args.epsilon,
        logger=logger
    )
    # å°†é¢å¤–çš„ç©ºå±‚åŠ å…¥åˆ° empty_layers åˆ—è¡¨
    pruning_stats['empty_layers'].extend(additional_empty_layers)

    # ========== Step 7: ç§»é™¤ç©ºå±‚ï¼ˆè‡ªåŠ¨æ‰§è¡Œï¼‰==========
    # æ³¨ï¼šæ—¢ç„¶ Auto-Collapse å·²æ£€æµ‹åˆ°ç¨€ç–å±‚ï¼Œåº”è‡ªåŠ¨æ›¿æ¢ä¸º Identity å±‚
    # è¿™ç¬¦åˆ H-GSP çš„æ ¸å¿ƒç†å¿µï¼š"ç•™ 10% ä¸å¦‚ä¸ç•™"
    all_empty_layers = pruning_stats['empty_layers']
    if len(all_empty_layers) > 0:
        logger.log(f"\n[Step 7] ç§»é™¤ç©ºå±‚...")
        logger.log(f"  åŸå§‹ç©ºå±‚: {len(all_empty_layers) - len(additional_empty_layers)}")
        if len(additional_empty_layers) > 0:
            logger.log(f"  åç¼©è§¦å‘: {len(additional_empty_layers)}")
        logger.log(f"  æ€»è®¡ç§»é™¤: {len(all_empty_layers)} å±‚")
        remove_empty_layers(model, all_empty_layers, logger)
    else:
        logger.log(f"\n[Step 7] âœ“ æ— éœ€ç§»é™¤ç©ºå±‚")

    # ========== Step 8: ç»Ÿè®¡å‰ªæç»“æœ ==========
    logger.log(f"\n{'='*60}")
    logger.log(f"å‰ªæç»Ÿè®¡")
    logger.log(f"{'='*60}")

    after_params = sum(p.numel() for p in model.parameters())
    actual_ratio = (total_params - after_params) / total_params

    logger.log(f"å‚æ•°ç»Ÿè®¡:")
    logger.log(f"  å‰ªæå‰: {total_params:,}")
    logger.log(f"  å‰ªæå: {after_params:,}")
    logger.log(f"  å®é™…å‰ªæç‡: {actual_ratio:.2%}")

    if len(pruning_stats['empty_layers']) > 0:
        logger.log(f"\nè‡ªåŠ¨æ·±åº¦å‰ªæ:")
        logger.log(f"  æ›¿æ¢ä¸ºIdentityçš„å±‚: {pruning_stats['empty_layers']}")
        logger.log(f"  ç‰©ç†å±‚æ•°: {len(model.model.layers)} (ä¿æŒä¸å˜)")
        logger.log(f"  æœ‰æ•ˆå±‚æ•°: {len(model.model.layers) - len(pruning_stats['empty_layers'])}")

    # ========== Step 8.5: ç”Ÿæˆè¯¦ç»†çš„æ¨¡å‹åˆ†ææŠ¥å‘Š ==========
    logger.log(f"\n[Step 8.5] ç”Ÿæˆè¯¦ç»†çš„æ¨¡å‹åˆ†ææŠ¥å‘Š...")

    # åˆ†æå‰ªæåçš„æ¨¡å‹
    pruned_analyzer = ModelAnalyzer(model, "å‰ªæåæ¨¡å‹")
    pruned_analysis = pruned_analyzer.analyze()
    logger.log(f"  âœ“ å‰ªæåæ¨¡å‹åˆ†æå®Œæˆ")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    comparator = ModelComparator(
        original_analysis=original_analysis,
        pruned_analysis=pruned_analysis,
        original_name="åŸå§‹æ¨¡å‹",
        pruned_name="å‰ªæåæ¨¡å‹"
    )
    comparison_result = comparator.compare()
    logger.log(f"  âœ“ å¯¹æ¯”åˆ†æå®Œæˆ")

    # ä¿å­˜åˆ†ææŠ¥å‘Š
    import json
    analysis_dir = output_dirs['analysis']

    # ä¿å­˜åŸå§‹æ¨¡å‹åˆ†æ
    original_analysis_path = os.path.join(analysis_dir, 'original_model_analysis.json')
    with open(original_analysis_path, 'w', encoding='utf-8') as f:
        json.dump(original_analysis, f, indent=2, ensure_ascii=False)
    logger.log(f"  âœ“ åŸå§‹æ¨¡å‹åˆ†æå·²ä¿å­˜: {original_analysis_path}")

    # ä¿å­˜å‰ªæåæ¨¡å‹åˆ†æ
    pruned_analysis_path = os.path.join(analysis_dir, 'pruned_model_analysis.json')
    with open(pruned_analysis_path, 'w', encoding='utf-8') as f:
        json.dump(pruned_analysis, f, indent=2, ensure_ascii=False)
    logger.log(f"  âœ“ å‰ªæåæ¨¡å‹åˆ†æå·²ä¿å­˜: {pruned_analysis_path}")

    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    comparison_path = os.path.join(analysis_dir, 'model_comparison.json')
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_result, f, indent=2, ensure_ascii=False)
    logger.log(f"  âœ“ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {comparison_path}")

    # åŒæ—¶ä¿å­˜ä¸º pruning_comparison.jsonï¼ˆå…¼å®¹å¯è§†åŒ–å·¥å…·ï¼‰
    pruning_comparison_path = os.path.join(analysis_dir, 'pruning_comparison.json')
    with open(pruning_comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_result, f, indent=2, ensure_ascii=False)
    logger.log(f"  âœ“ å‰ªæå¯¹æ¯”æ•°æ®å·²ä¿å­˜: {pruning_comparison_path}")

    # åœ¨æ—¥å¿—ä¸­æ‰“å°è¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
    logger.log(f"\n{'='*60}")
    logger.log(f"è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š")
    logger.log(f"{'='*60}")

    total = comparison_result['total_params']
    logger.log(f"\næ€»å‚æ•°é‡:")
    logger.log(f"  åŸå§‹: {total['original']:,}")
    logger.log(f"  å‰ªæå: {total['pruned']:,}")
    logger.log(f"  å‡å°‘: {total['reduced']:,} ({total['reduction_ratio']*100:.2f}%)")

    layer_params = comparison_result['layer_params']
    logger.log(f"\nDecoder Layers å‚æ•°:")
    logger.log(f"  åŸå§‹: {layer_params['original']:,}")
    logger.log(f"  å‰ªæå: {layer_params['pruned']:,}")
    logger.log(f"  å‡å°‘: {layer_params['reduced']:,} ({layer_params['reduction_ratio']*100:.2f}%)")

    # ç»Ÿè®¡å„å±‚å‰ªææƒ…å†µ
    logger.log(f"\næ¯å±‚å‰ªæè¯¦æƒ…:")
    logger.log(f"{'-'*60}")

    for layer_comp in comparison_result['layers']:
        layer_idx = layer_comp['layer_idx']
        total_comp = layer_comp['total']
        attn_comp = layer_comp['attention']
        mlp_comp = layer_comp['mlp']

        # æ ‡è®°ç‰¹æ®Šå±‚
        special_marker = ""
        if layer_comp['is_zero_layer']:
            special_marker = " [å®Œå…¨å‰ªç©º]"

        logger.log(f"\nLayer {layer_idx:2d}{special_marker}:")
        logger.log(f"  æ€»å‚æ•°: {total_comp['original']:,} â†’ {total_comp['pruned']:,} "
                  f"(-{total_comp['reduction_ratio']*100:.2f}%)")

        logger.log(f"  Attention: {attn_comp['original']:,} â†’ {attn_comp['pruned']:,} "
                  f"(-{attn_comp['reduction_ratio']*100:.2f}%)")
        if 'num_heads' in attn_comp:
            orig_q = attn_comp['num_heads']['original']
            pruned_q = attn_comp['num_heads']['pruned']
            orig_kv = attn_comp['num_kv_heads']['original']
            pruned_kv = attn_comp['num_kv_heads']['pruned']
            logger.log(f"    å¤´æ•°: {orig_q}Q:{orig_kv}KV â†’ {pruned_q}Q:{pruned_kv}KV")

        logger.log(f"  MLP: {mlp_comp['original']:,} â†’ {mlp_comp['pruned']:,} "
                  f"(-{mlp_comp['reduction_ratio']*100:.2f}%)")
        if 'intermediate_size' in mlp_comp:
            orig_size = mlp_comp['intermediate_size']['original']
            pruned_size = mlp_comp['intermediate_size']['pruned']
            logger.log(f"    ä¸­é—´ç»´åº¦: {orig_size} â†’ {pruned_size}")

    # ç»Ÿè®¡å®Œå…¨å‰ªç©ºçš„å±‚
    zero_layers = [l['layer_idx'] for l in comparison_result['layers'] if l['is_zero_layer']]
    if zero_layers:
        logger.log(f"\nå®Œå…¨å‰ªç©ºçš„å±‚ ({len(zero_layers)}ä¸ª): {zero_layers}")

    logger.log(f"\n{'='*60}")

    # ========== Step 8.6: æ¢¯åº¦è¯Šæ–­å’Œå¯è§†åŒ–ï¼ˆå®Œæ•´ç‰ˆï¼‰==========
    if args.importance_method in ['taylor', 'taylor_2nd'] and 'gradient_analyzer' in locals():
        logger.log(f"\n[Step 8.6] ç”Ÿæˆæ¢¯åº¦è¯Šæ–­å’Œå¯è§†åŒ–æŠ¥å‘Š...")

        num_layers = len(model.model.layers)

        # ä» comparison_result ä¸­æå–æ¯å±‚çš„å‰ªæç‡
        layer_pruning_rates = {}
        for layer_comp in comparison_result['layers']:
            layer_idx = layer_comp['layer_idx']
            # ä½¿ç”¨ MLP çš„å‰ªæç‡ä½œä¸ºå±‚å‰ªæç‡çš„ä»£è¡¨
            layer_pruning_rates[layer_idx] = layer_comp['mlp'].get('reduction_ratio', 0.0)

        # ä» df (global_analysis_table) ä¸­æå–é‡è¦æ€§å¾—åˆ†
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä» group table ä¸­èšåˆå¾—åˆ°æ¯å±‚çš„å¹³å‡é‡è¦æ€§
        layer_importance_scores = {}

        # æ£€æŸ¥ df æ˜¯å¦å­˜åœ¨å¹¶ä¸”ä¸ä¸ºç©º
        if 'df' in locals() and df is not None and not df.empty:
            for layer_idx in range(num_layers):
                # æ”¶é›†è¯¥å±‚æ‰€æœ‰ MLP groups çš„é‡è¦æ€§
                # æ³¨æ„ï¼šDataFrame åˆ—åæ˜¯ 'group_type'ï¼Œå€¼æ˜¯ 'mlp'
                layer_groups = df[(df['group_type'] == 'mlp') & (df['layer_idx'] == layer_idx)]

                if not layer_groups.empty:
                    layer_importance_scores[layer_idx] = layer_groups['importance'].mean()
                else:
                    layer_importance_scores[layer_idx] = 0.0
        else:
            # å¦‚æœ df ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
            logger.log(f"  âš ï¸  æ— æ³•æå–é‡è¦æ€§å¾—åˆ†ï¼ˆdf ä¸å­˜åœ¨ï¼‰ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            for layer_idx in range(num_layers):
                layer_importance_scores[layer_idx] = 1.0

        # ç”Ÿæˆå®Œæ•´çš„æ¢¯åº¦å¯è§†åŒ–ï¼ˆåŒ…æ‹¬é‡è¦æ€§å’Œå‰ªæç‡å¯¹æ¯”ï¼‰
        visualization_dir = output_dirs['visualization']
        gradient_analyzer.visualize_gradient_distribution(
            num_layers=num_layers,
            save_dir=visualization_dir,
            importance_scores=layer_importance_scores,
            pruning_rates=layer_pruning_rates
        )

        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        diagnosis_report = gradient_analyzer.diagnose_extreme_pruning(
            num_layers=num_layers,
            importance_scores=layer_importance_scores,
            pruning_rates=layer_pruning_rates,
            threshold=0.5  # å‰ªæç‡è¶…è¿‡ 50% è§†ä¸ºæç«¯
        )

        # æ‰“å°è¯Šæ–­æŠ¥å‘Š
        gradient_analyzer.print_diagnosis_report(diagnosis_report)

        # ä¿å­˜è¯Šæ–­æŠ¥å‘Š
        diagnosis_path = os.path.join(output_dirs['analysis'], 'gradient_diagnosis.json')
        with open(diagnosis_path, 'w') as f:
            json.dump(diagnosis_report, f, indent=2)
        logger.log(f"  âœ“ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: {diagnosis_path}")

        # å¦‚æœæ£€æµ‹åˆ°ä¸¥é‡é—®é¢˜ï¼Œç»™å‡ºå»ºè®®
        if diagnosis_report['diagnosis']:
            logger.log(f"\n{'âš ï¸ '*20}")
            logger.log(f"æ£€æµ‹åˆ°æ½œåœ¨é—®é¢˜ï¼Œå»ºè®®:")
            logger.log(f"  1. æ£€æŸ¥æ ¡å‡†æ•°æ®é›†ï¼ˆC4/Wikitext2ï¼‰æ˜¯å¦é€‚åˆå½“å‰æ¨¡å‹")
            logger.log(f"  2. å°è¯•è°ƒæ•´åºåˆ—é•¿åº¦å‚æ•°ï¼ˆ--taylor_seq_lenï¼‰")
            logger.log(f"  3. å°è¯•è°ƒæ•´æ ·æœ¬æ•°å‚æ•°ï¼ˆ--taylor_num_samplesï¼‰")
            logger.log(f"  4. ä½¿ç”¨ temperature > 0 å¯ç”¨å—çº§ä¿®æ­£")
            logger.log(f"{'âš ï¸ '*20}\n")

    # ========== Step 9: LoRA å¾®è°ƒæ¢å¤ï¼ˆå¯é€‰ï¼‰==========
    if args.finetune:
        logger.log(f"\n[Step 9] LoRA å¾®è°ƒæ¢å¤...")
        logger.log(f"  æ•°æ®é›†: {args.finetune_data_path}")
        logger.log(f"  è®­ç»ƒè½®æ•°: {args.finetune_epochs}")
        logger.log(f"  å­¦ä¹ ç‡: {args.finetune_lr}")
        logger.log(f"  Batch size: {args.finetune_batch_size} (micro: {args.finetune_micro_batch_size})")
        logger.log(f"  LoRA r={args.lora_r}, alpha={args.lora_alpha}")

        # æ„å»ºå¾®è°ƒè„šæœ¬å‘½ä»¤
        import subprocess

        finetune_cmd = [
            "python", "finetune_lora.py",
            "--pruned_model", save_path,
            "--data_path", args.finetune_data_path,
            "--num_epochs", str(args.finetune_epochs),
            "--learning_rate", str(args.finetune_lr),
            "--batch_size", str(args.finetune_batch_size),
            "--micro_batch_size", str(args.finetune_micro_batch_size),
            "--lora_r", str(args.lora_r),
            "--lora_alpha", str(args.lora_alpha),
            "--device", args.device
        ]

        # å¦‚æœæŒ‡å®šè·³è¿‡è¯„ä¼°ï¼Œæ·»åŠ å‚æ•°
        if args.skip_finetune_evaluation:
            finetune_cmd.append("--skip_evaluation")

        # æ‰§è¡Œå¾®è°ƒ
        logger.log(f"\n  å¯åŠ¨ LoRA å¾®è°ƒ...")
        try:
            result = subprocess.run(finetune_cmd, check=True, capture_output=False, text=True)
            logger.log(f"âœ“ LoRA å¾®è°ƒå®Œæˆ")

            # å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜è·¯å¾„
            finetuned_output_dir = os.path.join('results', f"{args.output_name}_finetuned")
            logger.log(f"  å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åœ¨: {finetuned_output_dir}")

            # å¦‚æœæ²¡æœ‰è·³è¿‡å¾®è°ƒåè¯„ä¼°ï¼Œè¯„ä¼°ç»“æœä¹Ÿä¿å­˜åœ¨è¯¥ç›®å½•ä¸‹
            if not args.skip_finetune_evaluation:
                finetuned_eval_path = os.path.join(finetuned_output_dir, 'evaluation', 'evaluation_results.json')
                logger.log(f"  å¾®è°ƒåçš„è¯„ä¼°ç»“æœ: {finetuned_eval_path}")

        except subprocess.CalledProcessError as e:
            logger.log(f"âš ï¸ LoRA å¾®è°ƒå¤±è´¥: {e}")
            logger.log(f"  ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")
    else:
        logger.log(f"\n[Step 9] è·³è¿‡å¾®è°ƒï¼ˆæœªæŒ‡å®š --finetuneï¼‰")

    # ========== Step 10: ä¿å­˜æ¨¡å‹ ==========
    # å‡†å¤‡æ¨¡å‹å­—å…¸ï¼ˆæ— è®ºæ˜¯å¦ä¿å­˜ï¼Œè¯„ä¼°éƒ½å¯èƒ½éœ€è¦ï¼‰
    save_dict = {
        'model': model,
        'tokenizer': tokenizer,
        'pruning_stats': pruning_stats,
        'pruning_ratio': args.pruning_ratio,
        'actual_ratio': actual_ratio,
        'method': 'H-GSP',
        'h-gsp_params': {
            'temperature': args.temperature,
            'tau': args.tau,
            'epsilon': args.epsilon
        },
        'config': args.__dict__
    }

    # æ€»æ˜¯ä¿å­˜æ¨¡å‹åˆ° models/ ç›®å½•
    logger.log(f"\n[Step 10] ä¿å­˜å‰ªæåçš„æ¨¡å‹...")
    save_path = os.path.join(output_dirs['models'], 'pruned_model.bin')
    torch.save(save_dict, save_path)
    logger.log(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {save_path}")
    logger.log(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(save_path) / (1024**3):.2f} GB")

    # ========== Step 10.5: ç”Ÿæˆå‰ªæå¯è§†åŒ–å›¾è¡¨ ==========
    logger.log(f"\n[Step 10.5] ç”Ÿæˆå‰ªæå¯è§†åŒ–å›¾è¡¨...")
    try:
        # é…ç½®ä¸­æ–‡å­—ä½“
        font_used = setup_chinese_font()
        
        if font_used:
            logger.log(f"  ä½¿ç”¨å­—ä½“: {font_used}")
            use_english = False
        else:
            logger.log(f"  âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
            logger.log(f"  æç¤º: å¯å®‰è£…ä¸­æ–‡å­—ä½“: sudo apt-get install fonts-wqy-microhei")
            use_english = True

        # ç”Ÿæˆå›¾è¡¨
        generate_pruning_charts(
            pruning_data=comparison_result,
            model_name=args.output_name,
            output_dir=output_dirs['visualization'],
            use_english=use_english,
        )
        logger.log(f"  âœ“ å‰ªæå›¾è¡¨å·²ä¿å­˜åˆ°: {output_dirs['visualization']}")
    except Exception as e:
        logger.log(f"  âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        logger.log(f"  æç¤º: è¯·ç¡®ä¿å®‰è£…äº† matplotlib: pip install matplotlib")

    args.run_evaluation = False  # ä¸´æ—¶ç¦ç”¨è¯„ä¼°ä»¥èŠ‚çœæ—¶é—´
    # ========== Step 11: è¿è¡Œè¯„ä¼°æµ‹è¯•ï¼ˆå¯é€‰ï¼‰==========
    if args.run_evaluation:
        logger.log(f"\n[Step 11] è¿è¡Œè¯„ä¼°æµ‹è¯•...")

        # æ¸…ç†æ˜¾å­˜ï¼šé‡Šæ”¾å‰ªæåçš„æ¨¡å‹ï¼Œä¸ºè¯„ä¼°è…¾å‡ºç©ºé—´
        logger.log(f"  æ¸…ç†æ˜¾å­˜...")
        del model
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        logger.log(f"  âœ“ æ˜¾å­˜å·²æ¸…ç†")

        # è§£æè¯„ä¼°ç±»å‹
        eval_types = [t.strip() for t in args.run_evaluation.split(',')]
        if 'all' in eval_types:
            eval_types = ['ppl', 'zeroshot', 'efficiency']

        logger.log(f"  è¯„ä¼°ç±»å‹: {', '.join(eval_types)}")

        # è§£ææ•°æ®é›†å’Œä»»åŠ¡
        ppl_datasets = [d.strip() for d in args.eval_ppl_datasets.split(',')] if 'ppl' in eval_types else None
        zeroshot_tasks = [t.strip() for t in args.eval_zeroshot_tasks.split(',')] if 'zeroshot' in eval_types else None

        # è¿è¡Œè¯„ä¼°
        logger.log(f"\n  å¼€å§‹è¯„ä¼°...")
        eval_results = evaluate_single_model(
            model_path=save_path,
            metrics=eval_types,
            device=args.device,
            ppl_datasets=ppl_datasets,
            ppl_seq_len=args.eval_ppl_seq_len,
            ppl_stride=args.eval_ppl_stride,
            zeroshot_tasks=zeroshot_tasks,
            speed_samples=50,
            verbose=True,
            use_custom_zeroshot=args.eval_use_custom_zeroshot,
            zeroshot_batch_size=8
        )

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_result_path = os.path.join(output_dirs['evaluation'], 'evaluation_results.json')
        with open(eval_result_path, 'w') as f:
            json.dump(eval_results, f, indent=2)
        logger.log(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_result_path}")

        # æ‰“å°ç®€è¦è¯„ä¼°æ‘˜è¦
        logger.log(f"\n{'='*60}")
        logger.log(f"è¯„ä¼°ç»“æœæ‘˜è¦")
        logger.log(f"{'='*60}")
        if 'ppl' in eval_results.get('metrics', {}):
            logger.log(f"\nPPL ç»“æœ:")
            for dataset, ppl in eval_results['metrics']['ppl'].items():
                logger.log(f"  {dataset}: {ppl:.2f}" if ppl else f"  {dataset}: N/A")

        if 'avg_zeroshot_acc' in eval_results.get('metrics', {}):
            acc = eval_results['metrics']['avg_zeroshot_acc']
            logger.log(f"\nZero-shot å¹³å‡å‡†ç¡®ç‡: {acc*100:.2f}%")

        if 'efficiency' in eval_results.get('metrics', {}):
            eff = eval_results['metrics']['efficiency']
            if 'speed' in eff:
                throughput = eff['speed'].get('batch_size_1', {}).get('throughput_tokens_per_sec', 'N/A')
                logger.log(f"\næ¨ç†é€Ÿåº¦: {throughput:.1f} tokens/s" if isinstance(throughput, (int, float)) else f"\næ¨ç†é€Ÿåº¦: {throughput}")
            if 'memory' in eff:
                mem = eff['memory'].get('model_memory_mb', 'N/A')
                logger.log(f"GPU æ˜¾å­˜: {mem:.0f} MB" if isinstance(mem, (int, float)) else f"GPU æ˜¾å­˜: {mem}")
    else:
        logger.log(f"\n[Step 11] è·³è¿‡è¯„ä¼°æµ‹è¯•ï¼ˆæœªæŒ‡å®š --run_evaluationï¼‰")

    logger.log(f"\n{'='*60}")
    logger.log(f"âœ“ å…¨éƒ¨å®Œæˆï¼")
    logger.log(f"{'='*60}")
    logger.log(f"\nè¾“å‡ºç›®å½•: {output_dirs['base']}")
    logger.log(f"  - æ¨¡å‹: {output_dirs['models']}")
    logger.log(f"  - åˆ†æç»“æœ: {output_dirs['analysis']}")
    logger.log(f"  - è¯„ä¼°ç»“æœ: {output_dirs['evaluation']}")
    logger.log(f"  - æ—¥å¿—: {output_dirs['logs']}")


if __name__ == '__main__':
    main()
