# æ ¡å‡†æ•°æ®é›†é…ç½®å¯¹æ¯”

æœ¬æ–‡æ¡£è®°å½•æ‰€æœ‰å‰ªææ–¹æ³•çš„æ ¡å‡†æ•°æ®é›†é…ç½®ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”å®éªŒã€‚

## âš ï¸ é‡è¦æ€§

**ä¸ºäº†ç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼Œæ‰€æœ‰éœ€è¦æ ¡å‡†æ•°æ®çš„æ–¹æ³•å¿…é¡»ä½¿ç”¨ç›¸åŒçš„é…ç½®ï¼š**
- ç›¸åŒçš„æ•°æ®é›†
- ç›¸åŒçš„æ ·æœ¬æ•°é‡
- ç›¸åŒçš„åºåˆ—é•¿åº¦
- ç›¸åŒçš„batch_size

## ğŸ“Š å½“å‰é…ç½®æ€»ç»“

| æ–¹æ³• | æ•°æ®é›† | æ ·æœ¬æ•° | seq_len | batch_size | å¤‡æ³¨ |
|------|--------|--------|---------|------------|------|
| **H-GSP (Taylor)** | wikitext2 | 128 | 128 | 4 | å†…éƒ¨å›ºå®š |
| **H-GSP (Wanda)** | wikitext2 | 128 | 128 | 4 | å†…éƒ¨å›ºå®š |
| **Wanda baseline** | wikitext2 | 128 | - | - | é€šè¿‡ --calibration_samples |
| **SlimGPT** | wikitext2 | 64 | 128 | 1 | âš ï¸ ä¸ä¸€è‡´ |
| **ShortGPT** | wikitext2 | 50 | 512 | - | âš ï¸ ä¸ä¸€è‡´ |
| **Magnitude** | - | - | - | - | âœ“ æ— éœ€æ ¡å‡†æ•°æ® |

## ğŸ” è¯¦ç»†é…ç½®

### 1. H-GSP (æˆ‘ä»¬çš„æ–¹æ³•)

**æ–‡ä»¶**: `run_global_pruning.py`

**å†…éƒ¨å›ºå®šå‚æ•°** (ä¸å¯¹å¤–æš´éœ²):
```python
# Taylor importance è®¡ç®— (ç”¨äºå®½åº¦å‰ªæ)
TAYLOR_NUM_SAMPLES = 128          # æ¢¯åº¦è®¡ç®—æ ·æœ¬æ•°
TAYLOR_SEQ_LEN = 128              # åºåˆ—é•¿åº¦
gradient_batch_size = 4            # æ‰¹æ¬¡å¤§å° (å¯é€šè¿‡ --gradient_batch_size ä¿®æ”¹)

# Layer importance è®¡ç®— (ç”¨äºæ·±åº¦å‰ªæ)
LAYER_IMPORTANCE_NUM_SAMPLES = 50  # å±‚çº§é‡è¦åº¦æ ·æœ¬æ•°
LAYER_IMPORTANCE_SEQ_LEN = 128     # åºåˆ—é•¿åº¦

# Block importance è®¡ç®— (ç”¨äºæ··åˆå‰ªæ)
BLOCK_IMPORTANCE_NUM_SAMPLES = 50  # å—çº§é‡è¦åº¦æ ·æœ¬æ•°
BLOCK_IMPORTANCE_SEQ_LEN = 128     # åºåˆ—é•¿åº¦
```

**æ•°æ®é›†**: `wikitext2` (é»˜è®¤ï¼Œå¯é€šè¿‡ `--dataset` ä¿®æ”¹)

**å…³é”®ä»£ç ä½ç½®**:
- run_global_pruning.py:586-592 (å‚æ•°å®šä¹‰)
- run_global_pruning.py:594-679 (Taylor è®¡ç®—)
- run_global_pruning.py:685-730 (Wanda æ¿€æ´»æ”¶é›†)

**ä½¿ç”¨æ–¹æ³•**:
```bash
python run_global_pruning.py \
    --base_model /path/to/llama \
    --pruning_ratio 0.2 \
    --importance_method taylor \
    --dataset wikitext2 \
    --gradient_batch_size 4
```

---

### 2. Wanda Baseline

**æ–‡ä»¶**: `baselines/run_wanda.py`

**é…ç½®å‚æ•°**:
```python
--calibration_samples 128    # é»˜è®¤: 128
--dataset wikitext2          # é»˜è®¤: wikitext2
```

**å†…éƒ¨ä½¿ç”¨**: æœ€ç»ˆè°ƒç”¨ `run_global_pruning.py`ï¼Œä½¿ç”¨ç›¸åŒçš„å†…éƒ¨å›ºå®šå‚æ•°

**ä½¿ç”¨æ–¹æ³•**:
```bash
python baselines/run_wanda.py \
    --base_model /path/to/llama \
    --pruning_ratio 0.2 \
    --calibration_samples 128 \
    --dataset wikitext2
```

---

### 3. SlimGPT Baseline

**æ–‡ä»¶**: `baselines/run_slimgpt.py`

**é…ç½®å‚æ•°**:
```python
--dataset wikitext2          # é»˜è®¤: wikitext2
--num_samples 64             # é»˜è®¤: 64 âš ï¸
--seq_len 128                # é»˜è®¤: 128
--max_samples 128            # Hessian æœ€å¤§tokenæ•° (128k)
batch_size = 1               # å›ºå®šåœ¨ä»£ç ä¸­
```

**âš ï¸ é—®é¢˜**:
- `num_samples=64` ä¸ H-GSP çš„ 128 ä¸ä¸€è‡´
- `batch_size=1` å›ºå®šï¼Œæ— æ³•ä¿®æ”¹

**å…³é”®ä»£ç ä½ç½®**:
- baselines/run_slimgpt.py:263-268 (å‚æ•°å®šä¹‰)
- baselines/run_slimgpt.py:62 (dataloader åˆ›å»ºï¼Œbatch_size=1)

**ä½¿ç”¨æ–¹æ³•**:
```bash
python baselines/run_slimgpt.py \
    --base_model /path/to/llama \
    --pruning_ratio 0.2 \
    --num_samples 128 \
    --seq_len 128 \
    --dataset wikitext2
```

---

### 4. ShortGPT Baseline

**æ–‡ä»¶**: `baselines/run_shortgpt.py`

**é…ç½®å‚æ•°**:
```python
--dataset wikitext2          # é»˜è®¤: wikitext2
--num_samples 50             # é»˜è®¤: 50 âš ï¸
--seq_len 512                # é»˜è®¤: 512 âš ï¸
--stride 256                 # æ»‘åŠ¨çª—å£æ­¥é•¿
```

**âš ï¸ é—®é¢˜**:
- `num_samples=50` ä¸ H-GSP çš„ 128 ä¸ä¸€è‡´
- `seq_len=512` ä¸ H-GSP çš„ 128 ä¸ä¸€è‡´

**å…³é”®ä»£ç ä½ç½®**:
- baselines/run_shortgpt.py:70-77 (å‚æ•°å®šä¹‰)

**ä½¿ç”¨æ–¹æ³•**:
```bash
python baselines/run_shortgpt.py \
    --base_model /path/to/llama \
    --n_remove_layers 6 \
    --num_samples 128 \
    --seq_len 128 \
    --dataset wikitext2
```

---

### 5. Magnitude Baseline

**æ–‡ä»¶**: `baselines/run_magnitude.py`

**é…ç½®**: âœ“ **æ— éœ€æ ¡å‡†æ•°æ®**

åªä½¿ç”¨æƒé‡ç»å¯¹å€¼ï¼Œä¸ä¾èµ–æ•°æ®é›†ã€‚

---

## âœ… æ¨èçš„ç»Ÿä¸€é…ç½®

ä¸ºç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼Œå»ºè®®æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ä»¥ä¸‹ç»Ÿä¸€é…ç½®ï¼š

```bash
# ç»Ÿä¸€çš„æ ¡å‡†é…ç½®
DATASET=wikitext2
NUM_SAMPLES=128
SEQ_LEN=128
BATCH_SIZE=4  # SlimGPT é™¤å¤–ï¼ˆå›ºå®šä¸º1ï¼‰
```

### å®Œæ•´å®éªŒè„šæœ¬ (ç»Ÿä¸€é…ç½®)

```bash
MODEL=/path/to/llama
DATASET=wikitext2
SAMPLES=128
SEQ_LEN=128

# 1. H-GSP (Taylor) - é»˜è®¤å·²ä½¿ç”¨ 128 samples
python run_global_pruning.py \
    --base_model $MODEL \
    --output_name HGSP_Taylor_20 \
    --pruning_ratio 0.2 \
    --importance_method taylor \
    --dataset $DATASET \
    --gradient_batch_size 4

# 2. H-GSP (Wanda) - é»˜è®¤å·²ä½¿ç”¨ 128 samples
python run_global_pruning.py \
    --base_model $MODEL \
    --output_name HGSP_Wanda_20 \
    --pruning_ratio 0.2 \
    --importance_method wanda \
    --dataset $DATASET \
    --gradient_batch_size 4

# 3. Wanda baseline - æ˜¾å¼æŒ‡å®š 128 samples
python baselines/run_wanda.py \
    --base_model $MODEL \
    --pruning_ratio 0.2 \
    --calibration_samples $SAMPLES \
    --dataset $DATASET

# 4. SlimGPT - ä¿®æ”¹ä¸º 128 samples (åŸé»˜è®¤ 64)
python baselines/run_slimgpt.py \
    --base_model $MODEL \
    --pruning_ratio 0.2 \
    --num_samples $SAMPLES \
    --seq_len $SEQ_LEN \
    --dataset $DATASET

# 5. ShortGPT - ä¿®æ”¹ä¸º 128 samples, seq_len 128 (åŸé»˜è®¤ 50, 512)
python baselines/run_shortgpt.py \
    --base_model $MODEL \
    --n_remove_layers 6 \
    --num_samples $SAMPLES \
    --seq_len $SEQ_LEN \
    --dataset $DATASET

# 6. Magnitude - æ— éœ€æ ¡å‡†æ•°æ®
python baselines/run_magnitude.py \
    --base_model $MODEL \
    --pruning_ratio 0.2
```

---

## ğŸ”§ éœ€è¦ä¿®æ”¹çš„åœ°æ–¹

### âš ï¸ SlimGPT

**é—®é¢˜**:
1. é»˜è®¤ `num_samples=64`ï¼Œéœ€æ”¹ä¸º 128
2. `batch_size=1` ç¡¬ç¼–ç åœ¨ `create_dataloader()` ä¸­

**ä¿®æ”¹å»ºè®®**:
```python
# baselines/run_slimgpt.py:263
parser.add_argument('--num_samples', type=int, default=128,  # æ”¹ä¸º 128
                   help='Hessian è®¡ç®—æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 128ï¼‰')

# baselines/run_slimgpt.py:62
def create_dataloader(dataset_manager, num_samples, seq_len, batch_size=4):  # æ”¹ä¸º 4
```

### âš ï¸ ShortGPT

**é—®é¢˜**:
1. é»˜è®¤ `num_samples=50`ï¼Œéœ€æ”¹ä¸º 128
2. é»˜è®¤ `seq_len=512`ï¼Œéœ€æ”¹ä¸º 128ï¼ˆä¸å…¶ä»–æ–¹æ³•ä¸€è‡´ï¼‰

**ä¿®æ”¹å»ºè®®**:
```python
# baselines/run_shortgpt.py:73
parser.add_argument('--num_samples', type=int, default=128,  # æ”¹ä¸º 128
                   help='BI è®¡ç®—æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 128ï¼‰')

# baselines/run_shortgpt.py:75
parser.add_argument('--seq_len', type=int, default=128,  # æ”¹ä¸º 128
                   help='åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 128ï¼‰')
```

**æ³¨æ„**: ShortGPT ä½¿ç”¨è¾ƒé•¿çš„ `seq_len=512` å¯èƒ½æœ‰å…¶ç†è®ºä¾æ®ï¼ˆBI è®¡ç®—éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼‰ï¼Œå¦‚æœæ”¹ä¸º 128 å¯èƒ½å½±å“æ•ˆæœã€‚å»ºè®®ï¼š
- **é€‰é¡¹1**: ä¿æŒ 512ï¼Œä½†åœ¨è®ºæ–‡ä¸­è¯´æ˜å·®å¼‚
- **é€‰é¡¹2**: æ”¹ä¸º 128ï¼Œç¡®ä¿å®Œå…¨å…¬å¹³å¯¹æ¯”
- **é€‰é¡¹3**: ä¸¤ç§é…ç½®éƒ½æµ‹è¯•ï¼Œåˆ†åˆ«æŠ¥å‘Šç»“æœ

---

## ğŸ“ æ•°æ®é›†è¯´æ˜

### WikiText-2

- **ç±»å‹**: è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡æœ¬
- **ç”¨é€”**: LLM æ ¡å‡†å’Œè¯„ä¼°çš„æ ‡å‡†æ•°æ®é›†
- **ä¼˜åŠ¿**: å¹²å‡€ã€ç»“æ„åŒ–ã€ä»£è¡¨æ€§å¼º

### å…¶ä»–å¯é€‰æ•°æ®é›†

- **PTB** (Penn Treebank): è¾ƒå°ï¼Œæ–°é—»æ–‡æœ¬
- **C4** (Colossal Clean Crawled Corpus): æ›´å¤§æ›´å¤šæ ·

**å»ºè®®**: ä½¿ç”¨ WikiText-2ï¼Œä¸å¤§å¤šæ•°è®ºæ–‡ä¸€è‡´ã€‚

---

## ğŸ¯ æ£€æŸ¥æ¸…å•

åœ¨è¿è¡Œå¯¹æ¯”å®éªŒå‰ï¼Œè¯·ç¡®è®¤ï¼š

- [ ] æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›† (wikitext2)
- [ ] æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬æ•° (128)
- [ ] æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„åºåˆ—é•¿åº¦ (128)
- [ ] SlimGPT å’Œ ShortGPT å·²æ›´æ–°é»˜è®¤å‚æ•°
- [ ] è®°å½•æ‰€æœ‰å®éªŒé…ç½®åˆ°æ—¥å¿—æ–‡ä»¶
- [ ] åœ¨è®ºæ–‡ä¸­æ˜ç¡®è¯´æ˜æ ¡å‡†æ•°æ®é…ç½®

---

## ğŸ“š å‚è€ƒä¿¡æ¯

### H-GSP å†…éƒ¨å‚æ•°æ¥æº

**ä¸ºä»€ä¹ˆé€‰æ‹© 128 samples?**
- å¹³è¡¡è®¡ç®—æˆæœ¬å’Œå‡†ç¡®æ€§
- ä¸ Wanda è®ºæ–‡ä¸€è‡´ (128 samples)
- è¶³å¤Ÿç»Ÿè®¡æ˜¾è‘—æ€§

**ä¸ºä»€ä¹ˆé€‰æ‹© seq_len=128?**
- æ ‡å‡† LLM è¯„ä¼°é•¿åº¦
- å¿«é€Ÿè¿­ä»£æµ‹è¯•
- ä¸å¤§å¤šæ•°å‰ªæè®ºæ–‡ä¸€è‡´

**ä¸ºä»€ä¹ˆ gradient_batch_size=4?**
- é€‚é…å¤§å¤šæ•°å•å¡ GPU (24GB)
- é¿å… OOM
- å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´ (1-8)

### Batch Size è¯´æ˜

- **Taylor/Wanda**: batch_size=4ï¼Œåˆ†æ‰¹è®¡ç®—èŠ‚çœæ˜¾å­˜
- **SlimGPT**: batch_size=1ï¼ŒHessian è®¡ç®—é€æ ·æœ¬å¤„ç†
- **ShortGPT**: æ—  batch_sizeï¼Œé€æ–‡æœ¬è®¡ç®— BI

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸åŒæ–¹æ³•çš„ batch_size ä¸åŒï¼Ÿ

**A**: batch_size åªå½±å“è®¡ç®—æ•ˆç‡å’Œæ˜¾å­˜ä½¿ç”¨ï¼Œä¸å½±å“æœ€ç»ˆç»“æœï¼ˆå‡è®¾æ­£ç¡®å½’ä¸€åŒ–ï¼‰ã€‚å…³é”®æ˜¯æ ·æœ¬æ•°å’Œåºåˆ—é•¿åº¦ä¸€è‡´ã€‚

### Q2: ShortGPT èƒ½å¦ä½¿ç”¨ seq_len=128 è€Œé 512ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†å¯èƒ½å½±å“ BI è®¡ç®—å‡†ç¡®æ€§ï¼ˆæ›´çŸ­çš„åºåˆ—å¯èƒ½æ— æ³•å……åˆ†ä½“ç°å±‚çš„å˜æ¢ä½œç”¨ï¼‰ã€‚å»ºè®®ä¸¤ç§é…ç½®éƒ½æµ‹è¯•ã€‚

### Q3: SlimGPT çš„ max_samples æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: é™åˆ¶ Hessian è®¡ç®—çš„æ€» token æ•°ï¼ˆ128k = 128000 tokensï¼‰ï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºã€‚ä¸å½±å“æ ¡å‡†æ ·æœ¬æ•°ã€‚

### Q4: æˆ‘åº”è¯¥å¦‚ä½•æŠ¥å‘Šå®éªŒé…ç½®ï¼Ÿ

**A**: åœ¨è®ºæ–‡ Methods æˆ– Appendix ä¸­æ˜ç¡®è¯´æ˜ï¼š
- æ•°æ®é›†åç§°å’Œç‰ˆæœ¬
- æ ·æœ¬æ•°å’Œåºåˆ—é•¿åº¦
- ä»»ä½•æ–¹æ³•ç‰¹å®šçš„å‚æ•°ï¼ˆå¦‚ max_samplesï¼‰
- ç¡¬ä»¶é…ç½®ï¼ˆGPU å‹å·ã€æ˜¾å­˜ï¼‰

---

æœ€åæ›´æ–°: 2025-11-26
