#!/usr/bin/env python3
"""
æ¢¯åº¦åˆ†æå’Œå¯è§†åŒ–å·¥å…·
ç”¨äºè¯Šæ–­å’Œå¯è§†åŒ–æ¨¡å‹å„å±‚çš„æ¢¯åº¦åˆ†å¸ƒï¼Œå¸®åŠ©ç†è§£æç«¯å‰ªæé—®é¢˜
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
import json
import os


class GradientAnalyzer:
    """æ¢¯åº¦åˆ†æå™¨ - æ”¶é›†ã€åˆ†æå’Œå¯è§†åŒ–æ¨¡å‹æ¢¯åº¦"""

    def __init__(self, model, logger=None):
        self.model = model
        self.logger = logger
        self.gradient_stats = defaultdict(lambda: {
            'mean': [],
            'std': [],
            'norm': [],
            'max': [],
            'min': []
        })

    def log(self, message):
        """æ—¥å¿—è¾“å‡º"""
        if self.logger:
            self.logger.log(message)
        else:
            print(message)

    def collect_gradient_stats(self, layer_prefix='model.layers'):
        """
        æ”¶é›†å½“å‰æ¨¡å‹çš„æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯

        Args:
            layer_prefix: å±‚åç§°å‰ç¼€ï¼Œç”¨äºè¿‡æ»¤ç‰¹å®šå±‚

        Returns:
            Dict[str, Dict]: å„å±‚çš„æ¢¯åº¦ç»Ÿè®¡
        """
        stats = {}

        for name, param in self.model.named_parameters():
            if param.grad is None:
                continue

            # åªæ”¶é›†æŒ‡å®šå‰ç¼€çš„å±‚
            if layer_prefix and not name.startswith(layer_prefix):
                continue

            grad = param.grad.detach()

            stats[name] = {
                'mean': grad.abs().mean().item(),
                'std': grad.abs().std().item(),
                'norm': grad.norm(p=2).item(),
                'max': grad.abs().max().item(),
                'min': grad.abs().min().item(),
                'shape': list(grad.shape)
            }

        return stats

    def accumulate_gradient_stats(self, layer_prefix='model.layers'):
        """
        ç´¯ç§¯æ¢¯åº¦ç»Ÿè®¡ï¼ˆç”¨äºå¤šæ‰¹æ¬¡åˆ†æï¼‰

        Args:
            layer_prefix: å±‚åç§°å‰ç¼€
        """
        current_stats = self.collect_gradient_stats(layer_prefix)

        for name, stat in current_stats.items():
            self.gradient_stats[name]['mean'].append(stat['mean'])
            self.gradient_stats[name]['std'].append(stat['std'])
            self.gradient_stats[name]['norm'].append(stat['norm'])
            self.gradient_stats[name]['max'].append(stat['max'])
            self.gradient_stats[name]['min'].append(stat['min'])

    def get_layer_gradient_summary(self, num_layers: int) -> Dict[int, Dict]:
        """
        è·å–æ¯å±‚çš„æ¢¯åº¦æ±‡æ€»ç»Ÿè®¡

        Args:
            num_layers: å±‚æ•°

        Returns:
            Dict[int, Dict]: {layer_idx: {metric: value}}
        """
        layer_summary = {}

        for layer_idx in range(num_layers):
            # æ”¶é›†è¯¥å±‚æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
            layer_grads = {
                'mean': [],
                'std': [],
                'norm': [],
                'max': [],
                'min': []
            }

            for name, stats in self.gradient_stats.items():
                if f'model.layers.{layer_idx}.' in name:
                    # å¯¹æ¯ä¸ªæŒ‡æ ‡å–å¹³å‡
                    layer_grads['mean'].extend(stats['mean'])
                    layer_grads['std'].extend(stats['std'])
                    layer_grads['norm'].extend(stats['norm'])
                    layer_grads['max'].extend(stats['max'])
                    layer_grads['min'].extend(stats['min'])

            # è®¡ç®—è¯¥å±‚çš„ç»Ÿè®¡
            if layer_grads['mean']:
                layer_summary[layer_idx] = {
                    'mean': np.mean(layer_grads['mean']),
                    'std': np.mean(layer_grads['std']),
                    'norm': np.mean(layer_grads['norm']),
                    'max': np.max(layer_grads['max']),
                    'min': np.min(layer_grads['min'])
                }
            else:
                layer_summary[layer_idx] = {
                    'mean': 0.0,
                    'std': 0.0,
                    'norm': 0.0,
                    'max': 0.0,
                    'min': 0.0
                }

        return layer_summary

    def visualize_gradient_distribution(
        self,
        num_layers: int,
        save_dir: str,
        importance_scores: Optional[Dict] = None,
        pruning_rates: Optional[Dict] = None
    ):
        """
        å¯è§†åŒ–æ¢¯åº¦åˆ†å¸ƒ

        Args:
            num_layers: å±‚æ•°
            save_dir: ä¿å­˜ç›®å½•
            importance_scores: é‡è¦æ€§å¾—åˆ†ï¼ˆå¯é€‰ï¼‰
            pruning_rates: å‰ªæç‡ï¼ˆå¯é€‰ï¼‰
        """
        os.makedirs(save_dir, exist_ok=True)

        layer_summary = self.get_layer_gradient_summary(num_layers)
        layers = sorted(layer_summary.keys())

        # æå–å„é¡¹æŒ‡æ ‡
        means = [layer_summary[i]['mean'] for i in layers]
        stds = [layer_summary[i]['std'] for i in layers]
        norms = [layer_summary[i]['norm'] for i in layers]
        maxs = [layer_summary[i]['max'] for i in layers]
        mins = [layer_summary[i]['min'] for i in layers]

        # åˆ›å»ºå¤šå­å›¾
        num_plots = 3 if importance_scores and pruning_rates else 2
        fig, axes = plt.subplots(num_plots, 1, figsize=(15, 5 * num_plots))
        if num_plots == 1:
            axes = [axes]

        # 1. æ¢¯åº¦ç»Ÿè®¡ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€èŒƒæ•°ï¼‰
        ax1 = axes[0]
        ax1.plot(layers, means, 'o-', label='Mean', linewidth=2, markersize=6)
        ax1.plot(layers, stds, 's-', label='Std', linewidth=2, markersize=6)
        ax1.plot(layers, norms, '^-', label='Norm', linewidth=2, markersize=6)
        ax1.set_xlabel('Layer Index', fontsize=12)
        ax1.set_ylabel('Gradient Magnitude', fontsize=12)
        ax1.set_title('Gradient Statistics by Layer', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦

        # 2. æ¢¯åº¦èŒƒå›´ï¼ˆæœ€å¤§å€¼å’Œæœ€å°å€¼ï¼‰
        ax2 = axes[1]
        ax2.fill_between(layers, mins, maxs, alpha=0.3, label='Min-Max Range')
        ax2.plot(layers, maxs, 'ro-', label='Max', linewidth=2, markersize=5)
        ax2.plot(layers, mins, 'bo-', label='Min', linewidth=2, markersize=5)
        ax2.set_xlabel('Layer Index', fontsize=12)
        ax2.set_ylabel('Gradient Magnitude', fontsize=12)
        ax2.set_title('Gradient Range by Layer', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=11)
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')

        # 3. æ¢¯åº¦ vs é‡è¦æ€§ vs å‰ªæç‡ï¼ˆå¦‚æœæä¾›ï¼‰
        if importance_scores and pruning_rates:
            ax3 = axes[2]

            # å½’ä¸€åŒ–ä»¥ä¾¿åœ¨åŒä¸€å›¾ä¸Šæ˜¾ç¤º
            norm_means = np.array(means)
            norm_means = (norm_means - norm_means.min()) / (norm_means.max() - norm_means.min() + 1e-8)

            importance_values = [importance_scores.get(i, 0) for i in layers]
            norm_importance = np.array(importance_values)
            if norm_importance.max() > 0:
                norm_importance = (norm_importance - norm_importance.min()) / (norm_importance.max() - norm_importance.min() + 1e-8)

            pruning_values = [pruning_rates.get(i, 0) for i in layers]

            ax3_twin1 = ax3.twinx()
            ax3_twin2 = ax3.twinx()
            ax3_twin2.spines['right'].set_position(('outward', 60))

            p1 = ax3.plot(layers, norm_means, 'g^-', label='Norm. Gradient Mean',
                         linewidth=2, markersize=6)
            p2 = ax3_twin1.plot(layers, norm_importance, 'bs-', label='Norm. Importance',
                               linewidth=2, markersize=6)
            p3 = ax3_twin2.plot(layers, pruning_values, 'ro-', label='Pruning Rate',
                               linewidth=2, markersize=6)

            ax3.set_xlabel('Layer Index', fontsize=12)
            ax3.set_ylabel('Normalized Gradient Mean', fontsize=12, color='g')
            ax3_twin1.set_ylabel('Normalized Importance', fontsize=12, color='b')
            ax3_twin2.set_ylabel('Pruning Rate', fontsize=12, color='r')
            ax3.set_title('Gradient vs Importance vs Pruning Rate', fontsize=14, fontweight='bold')

            ax3.tick_params(axis='y', labelcolor='g')
            ax3_twin1.tick_params(axis='y', labelcolor='b')
            ax3_twin2.tick_params(axis='y', labelcolor='r')

            # ç»„åˆå›¾ä¾‹
            lines = p1 + p2 + p3
            labels = [l.get_label() for l in lines]
            ax3.legend(lines, labels, loc='upper left', fontsize=11)
            ax3.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = os.path.join(save_dir, 'gradient_analysis.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        self.log(f"âœ“ æ¢¯åº¦åˆ†å¸ƒå¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    def diagnose_extreme_pruning(
        self,
        num_layers: int,
        importance_scores: Dict[int, float],
        pruning_rates: Dict[int, float],
        threshold: float = 0.5
    ) -> Dict:
        """
        è¯Šæ–­æç«¯å‰ªæé—®é¢˜

        Args:
            num_layers: å±‚æ•°
            importance_scores: é‡è¦æ€§å¾—åˆ†
            pruning_rates: å‰ªæç‡
            threshold: æç«¯å‰ªæé˜ˆå€¼ï¼ˆé»˜è®¤ 0.5ï¼‰

        Returns:
            Dict: è¯Šæ–­æŠ¥å‘Š
        """
        layer_summary = self.get_layer_gradient_summary(num_layers)

        # æ‰¾å‡ºæç«¯å‰ªæçš„å±‚
        extreme_layers = []
        for layer_idx, rate in pruning_rates.items():
            if rate > threshold:
                extreme_layers.append({
                    'layer_idx': layer_idx,
                    'pruning_rate': rate,
                    'importance': importance_scores.get(layer_idx, 0),
                    'gradient_mean': layer_summary.get(layer_idx, {}).get('mean', 0),
                    'gradient_norm': layer_summary.get(layer_idx, {}).get('norm', 0)
                })

        # æ¢¯åº¦ç»Ÿè®¡
        all_grad_means = [layer_summary[i]['mean'] for i in range(num_layers)]
        all_grad_norms = [layer_summary[i]['norm'] for i in range(num_layers)]

        report = {
            'extreme_pruning_layers': extreme_layers,
            'num_extreme_layers': len(extreme_layers),
            'gradient_statistics': {
                'mean_range': (min(all_grad_means), max(all_grad_means)),
                'mean_ratio': max(all_grad_means) / (min(all_grad_means) + 1e-10),
                'norm_range': (min(all_grad_norms), max(all_grad_norms)),
                'norm_ratio': max(all_grad_norms) / (min(all_grad_norms) + 1e-10)
            },
            'diagnosis': []
        }

        # è¯Šæ–­åˆ†æ
        if report['gradient_statistics']['mean_ratio'] > 1000:
            report['diagnosis'].append({
                'issue': 'æ¢¯åº¦å°ºåº¦å·®å¼‚è¿‡å¤§',
                'severity': 'high',
                'description': f"æ¢¯åº¦å‡å€¼åœ¨ä¸åŒå±‚é—´ç›¸å·® {report['gradient_statistics']['mean_ratio']:.1f} å€",
                'recommendation': 'å»ºè®®ä½¿ç”¨ layer-wise æ¢¯åº¦å½’ä¸€åŒ–æˆ–å¯¹æ•°å˜æ¢'
            })

        if len(extreme_layers) > num_layers * 0.2:
            report['diagnosis'].append({
                'issue': 'å¤§é‡å±‚è¢«è¿‡åº¦å‰ªæ',
                'severity': 'high',
                'description': f"{len(extreme_layers)} å±‚çš„å‰ªæç‡è¶…è¿‡ {threshold*100}%",
                'recommendation': 'å»ºè®®é™åˆ¶å‰ªæç‡èŒƒå›´ï¼ˆmin_rate, max_rateï¼‰æˆ–ä½¿ç”¨æ¸©åº¦å¹³æ»‘'
            })

        # æ£€æŸ¥æ˜¯å¦å‰å‡ å±‚è¢«è¿‡åº¦å‰ªæ
        early_extreme = [l for l in extreme_layers if l['layer_idx'] < 5]
        if early_extreme:
            report['diagnosis'].append({
                'issue': 'å‰å‡ å±‚è¢«è¿‡åº¦å‰ªæ',
                'severity': 'critical',
                'description': f"å‰5å±‚ä¸­æœ‰ {len(early_extreme)} å±‚è¢«è¿‡åº¦å‰ªæ",
                'recommendation': 'å‰å‡ å±‚é€šå¸¸å¾ˆé‡è¦ï¼Œå»ºè®®ä¸ºå…¶è®¾ç½®è¾ƒä½çš„ max_rate'
            })

        return report

    def print_diagnosis_report(self, report: Dict):
        """æ‰“å°è¯Šæ–­æŠ¥å‘Š"""
        self.log("\n" + "="*80)
        self.log("æ¢¯åº¦è¯Šæ–­æŠ¥å‘Š")
        self.log("="*80)

        self.log(f"\næç«¯å‰ªæå±‚æ•°: {report['num_extreme_layers']}")
        if report['extreme_pruning_layers']:
            self.log("\næç«¯å‰ªæçš„å±‚:")
            for layer_info in report['extreme_pruning_layers'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                self.log(f"  Layer {layer_info['layer_idx']:2d}: "
                        f"å‰ªæç‡={layer_info['pruning_rate']:.2%}, "
                        f"é‡è¦æ€§={layer_info['importance']:.4e}, "
                        f"æ¢¯åº¦å‡å€¼={layer_info['gradient_mean']:.4e}")

        self.log(f"\næ¢¯åº¦ç»Ÿè®¡:")
        stats = report['gradient_statistics']
        self.log(f"  æ¢¯åº¦å‡å€¼èŒƒå›´: {stats['mean_range'][0]:.4e} ~ {stats['mean_range'][1]:.4e}")
        self.log(f"  æ¢¯åº¦å‡å€¼æ¯”ç‡: {stats['mean_ratio']:.2f}x")
        self.log(f"  æ¢¯åº¦èŒƒæ•°èŒƒå›´: {stats['norm_range'][0]:.4e} ~ {stats['norm_range'][1]:.4e}")
        self.log(f"  æ¢¯åº¦èŒƒæ•°æ¯”ç‡: {stats['norm_ratio']:.2f}x")

        if report['diagnosis']:
            self.log(f"\nè¯Šæ–­ç»“æœ:")
            for diag in report['diagnosis']:
                severity_icon = "ğŸ”´" if diag['severity'] == 'critical' else "âš ï¸" if diag['severity'] == 'high' else "â„¹ï¸"
                self.log(f"\n  {severity_icon} {diag['issue']}")
                self.log(f"     æè¿°: {diag['description']}")
                self.log(f"     å»ºè®®: {diag['recommendation']}")

        self.log("="*80 + "\n")

    def save_gradient_stats(self, save_path: str):
        """ä¿å­˜æ¢¯åº¦ç»Ÿè®¡åˆ° JSON æ–‡ä»¶"""
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_stats = {}
        for name, stats in self.gradient_stats.items():
            serializable_stats[name] = {
                key: [float(v) for v in values]
                for key, values in stats.items()
            }

        with open(save_path, 'w') as f:
            json.dump(serializable_stats, f, indent=2)

        self.log(f"âœ“ æ¢¯åº¦ç»Ÿè®¡å·²ä¿å­˜: {save_path}")


def normalize_importance_scores(
    importance_scores: Dict[int, float],
    method: str = 'minmax',
    epsilon: float = 1e-8
) -> Dict[int, float]:
    """
    å½’ä¸€åŒ–é‡è¦æ€§å¾—åˆ†ä»¥ç¼“è§£æç«¯å‰ªæ

    Args:
        importance_scores: åŸå§‹é‡è¦æ€§å¾—åˆ†
        method: å½’ä¸€åŒ–æ–¹æ³•
            - 'minmax': æœ€å°-æœ€å¤§å½’ä¸€åŒ–
            - 'zscore': Z-score æ ‡å‡†åŒ–
            - 'log': å¯¹æ•°å˜æ¢
            - 'sqrt': å¹³æ–¹æ ¹å˜æ¢
        epsilon: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

    Returns:
        Dict[int, float]: å½’ä¸€åŒ–åçš„é‡è¦æ€§å¾—åˆ†
    """
    if not importance_scores:
        return {}

    values = np.array(list(importance_scores.values()))
    keys = list(importance_scores.keys())

    if method == 'minmax':
        # æœ€å°-æœ€å¤§å½’ä¸€åŒ–åˆ° [0, 1]
        min_val = values.min()
        max_val = values.max()
        normalized = (values - min_val) / (max_val - min_val + epsilon)

    elif method == 'zscore':
        # Z-score æ ‡å‡†åŒ–
        mean_val = values.mean()
        std_val = values.std()
        normalized = (values - mean_val) / (std_val + epsilon)
        # æ˜ å°„åˆ° [0, 1]
        normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + epsilon)

    elif method == 'log':
        # å¯¹æ•°å˜æ¢
        # å…ˆç¡®ä¿æ‰€æœ‰å€¼ä¸ºæ­£
        min_val = values.min()
        shifted = values - min_val + 1.0
        normalized = np.log(shifted)
        # å½’ä¸€åŒ–åˆ° [0, 1]
        normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + epsilon)

    elif method == 'sqrt':
        # å¹³æ–¹æ ¹å˜æ¢
        min_val = values.min()
        shifted = values - min_val
        normalized = np.sqrt(shifted)
        normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + epsilon)

    else:
        raise ValueError(f"Unknown normalization method: {method}")

    return {k: float(v) for k, v in zip(keys, normalized)}


def clip_importance_scores(
    importance_scores: Dict[int, float],
    percentile_low: float = 5.0,
    percentile_high: float = 95.0
) -> Dict[int, float]:
    """
    è£å‰ªé‡è¦æ€§å¾—åˆ†çš„æç«¯å€¼

    Args:
        importance_scores: åŸå§‹é‡è¦æ€§å¾—åˆ†
        percentile_low: ä¸‹é™ç™¾åˆ†ä½ï¼ˆé»˜è®¤5%ï¼‰
        percentile_high: ä¸Šé™ç™¾åˆ†ä½ï¼ˆé»˜è®¤95%ï¼‰

    Returns:
        Dict[int, float]: è£å‰ªåçš„é‡è¦æ€§å¾—åˆ†
    """
    if not importance_scores:
        return {}

    values = np.array(list(importance_scores.values()))
    keys = list(importance_scores.keys())

    low_bound = np.percentile(values, percentile_low)
    high_bound = np.percentile(values, percentile_high)

    clipped = np.clip(values, low_bound, high_bound)

    return {k: float(v) for k, v in zip(keys, clipped)}
