{
  "model": "Mistral",
  "config_type": "best_acc",
  "lora_config": {
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "num_epochs": 2,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "micro_batch_size": 4
  },
  "pruned_model_info": {
    "selection_criterion": "best_acc",
    "acc_mean": 0.5860786442076396,
    "ppl": 15.22390365600586,
    "pruning_method": "blockwise",
    "taylor_seq_len": "256.0",
    "taylor_num_samples": "4.0",
    "source_dir": "results/search_Mistral_blockwise_20/exp_016_taylor_seq_len256_taylor_num_samples4",
    "model": "Mistral",
    "task_accuracies": {
      "boolq": 0.7642201834862385,
      "piqa": 0.7181719260065288,
      "hellaswag": 0.6230830511850229,
      "winogrande": 0.6400947119179163,
      "arc_easy": 0.6001683501683501,
      "arc_challenge": 0.3848122866894198,
      "openbookqa": 0.372
    }
  }
}