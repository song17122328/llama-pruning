{
  "model": "Mistral",
  "config_type": "base",
  "lora_config": {
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "num_epochs": 2,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "micro_batch_size": 1
  },
  "base_model_path": "/newdata/LLMs/Mistral-7B-v0.3"
}