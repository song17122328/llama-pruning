{
  "model": "Llama",
  "config_type": "base",
  "lora_config": {
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "num_epochs": 2,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "micro_batch_size": 4
  },
  "base_model_path": "/newdata/LLMs/Llama-3-8B"
}