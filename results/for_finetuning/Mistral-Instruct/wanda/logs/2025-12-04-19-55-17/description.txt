- Training Parameters: 
  - base_model: /newdata/LLMs/Mistral-7B-Instruct-v0.3
  - output_name: for_finetuning/Mistral-Instruct/wanda
  - pruning_ratio: 0.2
  - importance_method: wanda
  - dataset: c4
  - gradient_batch_size: 4
  - use_gradient_checkpointing: False
  - temperature: 0.0
  - tau: None
  - epsilon: 0.0
  - freeze_first_n_layers: 0
  - freeze_last_n_layers: 0
  - taylor_num_samples: 256
  - taylor_seq_len: 32
  - layer_importance_num_samples: 50
  - layer_importance_seq_len: 32
  - block_importance_num_samples: 50
  - block_importance_seq_len: 32
  - head_dim: 128
  - gqa_ratio: 4
  - run_evaluation: ppl,zeroshot,speed,memory
  - eval_ppl_datasets: wikitext2,ptb
  - eval_ppl_seq_len: 128
  - eval_ppl_stride: None
  - eval_zeroshot_tasks: boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa
  - eval_use_custom_zeroshot: False
  - finetune: False
  - finetune_data_path: yahma/alpaca-cleaned
  - finetune_epochs: 2
  - finetune_lr: 0.0001
  - finetune_batch_size: 64
  - finetune_micro_batch_size: 4
  - lora_r: 8
  - lora_alpha: 16
  - skip_finetune_evaluation: False
  - device: cuda:2
  - layer_start: 0
  - layer_end: None
