{
  "model_path": "results/taylor_only_5000/pruned_model.bin",
  "timestamp": "2025-11-22T21:56:07.104049",
  "metrics": {
    "model_info": {
      "total_params": 4015140864,
      "trainable_params": 4015140864,
      "total_params_M": 4015.140864,
      "total_params_B": 4.015140864,
      "attention_params": 859832320,
      "mlp_params": 2104369152,
      "attention_params_M": 859.83232,
      "mlp_params_M": 2104.369152,
      "attention_ratio": 0.21414748551148216,
      "mlp_ratio": 0.5241084244062029,
      "num_layers": 32,
      "model_size_mb": 7658.273681640625,
      "model_size_gb": 7.478782892227173
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 426.2071533203125,
      "ptb": 535.9150390625
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.6195718654434251,
        "full_results": {
          "alias": "boolq",
          "acc,none": 0.6195718654434251,
          "acc_stderr,none": 0.00849131002705965
        }
      },
      "piqa": {
        "accuracy": 0.5571273122959739,
        "full_results": {
          "alias": "piqa",
          "acc,none": 0.5723612622415669,
          "acc_stderr,none": 0.011543009623283034,
          "acc_norm,none": 0.5571273122959739,
          "acc_norm_stderr,none": 0.011589430503509043
        }
      },
      "hellaswag": {
        "accuracy": 0.3233419637522406,
        "full_results": {
          "alias": "hellaswag",
          "acc,none": 0.279326827325234,
          "acc_stderr,none": 0.004477514681327925,
          "acc_norm,none": 0.3233419637522406,
          "acc_norm_stderr,none": 0.004667960519938462
        }
      },
      "winogrande": {
        "accuracy": 0.5114443567482242,
        "full_results": {
          "alias": "winogrande",
          "acc,none": 0.5114443567482242,
          "acc_stderr,none": 0.014048804199859391
        }
      },
      "arc_easy": {
        "accuracy": 0.2967171717171717,
        "full_results": {
          "alias": "arc_easy",
          "acc,none": 0.3194444444444444,
          "acc_stderr,none": 0.009567482017268035,
          "acc_norm,none": 0.2967171717171717,
          "acc_norm_stderr,none": 0.009373559492986735
        }
      },
      "arc_challenge": {
        "accuracy": 0.24914675767918087,
        "full_results": {
          "alias": "arc_challenge",
          "acc,none": 0.22098976109215018,
          "acc_stderr,none": 0.012124929206818241,
          "acc_norm,none": 0.24914675767918087,
          "acc_norm_stderr,none": 0.012639407111926505
        }
      },
      "openbookqa": {
        "accuracy": 0.26,
        "full_results": {
          "alias": "openbookqa",
          "acc,none": 0.122,
          "acc_stderr,none": 0.01465132494504475,
          "acc_norm,none": 0.26,
          "acc_norm_stderr,none": 0.019635965529725526
        }
      }
    },
    "avg_zeroshot_acc": 0.40247848966231664,
    "efficiency": {
      "model_info": {
        "total_params": 4015140864,
        "trainable_params": 4015140864,
        "total_params_M": 4015.140864,
        "total_params_B": 4.015140864,
        "attention_params": 859832320,
        "mlp_params": 2104369152,
        "attention_params_M": 859.83232,
        "mlp_params_M": 2104.369152,
        "attention_ratio": 0.21414748551148216,
        "mlp_ratio": 0.5241084244062029,
        "num_layers": 32,
        "model_size_mb": 7658.273681640625,
        "model_size_gb": 7.478782892227173
      },
      "speed": {
        "batch_size_1": {
          "throughput_tokens_per_sec": 29.899821582904416,
          "latency_ms_per_token": 33.445015624165535,
          "total_time_sec": 214.04809999465942,
          "total_tokens": 6400
        },
        "batch_size_4": {
          "throughput_tokens_per_sec": 113.7604800808275,
          "latency_ms_per_token": 8.790398909089467,
          "total_time_sec": 54.00821089744568,
          "total_tokens": 6144
        }
      },
      "memory": {
        "model_memory_mb": 30686.40771484375,
        "inference_peak_mb": 30766.66455078125
      }
    }
  }
}