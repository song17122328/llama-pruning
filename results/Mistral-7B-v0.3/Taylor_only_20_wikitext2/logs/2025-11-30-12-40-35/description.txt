- Training Parameters: 
  - base_model: /newdata/LLMs/Mistral-7B-v0.3
  - output_name: Mistral-7B-v0.3/Taylor_only_20_wikitext2
  - pruning_ratio: 0.2
  - importance_method: taylor
  - dataset: wikitext2
  - gradient_batch_size: 4
  - use_gradient_checkpointing: False
  - temperature: 0.0
  - tau: None
  - epsilon: 0
  - freeze_first_n_layers: 0
  - freeze_last_n_layers: 0
  - head_dim: 128
  - gqa_ratio: 4
  - run_evaluation: ppl, zeroshot,speed,memory
  - eval_ppl_datasets: wikitext2,ptb
  - eval_ppl_seq_len: 128
  - eval_ppl_stride: None
  - eval_zeroshot_tasks: boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa
  - eval_use_custom_zeroshot: False
  - finetune: False
  - finetune_data_path: yahma/alpaca-cleaned
  - finetune_epochs: 2
  - finetune_lr: 0.0001
  - finetune_batch_size: 64
  - finetune_micro_batch_size: 4
  - lora_r: 8
  - lora_alpha: 16
  - skip_finetune_evaluation: False
  - device: cuda:4
  - layer_start: 0
  - layer_end: None
