- Training Parameters: 
  - base_model: /newdata/LLMs/Llama-3-8B-Instruct
  - output_name: all_model_blockwise_128_128_8/Llama-Instruct/magnitude
  - pruning_ratio: 0.2
  - importance_method: magnitude
  - dataset: c4
  - gradient_batch_size: 8
  - use_gradient_checkpointing: False
  - temperature: 0.0
  - tau: -100
  - epsilon: 0
  - freeze_first_n_layers: 0
  - freeze_last_n_layers: 0
  - taylor_num_samples: 128
  - taylor_seq_len: 128
  - layer_importance_num_samples: 128
  - layer_importance_seq_len: 128
  - block_importance_num_samples: 128
  - block_importance_seq_len: 128
  - head_dim: 128
  - gqa_ratio: 4
  - run_evaluation: ppl, zeroshot
  - eval_ppl_datasets: wikitext2,ptb
  - eval_ppl_seq_len: 128
  - eval_ppl_stride: None
  - eval_zeroshot_tasks: boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa
  - eval_use_custom_zeroshot: False
  - finetune: False
  - finetune_data_path: yahma/alpaca-cleaned
  - finetune_epochs: 2
  - finetune_lr: 0.0001
  - finetune_batch_size: 64
  - finetune_micro_batch_size: 4
  - lora_r: 8
  - lora_alpha: 16
  - skip_finetune_evaluation: False
  - device: cuda:1
  - layer_start: 0
  - layer_end: None
