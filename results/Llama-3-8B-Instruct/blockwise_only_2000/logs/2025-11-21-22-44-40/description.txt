- Training Parameters: 
  - base_model: /newdata/LLMs/Llama-3-8B-Instruct
  - output_name: blockwise_only_200
  - pruning_ratio: 0.2
  - importance_method: taylor
  - dataset: wikitext2
  - gradient_batch_size: 4
  - use_gradient_checkpointing: False
  - temperature: 1.0
  - tau: 0.0
  - epsilon: 0.0
  - head_dim: 128
  - gqa_ratio: 4
  - run_evaluation: ppl,zeroshot
  - eval_ppl_datasets: wikitext2,ptb
  - eval_zeroshot_tasks: boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa
  - eval_use_custom_zeroshot: False
  - finetune: False
  - finetune_method: lora
  - finetune_samples: 500
  - finetune_lr: 0.0001
  - finetune_epochs: 1
  - lora_r: 8
  - lora_alpha: 16
  - device: cuda:0
  - layer_start: 0
  - layer_end: None
