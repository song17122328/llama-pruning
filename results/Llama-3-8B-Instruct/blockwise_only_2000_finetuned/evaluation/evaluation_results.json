{
  "model_path": "results/blockwise_only_2000_finetuned/pruned_model.bin",
  "timestamp": "2025-11-25T20:45:11.065287",
  "metrics": {
    "model_info": {
      "total_params": 6424215552,
      "trainable_params": 0,
      "total_params_M": 6424.215552,
      "total_params_B": 6.424215552,
      "attention_params": 1142947840,
      "mlp_params": 4230328320,
      "attention_params_M": 1142.94784,
      "mlp_params_M": 4230.32832,
      "attention_ratio": 0.17791243627312212,
      "mlp_ratio": 0.6584972570982625,
      "num_layers": 32,
      "model_size_mb": 12253.218872070312,
      "model_size_gb": 11.966034054756165
    },
    "ppl": {
      "wikitext2 (wikitext-2-raw-v1)": 29.040300369262695,
      "ptb": 47.44215393066406
    },
    "zeroshot": {
      "boolq": {
        "accuracy": 0.8348623853211009,
        "full_results": {
          "alias": "boolq",
          "acc,none": 0.8348623853211009,
          "acc_stderr,none": 0.00649416009152059
        }
      },
      "piqa": {
        "accuracy": 0.7442872687704026,
        "full_results": {
          "alias": "piqa",
          "acc,none": 0.7393906420021763,
          "acc_stderr,none": 0.010241826155811836,
          "acc_norm,none": 0.7442872687704026,
          "acc_norm_stderr,none": 0.010178690109459992
        }
      },
      "hellaswag": {
        "accuracy": 0.7030472017526389,
        "full_results": {
          "alias": "hellaswag",
          "acc,none": 0.5238000398327026,
          "acc_stderr,none": 0.004984125363319358,
          "acc_norm,none": 0.7030472017526389,
          "acc_norm_stderr,none": 0.004559817589181744
        }
      },
      "winogrande": {
        "accuracy": 0.6898184688239937,
        "full_results": {
          "alias": "winogrande",
          "acc,none": 0.6898184688239937,
          "acc_stderr,none": 0.0130004541448598
        }
      },
      "arc_easy": {
        "accuracy": 0.6767676767676768,
        "full_results": {
          "alias": "arc_easy",
          "acc,none": 0.7373737373737373,
          "acc_stderr,none": 0.009029861776763957,
          "acc_norm,none": 0.6767676767676768,
          "acc_norm_stderr,none": 0.009597218642045437
        }
      },
      "arc_challenge": {
        "accuracy": 0.4513651877133106,
        "full_results": {
          "alias": "arc_challenge",
          "acc,none": 0.42406143344709896,
          "acc_stderr,none": 0.014441889627464344,
          "acc_norm,none": 0.4513651877133106,
          "acc_norm_stderr,none": 0.014542104569955378
        }
      },
      "openbookqa": {
        "accuracy": 0.406,
        "full_results": {
          "alias": "openbookqa",
          "acc,none": 0.292,
          "acc_stderr,none": 0.020354375480530096,
          "acc_norm,none": 0.406,
          "acc_norm_stderr,none": 0.021983962090086417
        }
      }
    },
    "avg_zeroshot_acc": 0.6437354555927319,
    "efficiency": {
      "model_info": {
        "total_params": 6424215552,
        "trainable_params": 0,
        "total_params_M": 6424.215552,
        "total_params_B": 6.424215552,
        "attention_params": 1142947840,
        "mlp_params": 4230328320,
        "attention_params_M": 1142.94784,
        "mlp_params_M": 4230.32832,
        "attention_ratio": 0.17791243627312212,
        "mlp_ratio": 0.6584972570982625,
        "num_layers": 32,
        "model_size_mb": 12253.218872070312,
        "model_size_gb": 11.966034054756165
      },
      "speed": {
        "batch_size_1": {
          "throughput_tokens_per_sec": 29.21946494333229,
          "latency_ms_per_token": 34.22376152127981,
          "total_time_sec": 219.0320737361908,
          "total_tokens": 6400
        },
        "batch_size_4": {
          "throughput_tokens_per_sec": 110.80595358859229,
          "latency_ms_per_token": 9.024785831570625,
          "total_time_sec": 55.44828414916992,
          "total_tokens": 6144
        }
      },
      "memory": {
        "model_memory_mb": 12295.5322265625,
        "inference_peak_mb": 12394.173828125
      }
    }
  }
}