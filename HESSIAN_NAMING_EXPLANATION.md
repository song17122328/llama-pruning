# Hessian å‚æ•°å‘½åå’ŒäºŒé˜¶æ³°å‹’å®ç°è¯´æ˜

## ğŸ“‹ ä½ æå‡ºçš„é—®é¢˜

ä½ åœ¨ `core/methods/global_pruning.py` ä¸­å‘ç°ï¼š

```python
# äºŒé˜¶é¡¹ï¼ˆå¦‚æœæä¾›äº† Hessianï¼‰
if hessian_diag is not None and layer_idx is not None:
    full_name = f'model.layers.{layer_idx}.self_attn.{name}.weight'
    if full_name in hessian_diag:
        # ...
    else:
        print("âš ï¸ Warning: Hessian info missing for", full_name)
```

**ä½ çš„ç–‘é—®ï¼š**
1. `full_name` æ˜¯ä»€ä¹ˆæ„æ€ï¼Œå¯èƒ½ä¸å­˜åœ¨ï¼Ÿ
2. ä¸ºä»€ä¹ˆåªæœ‰ `self_attn`ï¼Œæ²¡æœ‰ `mlp`ï¼Ÿ

---

## âœ… é—®é¢˜è§£ç­”

### 1. `full_name` çš„å«ä¹‰

**`full_name` æ˜¯ PyTorch æ¨¡å‹ä¸­å‚æ•°çš„å®Œæ•´è·¯å¾„åç§°ã€‚**

#### åœ¨ `run_global_pruning.py` ä¸­ï¼ˆç¬¬ 889-892 è¡Œï¼‰ï¼š

```python
# åˆå§‹åŒ– Hessian å­—å…¸
for name, param in model.named_parameters():
    if param.requires_grad:
        hessian_diag[name] = torch.zeros_like(param.data, device='cpu')
```

è¿™é‡Œçš„ `name` æ˜¯ PyTorch è‡ªåŠ¨ç”Ÿæˆçš„å®Œæ•´å‚æ•°è·¯å¾„ï¼Œä¾‹å¦‚ï¼š
- `model.layers.0.self_attn.q_proj.weight`
- `model.layers.0.self_attn.k_proj.weight`
- `model.layers.0.mlp.gate_proj.weight`
- `model.layers.0.mlp.up_proj.weight`

#### åœ¨ `core/methods/global_pruning.py` ä¸­æ„é€  `full_name`ï¼š

```python
# Attention å±‚ï¼ˆç¬¬ 59 è¡Œï¼‰
full_name = f'model.layers.{layer_idx}.self_attn.{name}.weight'
# ä¾‹å¦‚ï¼šmodel.layers.0.self_attn.q_proj.weight

# MLP å±‚ï¼ˆç¬¬ 222 è¡Œï¼‰
full_name = f'model.layers.{layer_idx}.mlp.{name}.weight'
# ä¾‹å¦‚ï¼šmodel.layers.0.mlp.gate_proj.weight
```

**ç›®çš„ï¼š** é€šè¿‡æ„é€ çš„ `full_name` ä» `hessian_diag` å­—å…¸ä¸­æŸ¥æ‰¾å¯¹åº”å‚æ•°çš„ Hessian å¯¹è§’çº¿å€¼ã€‚

---

### 2. MLP å±‚ç¡®å®æœ‰äºŒé˜¶å¤„ç†ï¼

**ä½ å¯èƒ½è¯¯è§£äº†ï¼ŒMLP å±‚å…¶å®ä¹Ÿæœ‰å®Œæ•´çš„äºŒé˜¶æ³°å‹’å®ç°ã€‚**

#### Attention å±‚å¤„ç†ï¼ˆ`compute_attention_group_importance_taylor`ï¼Œç¬¬ 32-98 è¡Œï¼‰ï¼š

```python
def compute_attention_group_importance_taylor(layer, head_dim=128, gqa_ratio=4,
                                             hessian_diag=None, layer_idx=None):
    salience = {}
    for name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
        sub_layer = getattr(layer.self_attn, name)
        # ä¸€é˜¶é¡¹
        first_order = (sub_layer.weight * sub_layer.weight.grad).abs()

        # äºŒé˜¶é¡¹ï¼ˆå¦‚æœæä¾›äº† Hessianï¼‰
        if hessian_diag is not None and layer_idx is not None:
            full_name = f'model.layers.{layer_idx}.self_attn.{name}.weight'  # â† è¿™é‡Œ
            if full_name in hessian_diag:
                hess = hessian_diag[full_name].to(sub_layer.weight.device)
                second_order = 0.5 * (sub_layer.weight ** 2 * hess).abs()
                salience[name] = first_order + second_order
            else:
                salience[name] = first_order
    # ...
```

#### MLP å±‚å¤„ç†ï¼ˆ`compute_mlp_group_importance_taylor`ï¼Œç¬¬ 189-246 è¡Œï¼‰ï¼š

```python
def compute_mlp_group_importance_taylor(layer, hessian_diag=None, layer_idx=None):
    # ä¸€é˜¶é¡¹
    gate_salience = (layer.mlp.gate_proj.weight * layer.mlp.gate_proj.weight.grad).abs().sum(1)
    up_salience = (layer.mlp.up_proj.weight * layer.mlp.up_proj.weight.grad).abs().sum(1)
    down_salience = (layer.mlp.down_proj.weight * layer.mlp.down_proj.weight.grad).abs().sum(0)

    # äºŒé˜¶é¡¹ï¼ˆå¦‚æœæä¾›äº† Hessianï¼‰
    if hessian_diag is not None and layer_idx is not None:
        for name in ['gate_proj', 'up_proj', 'down_proj']:
            full_name = f'model.layers.{layer_idx}.mlp.{name}.weight'  # â† è¿™é‡Œï¼
            if full_name in hessian_diag:
                sub_layer = getattr(layer.mlp, name)
                hess = hessian_diag[full_name].to(sub_layer.weight.device)
                second_order = 0.5 * (sub_layer.weight ** 2 * hess).abs()

                # ç´¯åŠ äºŒé˜¶é¡¹
                if name == 'gate_proj':
                    gate_salience = gate_salience + second_order.sum(1)
                elif name == 'up_proj':
                    up_salience = up_salience + second_order.sum(1)
                else:  # down_proj
                    down_salience = down_salience + second_order.sum(0)

    channel_importance = gate_salience + up_salience + down_salience
    return channel_importance
```

**ç»“è®ºï¼šAttention å’Œ MLP éƒ½æœ‰å®Œæ•´çš„äºŒé˜¶æ³°å‹’å®ç°ï¼**

---

## ğŸ” å¯èƒ½çš„é—®é¢˜ï¼šå‚æ•°å‘½åä¸åŒ¹é…

### æ½œåœ¨é—®é¢˜ 1ï¼šæ¨¡å‹ç±»å‹ä¸åŒï¼Œå‚æ•°å‰ç¼€å¯èƒ½ä¸åŒ

ä¸åŒæ¨¡å‹çš„å‚æ•°è·¯å¾„å¯èƒ½æœ‰å·®å¼‚ï¼š

| æ¨¡å‹ | å‚æ•°è·¯å¾„ç¤ºä¾‹ |
|------|-------------|
| **LLaMA-3** | `model.layers.0.self_attn.q_proj.weight` |
| **Qwen2.5** | `model.layers.0.self_attn.q_proj.weight` |
| **æŸäº›æ¨¡å‹** | `transformer.h.0.attn.q_proj.weight` |
| **GPT-2** | `transformer.h.0.attn.c_attn.weight` |

å¦‚æœæ¨¡å‹ç»“æ„ä¸æ˜¯æ ‡å‡†çš„ `model.layers.X.self_attn`ï¼Œé‚£ä¹ˆæ„é€ çš„ `full_name` ä¼šæ‰¾ä¸åˆ°å¯¹åº”çš„é”®ã€‚

### æ½œåœ¨é—®é¢˜ 2ï¼šBias å‚æ•°

æŸäº›æ¨¡å‹ï¼ˆå¦‚ Qwen2.5ï¼‰æœ‰ bias å‚æ•°ï¼š
- `model.layers.0.self_attn.q_proj.weight` âœ…
- `model.layers.0.self_attn.q_proj.bias` â† Hessian å­—å…¸é‡Œä¹Ÿä¼šæœ‰è¿™ä¸ª

ä»£ç ç›®å‰åªæŸ¥æ‰¾ `.weight`ï¼Œä¸å¤„ç† `.bias`ï¼ˆè¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºå‰ªæä¸»è¦é’ˆå¯¹ weightï¼‰ã€‚

---

## ğŸ› ï¸ æˆ‘æ·»åŠ çš„è°ƒè¯•åŠŸèƒ½

ä¸ºäº†å¸®åŠ©ä½ è¯Šæ–­é—®é¢˜ï¼Œæˆ‘åœ¨ä»£ç ä¸­æ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼š

### 1. åœ¨ `run_global_pruning.py` ä¸­ï¼ˆç¬¬ 963-978 è¡Œï¼‰ï¼š

```python
if args.importance_method == 'taylor_2nd':
    logger.log(f"  âœ“ Hessian å¯¹è§’çº¿è¿‘ä¼¼è®¡ç®—å®Œæˆ")
    logger.log(f"  Hessian å­—å…¸åŒ…å« {len(hessian_diag)} ä¸ªå‚æ•°")

    # æ‰“å°ä¸€äº›ç¤ºä¾‹é”®åï¼Œç”¨äºè°ƒè¯•
    sample_keys = list(hessian_diag.keys())[:10]
    logger.log(f"  ç¤ºä¾‹ Hessian é”®åï¼ˆå‰10ä¸ªï¼‰ï¼š")
    for key in sample_keys:
        logger.log(f"    - {key}")

    # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„é”®å
    layer_0_keys = [k for k in hessian_diag.keys() if 'layers.0.' in k]
    if layer_0_keys:
        logger.log(f"  Layer 0 çš„å‚æ•°ç¤ºä¾‹ï¼š")
        for key in layer_0_keys[:5]:
            logger.log(f"    - {key}")
```

**ä½œç”¨ï¼š** è¿è¡ŒäºŒé˜¶æ³°å‹’æ—¶ï¼Œä¼šæ‰“å° Hessian å­—å…¸ä¸­çš„å®é™…é”®åï¼Œä½ å¯ä»¥ç›´æ¥çœ‹åˆ°å‚æ•°è·¯å¾„æ ¼å¼ã€‚

### 2. åœ¨ `core/methods/global_pruning.py` ä¸­ï¼ˆç¬¬ 68-73 è¡Œå’Œ 238-243 è¡Œï¼‰ï¼š

```python
# Attention å±‚
if layer_idx == 0:
    print(f"âš ï¸ Warning: Hessian key not found: '{full_name}'")
    # å°è¯•æŸ¥æ‰¾ç›¸ä¼¼çš„é”®
    similar_keys = [k for k in hessian_diag.keys() if name in k and 'attn' in k][:3]
    if similar_keys:
        print(f"   å¯èƒ½çš„åŒ¹é…é”®: {similar_keys}")

# MLP å±‚
if layer_idx == 0:
    print(f"âš ï¸ Warning: Hessian key not found: '{full_name}'")
    similar_keys = [k for k in hessian_diag.keys() if name in k and 'mlp' in k][:3]
    if similar_keys:
        print(f"   å¯èƒ½çš„åŒ¹é…é”®: {similar_keys}")
```

**ä½œç”¨ï¼š**
- å¦‚æœæ‰¾ä¸åˆ° Hessian é”®ï¼Œä¼šæ‰“å°è¯¦ç»†çš„è­¦å‘Š
- åªåœ¨ç¬¬ä¸€å±‚ï¼ˆlayer 0ï¼‰æ‰“å°ï¼Œé¿å…åˆ·å±
- è‡ªåŠ¨æœç´¢å¹¶æ˜¾ç¤ºå¯èƒ½çš„åŒ¹é…é”®ï¼Œå¸®åŠ©ä½ å‘ç°å‘½åå·®å¼‚

---

## ğŸ§ª å¦‚ä½•éªŒè¯

### æ–¹æ³• 1ï¼šè¿è¡ŒäºŒé˜¶æ³°å‹’å‰ªæï¼ˆæ¨èï¼‰

```bash
python run_global_pruning.py \
  --base_model /newdata/LLMs/Llama-3-8B-Instruct \
  --importance_method taylor_2nd \
  --pruning_ratio 0.2 \
  --output_name test_hessian_naming
```

**æŸ¥çœ‹è¾“å‡ºï¼š**
1. åœ¨æ¢¯åº¦è®¡ç®—å®Œæˆåï¼Œä¼šæ‰“å° Hessian å­—å…¸çš„ç¤ºä¾‹é”®å
2. åœ¨å‰ªæè¿‡ç¨‹ä¸­ï¼Œå¦‚æœæœ‰é”®åä¸åŒ¹é…ï¼Œä¼šæ‰“å°è­¦å‘Šå’Œå¯èƒ½çš„åŒ¹é…

**é¢„æœŸæƒ…å†µï¼š**

#### âœ… æ­£å¸¸æƒ…å†µï¼ˆLLaMA-3ï¼‰ï¼š
```
âœ“ Hessian å¯¹è§’çº¿è¿‘ä¼¼è®¡ç®—å®Œæˆ
  Hessian å­—å…¸åŒ…å« 291 ä¸ªå‚æ•°
  ç¤ºä¾‹ Hessian é”®åï¼ˆå‰10ä¸ªï¼‰ï¼š
    - model.embed_tokens.weight
    - model.layers.0.self_attn.q_proj.weight
    - model.layers.0.self_attn.k_proj.weight
    - model.layers.0.self_attn.v_proj.weight
    - model.layers.0.self_attn.o_proj.weight
    - model.layers.0.mlp.gate_proj.weight
    - model.layers.0.mlp.up_proj.weight
    - model.layers.0.mlp.down_proj.weight
    - ...
  Layer 0 çš„å‚æ•°ç¤ºä¾‹ï¼š
    - model.layers.0.self_attn.q_proj.weight
    - model.layers.0.self_attn.k_proj.weight
    - model.layers.0.mlp.gate_proj.weight
```

å‰ªæè¿‡ç¨‹ä¸­**ä¸ä¼šå‡ºç°è­¦å‘Š**ï¼Œè¯´æ˜é”®åå®Œå…¨åŒ¹é…ã€‚

#### âŒ å¼‚å¸¸æƒ…å†µï¼ˆéæ ‡å‡†æ¨¡å‹ï¼‰ï¼š
```
âš ï¸ Warning: Hessian key not found: 'model.layers.0.self_attn.q_proj.weight'
   å¯èƒ½çš„åŒ¹é…é”®: ['transformer.h.0.attn.q_proj.weight', ...]
```

è¯´æ˜æ¨¡å‹ç»“æ„ä¸æ˜¯æ ‡å‡†çš„ `model.layers` æ ¼å¼ï¼Œéœ€è¦ä¿®æ”¹ä»£ç ä¸­çš„ `full_name` æ„é€ é€»è¾‘ã€‚

### æ–¹æ³• 2ï¼šä½¿ç”¨æµ‹è¯•è„šæœ¬ï¼ˆéœ€è¦ torch ç¯å¢ƒï¼‰

æˆ‘å·²ç»åˆ›å»ºäº† `test_hessian_naming.py`ï¼š

```bash
python test_hessian_naming.py --model /newdata/LLMs/Llama-3-8B-Instruct
```

è¿™ä¼šç›´æ¥æ‰“å°æ¨¡å‹çš„å‚æ•°å‘½åæ ¼å¼ã€‚

---

## ğŸ“Š å‚æ•°å‘½åè§„åˆ™æ€»ç»“

### æ ‡å‡† Transformer æ¨¡å‹ï¼ˆLLaMAã€Qwenã€Mistralï¼‰ï¼š

```
æ¨¡å‹å‰ç¼€: model.
  â”œâ”€ embed_tokens.weight                              # Embedding
  â”œâ”€ layers.{i}.                                      # ç¬¬ i å±‚
  â”‚    â”œâ”€ self_attn.                                  # Attention å—
  â”‚    â”‚    â”œâ”€ q_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”‚    â”œâ”€ k_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”‚    â”œâ”€ v_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”‚    â””â”€ o_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”œâ”€ mlp.                                        # MLP å—
  â”‚    â”‚    â”œâ”€ gate_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”‚    â”œâ”€ up_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”‚    â””â”€ down_proj.weight (å¯èƒ½æœ‰ .bias)
  â”‚    â”œâ”€ input_layernorm.weight
  â”‚    â””â”€ post_attention_layernorm.weight
  â””â”€ norm.weight                                      # æœ€åçš„ LayerNorm
```

### ä»£ç ä¸­æ„é€ çš„é”®åï¼š

| å±‚ç±»å‹ | æ„é€ æ ¼å¼ | ç¤ºä¾‹ |
|--------|---------|------|
| Attention | `f'model.layers.{layer_idx}.self_attn.{name}.weight'` | `model.layers.5.self_attn.q_proj.weight` |
| MLP | `f'model.layers.{layer_idx}.mlp.{name}.weight'` | `model.layers.5.mlp.gate_proj.weight` |

**è¿™ä¸ PyTorch çš„ `model.named_parameters()` è¿”å›çš„åç§°ä¸€è‡´ï¼**

---

## ğŸ¯ ç»“è®º

### å›ç­”ä½ çš„é—®é¢˜ï¼š

1. **`full_name` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**
   - æ˜¯ PyTorch æ¨¡å‹ä¸­å‚æ•°çš„å®Œæ•´è·¯å¾„åç§°
   - ç”¨äºä» `hessian_diag` å­—å…¸ä¸­æŸ¥æ‰¾å¯¹åº”çš„ Hessian å€¼
   - å¯¹äºæ ‡å‡†æ¨¡å‹ï¼ˆLLaMAã€Qwenï¼‰ï¼Œå‘½ååº”è¯¥å®Œå…¨åŒ¹é…ï¼Œä¸ä¼š"ä¸å­˜åœ¨"

2. **ä¸ºä»€ä¹ˆåªæœ‰ `self_attn`ï¼Œæ²¡æœ‰ `mlp`ï¼Ÿ**
   - **è¿™æ˜¯è¯¯è§£ï¼** MLP å±‚ä¹Ÿæœ‰å®Œæ•´çš„äºŒé˜¶æ³°å‹’å®ç°
   - åœ¨ `compute_mlp_group_importance_taylor` å‡½æ•°ä¸­ï¼ˆç¬¬ 222 è¡Œï¼‰
   - å¤„ç† `gate_proj`ã€`up_proj`ã€`down_proj` ä¸‰ä¸ªå±‚

### å¦‚æœç¡®å®å‡ºç°"é”®åä¸å­˜åœ¨"çš„é—®é¢˜ï¼š

**å¯èƒ½åŸå› ï¼š**
- ä½¿ç”¨äº†éæ ‡å‡†ç»“æ„çš„æ¨¡å‹ï¼ˆä¸æ˜¯ `model.layers` æ ¼å¼ï¼‰
- æ¨¡å‹å‚æ•°è·¯å¾„æœ‰ç‰¹æ®Šå‰ç¼€æˆ–åç¼€

**è§£å†³æ–¹æ³•ï¼š**
1. è¿è¡Œå‰ªæï¼ŒæŸ¥çœ‹è°ƒè¯•è¾“å‡ºä¸­çš„å®é™…é”®å
2. æ ¹æ®å®é™…é”®åæ ¼å¼ï¼Œä¿®æ”¹ `full_name` çš„æ„é€ é€»è¾‘
3. æˆ–è€…æä¾›å…·ä½“çš„æ¨¡å‹è·¯å¾„ï¼Œæˆ‘å¯ä»¥å¸®ä½ é€‚é…

---

## ğŸ“ ä¸‹ä¸€æ­¥å»ºè®®

1. **è¿è¡Œä¸€æ¬¡äºŒé˜¶æ³°å‹’å‰ªæ**ï¼ŒæŸ¥çœ‹è°ƒè¯•è¾“å‡ºï¼š
   ```bash
   python run_global_pruning.py \
     --base_model /newdata/LLMs/Llama-3-8B-Instruct \
     --importance_method taylor_2nd \
     --pruning_ratio 0.2 \
     --output_name debug_hessian
   ```

2. **æ£€æŸ¥æ—¥å¿—è¾“å‡º**ï¼š
   - æŸ¥çœ‹ "ç¤ºä¾‹ Hessian é”®å" éƒ¨åˆ†
   - æŸ¥çœ‹æ˜¯å¦æœ‰ "âš ï¸ Warning: Hessian key not found" è­¦å‘Š

3. **å¦‚æœæœ‰è­¦å‘Š**ï¼š
   - è®°å½•å®é™…çš„é”®åæ ¼å¼
   - å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®ä½ ä¿®æ”¹ä»£ç é€‚é…è¯¥æ¨¡å‹

4. **å¦‚æœæ²¡æœ‰è­¦å‘Š**ï¼š
   - è¯´æ˜å‘½åå®Œå…¨åŒ¹é…ï¼ŒäºŒé˜¶æ³°å‹’åº”è¯¥æ­£å¸¸å·¥ä½œ
   - å¯ä»¥å¯¹æ¯”ä¸€é˜¶å’ŒäºŒé˜¶çš„å‰ªæç»“æœï¼ŒéªŒè¯äºŒé˜¶ç¡®å®ä¸åŒ

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `run_global_pruning.py` (ç¬¬ 889-978 è¡Œ)ï¼šHessian å­—å…¸åˆå§‹åŒ–å’Œæ¢¯åº¦è®¡ç®—
- `core/methods/global_pruning.py` (ç¬¬ 32-246 è¡Œ)ï¼šAttention å’Œ MLP çš„äºŒé˜¶æ³°å‹’å®ç°
- `TAYLOR_2ND_FIX.md`ï¼šäºŒé˜¶æ³°å‹’æ¢¯åº¦è®¡ç®—ä¿®å¤è¯´æ˜
